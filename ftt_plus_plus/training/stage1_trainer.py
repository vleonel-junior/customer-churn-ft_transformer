"""
Stage1Trainer - Gestionnaire de l'√âtape 1 du Pipeline FTT++

Ce module g√®re l'entra√Ænement du mod√®le FTT+ et la s√©lection des features importantes.
Il re√ßoit les fonctions d'entra√Ænement en param√®tre pour √©viter les d√©pendances.
"""

import torch
import torch.nn as nn
import time
from typing import Dict, List, Tuple, Optional, Any, Callable

from ..core.model_ftt_plus import FTTPlusModelWrapper
from ..config.feature_mapping import FeatureMapping


class Stage1Trainer:
    """
    Gestionnaire de l'√©tape 1 : Entra√Ænement FTT+ et s√©lection des features importantes.
    
    Cette classe est g√©n√©rique et ne contient aucune d√©pendance √† un dataset sp√©cifique.
    Toutes les fonctions d'entra√Ænement sont pass√©es en param√®tre.
    """
    
    def __init__(
        self, 
        feature_mapping: FeatureMapping, 
        ftt_plus_config: Dict[str, Any], 
        M: int, 
        results_dir: str
    ):
        """
        Args:
            feature_mapping: Mapping g√©n√©rique des features
            ftt_plus_config: Configuration du mod√®le FTT+
            M: Nombre de features √† s√©lectionner
            results_dir: R√©pertoire de sauvegarde
        """
        self.feature_mapping = feature_mapping
        self.ftt_plus_config = ftt_plus_config
        self.M = M
        self.results_dir = results_dir
    
    def train_ftt_plus(
        self,
        X: Dict[str, Tuple[torch.Tensor, torch.Tensor]],
        y: Dict[str, torch.Tensor],
        cat_cardinalities: List[int],
        train_func: Callable,
        val_func: Callable,
        evaluate_func: Callable,
        create_loaders_func: Callable,
        n_epochs: int = 50,
        lr: float = 1e-3,
        batch_size: int = 64,
        patience: int = 10,
        seed: int = 0,
        embedding_type: str = "LR",
        device: str = 'cuda'
    ) -> Dict[str, Any]:
        """
        Entra√Æne un mod√®le FTT+ complet et s√©lectionne les features importantes.
        
        Args:
            X: Donn√©es d'entr√©e (train, val, test) avec X[split] = (x_num, x_cat)
            y: Labels (train, val, test)
            cat_cardinalities: Cardinalit√©s des features cat√©gorielles
            train_func: Fonction d'entra√Ænement pour une √©poque
            val_func: Fonction de validation pour une √©poque
            evaluate_func: Fonction d'√©valuation finale
            create_loaders_func: Fonction pour cr√©er les loaders
            n_epochs: Nombre d'√©poques d'entra√Ænement
            lr: Taux d'apprentissage
            batch_size: Taille des batches
            patience: Patience pour early stopping
            seed: Seed pour la reproductibilit√©
            embedding_type: Type d'embedding num√©rique
            device: Device configur√© par le script d'entra√Ænement
            
        Returns:
            R√©sultats de l'√©tape 1 avec features s√©lectionn√©es
        """
        print("üöÄ === √âTAPE 1: Entra√Ænement FTT+ Complet ===")
        print(f"üñ•Ô∏è  Device utilis√©: {device}")
        
        # Validation des donn√©es avec le mapping
        n_num_features = X['train'][0].shape[1]
        n_cat_features = X['train'][1].shape[1] if X['train'][1] is not None else 0
        self.feature_mapping.validate_data_consistency(n_num_features, n_cat_features)
        
        # Cr√©er et configurer le mod√®le FTT+
        model_ftt_plus = self._create_ftt_plus_model(
            n_num_features, cat_cardinalities, embedding_type, X, y, device
        )
        
        # Entra√Ænement complet
        training_results = self._train_model(
            model_ftt_plus, X, y, train_func, val_func, create_loaders_func,
            n_epochs, lr, batch_size, patience, device
        )
        
        # √âvaluation finale
        performance_results = self._evaluate_model(
            model_ftt_plus, X, y, evaluate_func, seed
        )
        
        # Analyse d'interpr√©tabilit√© et s√©lection des features
        interpretability_results, selected_features, feature_importance_scores = (
            self._analyze_and_select_features(
                model_ftt_plus, X, y, seed, n_num_features, cat_cardinalities, 
                embedding_type, training_results, performance_results
            )
        )
        
        # R√©sultats de l'√©tape 1
        stage1_results = {
            'model_ftt_plus': model_ftt_plus,
            'selected_features': selected_features,
            'feature_importance_scores': feature_importance_scores,
            'cls_importance_all': interpretability_results['cls_importance'],
            'n_total_features': self.feature_mapping.n_total_features,
            'n_selected_features': self.M,
            'selection_ratio': self.M / self.feature_mapping.n_total_features,
            'training_results': training_results,
            'performance_results': performance_results,
            'interpretability_results': interpretability_results
        }
        
        return stage1_results
    
    def _create_ftt_plus_model(
        self, 
        n_num_features: int, 
        cat_cardinalities: List[int],
        embedding_type: str, 
        X: Dict, 
        y: Dict, 
        device: str
    ) -> FTTPlusModelWrapper:
        """Cr√©e et configure le mod√®le FTT+."""
        model_ftt_plus = FTTPlusModelWrapper.create_model(
            n_num_features=n_num_features,
            cat_cardinalities=cat_cardinalities,
            feature_names=self.feature_mapping.all_feature_names,
            model_config=self.ftt_plus_config,
            device=device
        )
        
        # Embedding num√©rique personnalis√©
        print(f"Type d'embedding num√©rique: {embedding_type}")
        # Forcer les tenseurs sur CPU pour √©viter le warning rtdl_num_embeddings
        X_train_cpu = X['train'][0].cpu()
        model_ftt_plus.configure_num_embedding(
            embedding_type=embedding_type,
            X_train=X_train_cpu,
            d_embedding=self.ftt_plus_config['d_token'],
            y_train=y['train'] if embedding_type in ("T", "T-L", "T-LR", "T-LR-LR") else None
        )
        
        model_info = model_ftt_plus.get_model_info()
        print(f"Mod√®le FTT+ cr√©√© avec {model_info['n_parameters']:,} param√®tres")
        
        return model_ftt_plus
    
    def _train_model(
        self, 
        model: FTTPlusModelWrapper, 
        X: Dict, 
        y: Dict, 
        train_func: Callable,
        val_func: Callable,
        create_loaders_func: Callable,
        n_epochs: int,
        lr: float, 
        batch_size: int, 
        patience: int, 
        device: str
    ) -> Dict[str, Any]:
        """Entra√Æne le mod√®le avec early stopping."""
        # Cr√©er les loaders via la fonction pass√©e en param√®tre
        train_loader, val_loader = create_loaders_func(y, batch_size, device)
        
        # Optimiseur et fonction de perte
        optimizer = torch.optim.AdamW(model.optimization_param_groups(), lr=lr, weight_decay=0.0)
        loss_fn = torch.nn.BCEWithLogitsLoss()
        
        # Entra√Ænement
        print("‚è≥ Entra√Ænement du mod√®le FTT+ en cours...")
        train_loss_list = []
        val_loss_list = []
        best_val_loss = float('inf')
        best_epoch = 0
        patience_counter = 0
        best_model_state = None
        
        for epoch in range(n_epochs):
            start_time = time.time()
            
            # Entra√Ænement via la fonction pass√©e en param√®tre
            train_loss = train_func(epoch, model, optimizer, X, y, train_loader, loss_fn)
            
            # Validation via la fonction pass√©e en param√®tre
            val_loss = val_func(epoch, model, X, y, val_loader, loss_fn)
            
            # Sauvegarde des m√©triques
            train_loss_list.append(train_loss)
            val_loss_list.append(val_loss)
            
            epoch_time = time.time() - start_time
            print(f'Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {epoch_time:.2f}s')
            
            # Early stopping et sauvegarde du meilleur mod√®le
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_epoch = epoch
                patience_counter = 0
                best_model_state = model.state_dict().copy()
                print(f' <<< NOUVEAU MEILLEUR MOD√àLE (val_loss: {val_loss:.4f})')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f'\nEarly stopping √† l\'√©poque {epoch} (patience: {patience})')
                    break
        
        # Charger le meilleur mod√®le
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
            print(f"‚úÖ Meilleur mod√®le charg√© (√©poque {best_epoch}, val_loss: {best_val_loss:.4f})")
        
        return {
            'train_losses': train_loss_list,
            'val_losses': val_loss_list,
            'best_epoch': best_epoch,
            'best_val_loss': best_val_loss
        }
    
    def _evaluate_model(
        self, 
        model: FTTPlusModelWrapper, 
        X: Dict, 
        y: Dict, 
        evaluate_func: Callable,
        seed: int
    ) -> Dict[str, Any]:
        """√âvalue le mod√®le sur validation et test."""
        print("\nüìä √âvaluation finale du mod√®le FTT+...")
        val_performance = evaluate_func(model, 'val', X, y, seed)
        test_performance = evaluate_func(model, 'test', X, y, seed)
        
        return {
            'val': val_performance,
            'test': test_performance
        }
    
    def _analyze_and_select_features(
        self, 
        model: FTTPlusModelWrapper, 
        X: Dict, 
        y: Dict, 
        seed: int,
        n_num_features: int, 
        cat_cardinalities: List[int],
        embedding_type: str, 
        training_results: Dict,
        performance_results: Dict
    ) -> Tuple[Dict, List[str], Dict[str, float]]:
        """Analyse l'interpr√©tabilit√© et s√©lectionne les M features importantes."""
        print("\nüîç Analyse d'interpr√©tabilit√© avec interpretability_analyzer...")
        
        model_config = {
            'n_num_features': n_num_features,
            'cat_cardinalities': cat_cardinalities,
            'embedding_type': embedding_type,
            **self.ftt_plus_config
        }

        from interpretability_analyzer import analyze_interpretability
        
        # Utiliser interpretability_analyzer pour l'analyse compl√®te
        interpretability_results = analyze_interpretability(
            model=model,
            X=X,
            y=y,
            model_name='interpretable_ftt_plus_from_ftt_plus_plus',
            seed=seed,
            model_config=model_config,
            training_results=training_results,
            performance_results=performance_results,
            feature_names=self.feature_mapping.all_feature_names,
            local_output_dir=None,  # Pas de sauvegarde locale pour l'√©tape 1
            results_base_dir=str(self.results_dir)
        )
        
        # Extraire l'importance des features depuis les r√©sultats
        cls_importance = interpretability_results['cls_importance']
        
        # S√©lectionner les M features les plus importantes
        print(f"\nüéØ S√©lection des {self.M} features les plus importantes...")
        sorted_importance = sorted(cls_importance.items(), key=lambda x: x[1], reverse=True)
        selected_features = [name for name, score in sorted_importance[:self.M]]
        feature_importance_scores = {name: score for name, score in sorted_importance[:self.M]}
        
        print("üìã Features s√©lectionn√©es:")
        for i, (feature, score) in enumerate(feature_importance_scores.items(), 1):
            feature_type = self.feature_mapping.get_feature_type(feature)
            print(f"  {i:2d}. {feature:<20} ({feature_type}): {score:.4f}")
        
        return interpretability_results, selected_features, feature_importance_scores