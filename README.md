# FTT+ : Feature Tokenizer Transformers interprÃ©tables pour donnÃ©es tabulaires

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![Deep Learning](https://img.shields.io/badge/Deep%20Learning-Transformers-orange)](https://pytorch.org)

*Concilier **performance** et **interprÃ©tabilitÃ©** sur donnÃ©es tabulaires*

</div>

---

## ğŸ“‹ Table des matiÃ¨res

- [Introduction](#-introduction)
- [Architecture FTT+](#-architecture-ftt)
- [Sparse FTT+](#-sparse-ftt)
- [Structure du code](#-structure-du-code)
- [Configuration](#-configuration)
- [Installation](#-installation)
- [RÃ©fÃ©rences](#-rÃ©fÃ©rences)

---

## ğŸ¯ Introduction

Ce dÃ©pÃ´t propose une architecture innovante pour l'apprentissage sur donnÃ©es tabulaires :

- **FTT+** (FT-Transformer Plus) : attention sÃ©lective et interprÃ©table
- **Sparse FTT+** : variante utilisant une attention sparse pour une interprÃ©tabilitÃ© encore plus fine

### ğŸ¨ Modes d'attention supportÃ©s

| Mode | Description | Interactions |
|------|-------------|--------------|
| `cls` | FTT+ original | CLS â†” Features uniquement |
| `hybrid` | **Mode par dÃ©faut** | CLS â†” Features + Features â†” Features |
| `full` | Attention complÃ¨te | Toutes positions (hors diagonale) |

---

## ğŸ—ï¸ Architecture FTT+

### Vue d'ensemble

<div align="center">
<img src="images/FT_Transformer architecture.png" alt="Architecture globale" width="80%" style="max-width: 700px;">
<br><em>Architecture globale du FT-Transformer pour donnÃ©es tabulaires</em>
</div>

### ğŸ”„ Pipeline de traitement

#### 1. **Tokenisation des features**

<div align="center">
<img src="images/Illustration%20d'un%20Feature%20Tokenizer.png" alt="Feature Tokenizer" width="80%" style="max-width: 700px;">
<br><em>Processus de tokenisation : variables brutes â†’ vecteurs denses</em>
</div>

Le `FeatureTokenizer` transforme chaque variable (numÃ©rique/catÃ©gorielle) en reprÃ©sentation vectorielle dense.

#### 2. **Token CLS & Blocs Transformer**

Chaque bloc Transformer applique sÃ©quentiellement :

<div align="center">
<img src="images/One Transformer layer.png" alt="Bloc Transformer" width="60%" style="max-width: 400px;">
<br><em>Architecture d'un bloc Transformer FTT+</em>
</div>

##### **Interpretable Multi-Head Attention**

<div align="center">
<img src="images/Scaled Dot-Product Attention.png" alt="Attention mÃ©canisme" width="50%" style="max-width: 350px;">
<br><em>MÃ©canisme d'attention adaptatif selon le mode choisi</em>
</div>

**CaractÃ©ristiques clÃ©s :**
- Q/K spÃ©cifiques par tÃªte, V partagÃ©e
- Moyenne des scores d'attention pour interprÃ©tabilitÃ© directe
- SchÃ©ma d'attention flexible selon configuration

<div align="center">
<img src="images/Interpretable Multi-Head Attention.png" alt="Multi-Head Attention" width="80%" style="max-width: 700px;">
<br><em>Interpretable Multi-Head Attention : importance rÃ©elle des features</em>
</div>

##### **Feed-Forward Network & Normalisation**
- Transformation non-linÃ©aire classique
- LayerNorm et connexions rÃ©siduelles

#### 3. **Classification finale**
PrÃ©diction basÃ©e sur la reprÃ©sentation du token CLS.

### ğŸ“Š Extraction de l'interprÃ©tabilitÃ©

L'importance des features est extraite directement de la matrice d'attention CLSâ†’features, permettant :
- **Visualisations intuitives** : barplots, heatmaps
- **Transparence des dÃ©cisions** : identification des features influents
- **Analyse des interactions** : comprendre les relations entre variables

---

## âš¡ Sparse FTT+

### Principe

Sparse FTT+ utilise l'attention sparse (`sparsemax`) au lieu de l'attention softmax standard.

### Avantages

| Aspect | BÃ©nÃ©fice |
|--------|----------|
| **Interactions** | RÃ©duction du nombre d'interactions significatives |
| **Performance** | Concentration sur les relations les plus pertinentes |
| **InterprÃ©tabilitÃ©** | Identification explicite des features les plus influents |

### Fonctionnement

La fonction `sparsemax` produit des distributions d'attention :
- **Strictement positives** sur un sous-ensemble restreint
- **Nulles** ailleurs, Ã©liminant le bruit

---

## ğŸ“ Structure du code

```
ğŸ“¦ ftt_plus/
â”œâ”€â”€ ğŸ“„ attention.py         # Attention sÃ©lective/interprÃ©table
â””â”€â”€ ğŸ“„ model.py             # Architecture FTT+ complÃ¨te

ğŸ“¦ sparse_ftt_plus/
â”œâ”€â”€ ğŸ“„ attention.py         # Attention sparse/interprÃ©table
â””â”€â”€  ğŸ“„ model.py             # Architecture Sparse FTT+
```

---

## âš™ï¸ Configuration

### ParamÃ©trage FTT+

```python
ftt_plus_config = {
    # Architecture
    'd_model': 256,
    'n_heads': 8,
    'n_layers': 6,
    
    # Mode d'attention
    'attention_mode': 'hybrid',  # 'cls', 'hybrid', 'full'
    
    # Autres paramÃ¨tres...
}
```

### Modes d'attention dÃ©taillÃ©s

- **`cls`** : Reproduction fidÃ¨le du FTT+ original
- **`hybrid`** : **RecommandÃ©** - Ã‰quilibre performance/interprÃ©tabilitÃ©  
- **`full`** : Maximum d'interactions, plus coÃ»teux

---

## ğŸ”§ Installation

### DÃ©pendances de base
```bash
pip install torch transformers numpy pandas matplotlib seaborn
```

### Pour Sparse FTT+
```bash
pip install sparsemax
```

---

## ğŸ¯ Pourquoi cette Ã©tude ?

### Enjeux actuels

| Domaine | ProblÃ©matique | Solution FTT+ |
|---------|---------------|---------------|
| **Entreprise** | DÃ©cisions opaques en finance/santÃ© | InterprÃ©tabilitÃ© native |
| **IA Responsable** | "Black box effect" | Transparence des modÃ¨les |
| **Recherche** | Trade-off performance/explicabilitÃ© | Architecture optimisÃ©e |

### Contributions

- **ğŸ” Transparence** : MÃ©canismes d'attention interprÃ©tables
- **ğŸ“ˆ Performance** : Architecture optimisÃ©e pour donnÃ©es tabulaires  
- **ğŸ› ï¸ RÃ©utilisabilitÃ©** : Code modulaire et visualisations prÃªtes

---

## ğŸ“š RÃ©fÃ©rences

- **Vaswani, A., et al.** (2017). *Attention Is All You Need*. NeurIPS.
- **Gorishniy, Y., et al.** (2021). *Revisiting Deep Learning Models for Tabular Data*.
- **Isomura, T., et al.** (2023). *Optimizing FT-Transformer: Sparse Attention for Improved Performance*.
- **Devlin, J., et al.** (2018). *BERT: Pre-training of Deep Bidirectional Transformers*.

---

<div align="center">

**ğŸš€ FTT+ â€“ InterprÃ©tabilitÃ© avancÃ©e pour donnÃ©es tabulaires**

*DÃ©veloppÃ© par **LÃ©onel VODOUNOU** â€¢ 2025*

[![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](votre-repo-url)
[![Documentation](https://img.shields.io/badge/Docs-Available-blue)](docs-url)

</div>
