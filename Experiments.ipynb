{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5ba49b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-19T18:09:55.916878Z",
     "iopub.status.busy": "2025-07-19T18:09:55.916626Z",
     "iopub.status.idle": "2025-07-19T18:09:57.085613Z",
     "shell.execute_reply": "2025-07-19T18:09:57.084651Z"
    },
    "papermill": {
     "duration": 1.175333,
     "end_time": "2025-07-19T18:09:57.087045",
     "exception": false,
     "start_time": "2025-07-19T18:09:55.911712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'customer-churn-ft_transformer'...\r\n",
      "remote: Enumerating objects: 911, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (265/265), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (178/178), done.\u001b[K\r\n",
      "remote: Total 911 (delta 115), reused 199 (delta 61), pack-reused 646 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (911/911), 2.69 MiB | 14.95 MiB/s, done.\r\n",
      "Resolving deltas: 100% (439/439), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/vleonel-junior/customer-churn-ft_transformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2170ee36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:09:57.096168Z",
     "iopub.status.busy": "2025-07-19T18:09:57.095884Z",
     "iopub.status.idle": "2025-07-19T18:09:57.099956Z",
     "shell.execute_reply": "2025-07-19T18:09:57.099292Z"
    },
    "papermill": {
     "duration": 0.009665,
     "end_time": "2025-07-19T18:09:57.100972",
     "exception": false,
     "start_time": "2025-07-19T18:09:57.091307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/customer-churn-ft_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef33b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:09:57.109187Z",
     "iopub.status.busy": "2025-07-19T18:09:57.108713Z",
     "iopub.status.idle": "2025-07-19T18:09:57.455606Z",
     "shell.execute_reply": "2025-07-19T18:09:57.454691Z"
    },
    "papermill": {
     "duration": 0.352377,
     "end_time": "2025-07-19T18:09:57.457018",
     "exception": false,
     "start_time": "2025-07-19T18:09:57.104641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/customer-churn-ft_transformer\r\n",
      "total 1300\r\n",
      "drwxr-xr-x 9 root root   4096 Jul 19 18:09  .\r\n",
      "drwxr-xr-x 3 root root   4096 Jul 19 18:09  ..\r\n",
      "drwxr-xr-x 3 root root   4096 Jul 19 18:09  data\r\n",
      "-rw-r--r-- 1 root root  33381 Jul 19 18:09  Experiments.ipynb\r\n",
      "drwxr-xr-x 2 root root   4096 Jul 19 18:09  ftt_plus\r\n",
      "drwxr-xr-x 7 root root   4096 Jul 19 18:09  ftt_plus_plus\r\n",
      "-rw-r--r-- 1 root root  16100 Jul 19 18:09 'FT_Transformer architecture.png'\r\n",
      "drwxr-xr-x 8 root root   4096 Jul 19 18:09  .git\r\n",
      "-rw-r--r-- 1 root root  36514 Jul 19 18:09 \"Illustration d'un Feature Tokenizer.png\"\r\n",
      "-rw-r--r-- 1 root root   8324 Jul 19 18:09  interpretability_analyzer.py\r\n",
      "-rw-r--r-- 1 root root 311104 Jul 19 18:09 'Interpretable Multi-Head Attention.png'\r\n",
      "-rw-r--r-- 1 root root  10266 Jul 19 18:09  num_embedding_factory.py\r\n",
      "-rw-r--r-- 1 root root 252941 Jul 19 18:09 'One Transformer layer.png'\r\n",
      "-rw-r--r-- 1 root root 183257 Jul 19 18:09  pipeline_ftt_plus_plus.png\r\n",
      "drwxr-xr-x 2 root root   4096 Jul 19 18:09  __pycache__\r\n",
      "-rw-r--r-- 1 root root   6862 Jul 19 18:09  README.md\r\n",
      "drwxr-xr-x 5 root root   4096 Jul 19 18:09  rtdl_lib\r\n",
      "-rw-r--r-- 1 root root 409306 Jul 19 18:09 'Scaled Dot-Product Attention.png'\r\n",
      "-rw-r--r-- 1 root root   6821 Jul 19 18:09  test_embeddings.py\r\n",
      "drwxr-xr-x 3 root root   4096 Jul 19 18:09  train\r\n",
      "-rw-r--r-- 1 root root   2333 Jul 19 18:09  utils.py\r\n",
      "total 968\r\n",
      "drwxr-xr-x 3 root root   4096 Jul 19 18:09 .\r\n",
      "drwxr-xr-x 9 root root   4096 Jul 19 18:09 ..\r\n",
      "-rw-r--r-- 1 root root   4224 Jul 19 18:09 process_telecom_data.py\r\n",
      "drwxr-xr-x 2 root root   4096 Jul 19 18:09 __pycache__\r\n",
      "-rw-r--r-- 1 root root 970457 Jul 19 18:09 Telco_Customer_Churn.csv\r\n",
      "-rw-r--r-- 1 root root      0 Jul 19 18:09 utils.py\r\n"
     ]
    }
   ],
   "source": [
    "# Le bon chemin est directement :\n",
    "import os\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Vérifier que vous êtes au bon endroit\n",
    "!pwd\n",
    "!ls -la\n",
    "\n",
    "# Vérifier que les datasets sont là\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcb0c12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:09:57.466162Z",
     "iopub.status.busy": "2025-07-19T18:09:57.465891Z",
     "iopub.status.idle": "2025-07-19T18:11:13.367043Z",
     "shell.execute_reply": "2025-07-19T18:11:13.366042Z"
    },
    "papermill": {
     "duration": 75.907262,
     "end_time": "2025-07-19T18:11:13.368544",
     "exception": false,
     "start_time": "2025-07-19T18:09:57.461282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation de rtdl_num_embeddings réussie\r\n",
      "Installation de scikit-learn réussie\r\n"
     ]
    }
   ],
   "source": [
    "# Installation silencieuse des packages pour rtdl_num_embeddings\n",
    "!pip install rtdl_num_embeddings -qqq > /dev/null 2>&1 && echo 'Installation de rtdl_num_embeddings réussie'\n",
    "!pip install \"scikit-learn>=1.0,<2\" -qqq > /dev/null 2>&1 && echo 'Installation de scikit-learn réussie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e04ebaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:11:13.377973Z",
     "iopub.status.busy": "2025-07-19T18:11:13.377343Z",
     "iopub.status.idle": "2025-07-19T18:12:50.265916Z",
     "shell.execute_reply": "2025-07-19T18:12:50.264943Z"
    },
    "papermill": {
     "duration": 96.895093,
     "end_time": "2025-07-19T18:12:50.267751",
     "exception": false,
     "start_time": "2025-07-19T18:11:13.372658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation de libzero réussie\r\n",
      "Installation de optuna réussie\r\n"
     ]
    }
   ],
   "source": [
    "!pip install libzero==0.0.4 -qqq > /dev/null 2>&1 && echo 'Installation de libzero réussie'\n",
    "!pip install optuna -qqq > /dev/null 2>&1 && echo 'Installation de optuna réussie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c8219",
   "metadata": {
    "papermill": {
     "duration": 0.003993,
     "end_time": "2025-07-19T18:12:50.276016",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.272023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d96037",
   "metadata": {
    "papermill": {
     "duration": 0.003717,
     "end_time": "2025-07-19T18:12:50.283526",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.279809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FT-T training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b51e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:12:50.293181Z",
     "iopub.status.busy": "2025-07-19T18:12:50.292645Z",
     "iopub.status.idle": "2025-07-19T18:12:50.298750Z",
     "shell.execute_reply": "2025-07-19T18:12:50.298176Z"
    },
    "papermill": {
     "duration": 0.012547,
     "end_time": "2025-07-19T18:12:50.299850",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.287303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\n# Changer vers le répertoire racine\\nos.chdir('/kaggle/working/customer-churn-ft_transformer')\\n\\n# Utiliser PYTHONPATH pour que train.py trouve les modules\\n!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt/train.py\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Changer vers le répertoire racine\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Utiliser PYTHONPATH pour que train.py trouve les modules\n",
    "!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt/train.py\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44d9b6",
   "metadata": {
    "papermill": {
     "duration": 0.003834,
     "end_time": "2025-07-19T18:12:50.307773",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.303939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edb02123",
   "metadata": {
    "papermill": {
     "duration": 0.003858,
     "end_time": "2025-07-19T18:12:50.315655",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.311797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Interpretable FT-T+ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb758bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:12:50.324476Z",
     "iopub.status.busy": "2025-07-19T18:12:50.324276Z",
     "iopub.status.idle": "2025-07-19T18:12:50.329208Z",
     "shell.execute_reply": "2025-07-19T18:12:50.328399Z"
    },
    "papermill": {
     "duration": 0.010724,
     "end_time": "2025-07-19T18:12:50.330357",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.319633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\n# Changer vers le répertoire racine\\nos.chdir('/kaggle/working/customer-churn-ft_transformer')\\n\\n# Utiliser PYTHONPATH pour que train.py trouve les modules\\n!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus/train.py\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Changer vers le répertoire racine\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Utiliser PYTHONPATH pour que train.py trouve les modules\n",
    "!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus/train.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863fcc8",
   "metadata": {
    "papermill": {
     "duration": 0.003904,
     "end_time": "2025-07-19T18:12:50.338250",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.334346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7f9f478",
   "metadata": {
    "papermill": {
     "duration": 0.003919,
     "end_time": "2025-07-19T18:12:50.346271",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.342352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Interpretable FT-T++ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14529704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:12:50.355413Z",
     "iopub.status.busy": "2025-07-19T18:12:50.354786Z",
     "iopub.status.idle": "2025-07-19T18:12:50.358828Z",
     "shell.execute_reply": "2025-07-19T18:12:50.358351Z"
    },
    "papermill": {
     "duration": 0.009586,
     "end_time": "2025-07-19T18:12:50.359848",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.350262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\n# Changer vers le répertoire racine\\nos.chdir('/kaggle/working/customer-churn-ft_transformer')\\n\\n# Embedding par défaut (LR)\\n!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus_plus/train.py\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Changer vers le répertoire racine\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Embedding par défaut (LR)\n",
    "!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus_plus/train.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d727c8",
   "metadata": {
    "papermill": {
     "duration": 0.003968,
     "end_time": "2025-07-19T18:12:50.368050",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.364082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52f383",
   "metadata": {
    "papermill": {
     "duration": 0.003889,
     "end_time": "2025-07-19T18:12:50.375960",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.372071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a85addd",
   "metadata": {
    "papermill": {
     "duration": 0.003927,
     "end_time": "2025-07-19T18:12:50.384104",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.380177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FT-T Model Optimized with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebacba25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:12:50.393100Z",
     "iopub.status.busy": "2025-07-19T18:12:50.392902Z",
     "iopub.status.idle": "2025-07-19T18:12:50.397074Z",
     "shell.execute_reply": "2025-07-19T18:12:50.396420Z"
    },
    "papermill": {
     "duration": 0.00993,
     "end_time": "2025-07-19T18:12:50.398084",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.388154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\n# Changer vers le répertoire racine\\nos.chdir('/kaggle/working/customer-churn-ft_transformer')\\n\\n# Utiliser PYTHONPATH pour que train.py trouve les modules\\n!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt/experiment_with_optuna.py\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Changer vers le répertoire racine\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Utiliser PYTHONPATH pour que train.py trouve les modules\n",
    "!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt/experiment_with_optuna.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bdbb5e",
   "metadata": {
    "papermill": {
     "duration": 0.004094,
     "end_time": "2025-07-19T18:12:50.406533",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.402439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137c319d",
   "metadata": {
    "papermill": {
     "duration": 0.004108,
     "end_time": "2025-07-19T18:12:50.414878",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.410770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FT-T+ Model Optimized with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aec3f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:12:50.424183Z",
     "iopub.status.busy": "2025-07-19T18:12:50.423626Z",
     "iopub.status.idle": "2025-07-19T18:12:50.427863Z",
     "shell.execute_reply": "2025-07-19T18:12:50.427252Z"
    },
    "papermill": {
     "duration": 0.009847,
     "end_time": "2025-07-19T18:12:50.428843",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.418996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\n# Changer vers le répertoire racine\\nos.chdir('/kaggle/working/customer-churn-ft_transformer')\\n\\n# Utiliser PYTHONPATH pour que train.py trouve les modules\\n!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus/experiment_with_optuna.py\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Changer vers le répertoire racine\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Utiliser PYTHONPATH pour que train.py trouve les modules\n",
    "!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus/experiment_with_optuna.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70ef26",
   "metadata": {
    "papermill": {
     "duration": 0.004108,
     "end_time": "2025-07-19T18:12:50.437400",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.433292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b3fa306",
   "metadata": {
    "papermill": {
     "duration": 0.004135,
     "end_time": "2025-07-19T18:12:50.445721",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.441586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FT-T++ Model Optimized with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbf4cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T18:12:50.455000Z",
     "iopub.status.busy": "2025-07-19T18:12:50.454795Z",
     "iopub.status.idle": "2025-07-20T02:33:21.342958Z",
     "shell.execute_reply": "2025-07-20T02:33:21.342165Z"
    },
    "papermill": {
     "duration": 30030.894381,
     "end_time": "2025-07-20T02:33:21.344382",
     "exception": false,
     "start_time": "2025-07-19T18:12:50.450001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-07-19 18:12:57,194]\u001b[0m A new study created in memory with name: ftt_plus_plus_optuna_enhanced\u001b[0m\r\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 62,497 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6300 | Val Loss: 0.5769 | Time: 3.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5769)\r\n",
      "Epoch 001 | Train Loss: 0.5604 | Val Loss: 0.5190 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5190)\r\n",
      "Epoch 002 | Train Loss: 0.5143 | Val Loss: 0.4696 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4696)\r\n",
      "Epoch 003 | Train Loss: 0.4833 | Val Loss: 0.4548 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4548)\r\n",
      "Epoch 004 | Train Loss: 0.4653 | Val Loss: 0.4492 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4492)\r\n",
      "Epoch 005 | Train Loss: 0.4595 | Val Loss: 0.4456 | Time: 3.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4456)\r\n",
      "Epoch 006 | Train Loss: 0.4534 | Val Loss: 0.4412 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4412)\r\n",
      "Epoch 007 | Train Loss: 0.4485 | Val Loss: 0.4395 | Time: 3.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4395)\r\n",
      "Epoch 008 | Train Loss: 0.4444 | Val Loss: 0.4378 | Time: 3.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4378)\r\n",
      "Epoch 009 | Train Loss: 0.4462 | Val Loss: 0.4335 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4335)\r\n",
      "Epoch 010 | Train Loss: 0.4396 | Val Loss: 0.4324 | Time: 3.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4324)\r\n",
      "Epoch 011 | Train Loss: 0.4380 | Val Loss: 0.4312 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4312)\r\n",
      "Epoch 012 | Train Loss: 0.4383 | Val Loss: 0.4266 | Time: 3.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4266)\r\n",
      "Epoch 013 | Train Loss: 0.4350 | Val Loss: 0.4241 | Time: 3.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 014 | Train Loss: 0.4296 | Val Loss: 0.4228 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4228)\r\n",
      "Epoch 015 | Train Loss: 0.4314 | Val Loss: 0.4217 | Time: 3.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 016 | Train Loss: 0.4285 | Val Loss: 0.4199 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4199)\r\n",
      "Epoch 017 | Train Loss: 0.4318 | Val Loss: 0.4178 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 018 | Train Loss: 0.4268 | Val Loss: 0.4192 | Time: 3.43s\r\n",
      "Epoch 019 | Train Loss: 0.4307 | Val Loss: 0.4189 | Time: 3.44s\r\n",
      "Epoch 020 | Train Loss: 0.4254 | Val Loss: 0.4168 | Time: 3.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4168)\r\n",
      "Epoch 021 | Train Loss: 0.4277 | Val Loss: 0.4172 | Time: 3.42s\r\n",
      "Epoch 022 | Train Loss: 0.4262 | Val Loss: 0.4160 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4160)\r\n",
      "Epoch 023 | Train Loss: 0.4238 | Val Loss: 0.4144 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4144)\r\n",
      "Epoch 024 | Train Loss: 0.4243 | Val Loss: 0.4153 | Time: 3.36s\r\n",
      "Epoch 025 | Train Loss: 0.4241 | Val Loss: 0.4152 | Time: 3.38s\r\n",
      "Epoch 026 | Train Loss: 0.4213 | Val Loss: 0.4153 | Time: 3.36s\r\n",
      "Epoch 027 | Train Loss: 0.4216 | Val Loss: 0.4129 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 028 | Train Loss: 0.4215 | Val Loss: 0.4126 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4126)\r\n",
      "Epoch 029 | Train Loss: 0.4194 | Val Loss: 0.4125 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4125)\r\n",
      "Epoch 030 | Train Loss: 0.4212 | Val Loss: 0.4122 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4122)\r\n",
      "Epoch 031 | Train Loss: 0.4179 | Val Loss: 0.4107 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4107)\r\n",
      "Epoch 032 | Train Loss: 0.4217 | Val Loss: 0.4112 | Time: 3.42s\r\n",
      "Epoch 033 | Train Loss: 0.4192 | Val Loss: 0.4121 | Time: 3.43s\r\n",
      "Epoch 034 | Train Loss: 0.4169 | Val Loss: 0.4110 | Time: 3.40s\r\n",
      "Epoch 035 | Train Loss: 0.4175 | Val Loss: 0.4106 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 036 | Train Loss: 0.4164 | Val Loss: 0.4109 | Time: 3.36s\r\n",
      "Epoch 037 | Train Loss: 0.4200 | Val Loss: 0.4096 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4096)\r\n",
      "Epoch 038 | Train Loss: 0.4166 | Val Loss: 0.4107 | Time: 3.64s\r\n",
      "Epoch 039 | Train Loss: 0.4192 | Val Loss: 0.4091 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4091)\r\n",
      "Epoch 040 | Train Loss: 0.4176 | Val Loss: 0.4094 | Time: 3.43s\r\n",
      "Epoch 041 | Train Loss: 0.4182 | Val Loss: 0.4083 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4083)\r\n",
      "Epoch 042 | Train Loss: 0.4165 | Val Loss: 0.4084 | Time: 3.39s\r\n",
      "Epoch 043 | Train Loss: 0.4156 | Val Loss: 0.4078 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4078)\r\n",
      "Epoch 044 | Train Loss: 0.4185 | Val Loss: 0.4084 | Time: 3.37s\r\n",
      "Epoch 045 | Train Loss: 0.4172 | Val Loss: 0.4077 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4077)\r\n",
      "Epoch 046 | Train Loss: 0.4150 | Val Loss: 0.4064 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4064)\r\n",
      "Epoch 047 | Train Loss: 0.4132 | Val Loss: 0.4069 | Time: 3.44s\r\n",
      "Epoch 048 | Train Loss: 0.4150 | Val Loss: 0.4075 | Time: 3.45s\r\n",
      "Epoch 049 | Train Loss: 0.4151 | Val Loss: 0.4073 | Time: 3.37s\r\n",
      "✅ Meilleur modèle chargé (époque 46, val_loss: 0.4064)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.0742\r\n",
      "   2. SeniorCitizen       : 0.0684\r\n",
      "   3. TechSupport         : 0.0683\r\n",
      "   4. MonthlyCharges      : 0.0668\r\n",
      "   5. Dependents          : 0.0582\r\n",
      "   6. StreamingMovies     : 0.0532\r\n",
      "   7. MultipleLines       : 0.0527\r\n",
      "   8. Partner             : 0.0522\r\n",
      "   9. PhoneService        : 0.0514\r\n",
      "  10. OnlineBackup        : 0.0498\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Contract             (CAT): 0.0742\r\n",
      "   2. SeniorCitizen        (CAT): 0.0684\r\n",
      "   3. TechSupport          (CAT): 0.0683\r\n",
      "   4. MonthlyCharges       (NUM): 0.0668\r\n",
      "   5. Dependents           (CAT): 0.0582\r\n",
      "   6. StreamingMovies      (CAT): 0.0532\r\n",
      "   7. MultipleLines        (CAT): 0.0527\r\n",
      "   8. Partner              (CAT): 0.0522\r\n",
      "   9. PhoneService         (CAT): 0.0514\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['Contract', 'SeniorCitizen', 'TechSupport', 'Dependents', 'StreamingMovies', 'MultipleLines', 'Partner', 'PhoneService'] → indices [13, 1, 10, 3, 12, 5, 2, 4]\r\n",
      "📊 Features sélectionnées: 1 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 335,361 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.00%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6020 | Val Loss: 0.5357 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5357)\r\n",
      "Epoch 001 | Train Loss: 0.5379 | Val Loss: 0.5027 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5027)\r\n",
      "Epoch 002 | Train Loss: 0.5127 | Val Loss: 0.4751 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4751)\r\n",
      "Epoch 003 | Train Loss: 0.4913 | Val Loss: 0.4688 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4688)\r\n",
      "Epoch 004 | Train Loss: 0.4817 | Val Loss: 0.4687 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4687)\r\n",
      "Epoch 005 | Train Loss: 0.4774 | Val Loss: 0.4674 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4674)\r\n",
      "Epoch 006 | Train Loss: 0.4790 | Val Loss: 0.4677 | Time: 3.29s\r\n",
      "Epoch 007 | Train Loss: 0.4753 | Val Loss: 0.4680 | Time: 3.37s\r\n",
      "Epoch 008 | Train Loss: 0.4740 | Val Loss: 0.4688 | Time: 3.29s\r\n",
      "Epoch 009 | Train Loss: 0.4737 | Val Loss: 0.4666 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4666)\r\n",
      "Epoch 010 | Train Loss: 0.4715 | Val Loss: 0.4659 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4659)\r\n",
      "Epoch 011 | Train Loss: 0.4675 | Val Loss: 0.4649 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4649)\r\n",
      "Epoch 012 | Train Loss: 0.4742 | Val Loss: 0.4649 | Time: 3.23s\r\n",
      "Epoch 013 | Train Loss: 0.4690 | Val Loss: 0.4646 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4646)\r\n",
      "Epoch 014 | Train Loss: 0.4679 | Val Loss: 0.4654 | Time: 3.27s\r\n",
      "Epoch 015 | Train Loss: 0.4641 | Val Loss: 0.4669 | Time: 3.27s\r\n",
      "Epoch 016 | Train Loss: 0.4641 | Val Loss: 0.4656 | Time: 3.46s\r\n",
      "Epoch 017 | Train Loss: 0.4687 | Val Loss: 0.4653 | Time: 3.26s\r\n",
      "Epoch 018 | Train Loss: 0.4638 | Val Loss: 0.4645 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4645)\r\n",
      "Epoch 019 | Train Loss: 0.4648 | Val Loss: 0.4633 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4633)\r\n",
      "Epoch 020 | Train Loss: 0.4617 | Val Loss: 0.4645 | Time: 3.26s\r\n",
      "Epoch 021 | Train Loss: 0.4622 | Val Loss: 0.4655 | Time: 3.28s\r\n",
      "Epoch 022 | Train Loss: 0.4632 | Val Loss: 0.4654 | Time: 3.28s\r\n",
      "Epoch 023 | Train Loss: 0.4690 | Val Loss: 0.4643 | Time: 3.24s\r\n",
      "Epoch 024 | Train Loss: 0.4638 | Val Loss: 0.4637 | Time: 3.26s\r\n",
      "Epoch 025 | Train Loss: 0.4634 | Val Loss: 0.4633 | Time: 3.30s\r\n",
      "Epoch 026 | Train Loss: 0.4598 | Val Loss: 0.4643 | Time: 3.35s\r\n",
      "Epoch 027 | Train Loss: 0.4655 | Val Loss: 0.4634 | Time: 3.27s\r\n",
      "Epoch 028 | Train Loss: 0.4625 | Val Loss: 0.4642 | Time: 3.26s\r\n",
      "Epoch 029 | Train Loss: 0.4593 | Val Loss: 0.4649 | Time: 3.26s\r\n",
      "Epoch 030 | Train Loss: 0.4610 | Val Loss: 0.4627 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4627)\r\n",
      "Epoch 031 | Train Loss: 0.4573 | Val Loss: 0.4646 | Time: 3.29s\r\n",
      "Epoch 032 | Train Loss: 0.4631 | Val Loss: 0.4634 | Time: 3.25s\r\n",
      "Epoch 033 | Train Loss: 0.4592 | Val Loss: 0.4639 | Time: 3.29s\r\n",
      "Epoch 034 | Train Loss: 0.4587 | Val Loss: 0.4625 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4625)\r\n",
      "Epoch 035 | Train Loss: 0.4558 | Val Loss: 0.4635 | Time: 3.33s\r\n",
      "Epoch 036 | Train Loss: 0.4581 | Val Loss: 0.4636 | Time: 3.43s\r\n",
      "Epoch 037 | Train Loss: 0.4551 | Val Loss: 0.4643 | Time: 3.31s\r\n",
      "Epoch 038 | Train Loss: 0.4592 | Val Loss: 0.4633 | Time: 3.28s\r\n",
      "Epoch 039 | Train Loss: 0.4553 | Val Loss: 0.4624 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4624)\r\n",
      "Epoch 040 | Train Loss: 0.4566 | Val Loss: 0.4621 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4621)\r\n",
      "Epoch 041 | Train Loss: 0.4530 | Val Loss: 0.4641 | Time: 3.24s\r\n",
      "Epoch 042 | Train Loss: 0.4552 | Val Loss: 0.4621 | Time: 3.29s\r\n",
      "Epoch 043 | Train Loss: 0.4550 | Val Loss: 0.4628 | Time: 3.30s\r\n",
      "Epoch 044 | Train Loss: 0.4569 | Val Loss: 0.4622 | Time: 3.30s\r\n",
      "Epoch 045 | Train Loss: 0.4575 | Val Loss: 0.4618 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4618)\r\n",
      "Epoch 046 | Train Loss: 0.4543 | Val Loss: 0.4621 | Time: 3.28s\r\n",
      "Epoch 047 | Train Loss: 0.4536 | Val Loss: 0.4616 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4616)\r\n",
      "Epoch 048 | Train Loss: 0.4544 | Val Loss: 0.4634 | Time: 3.30s\r\n",
      "Epoch 049 | Train Loss: 0.4526 | Val Loss: 0.4626 | Time: 3.31s\r\n",
      "✅ Meilleur modèle Random chargé (époque 47, val_loss: 0.4616)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. MultipleLines        (CAT): 0.1535\r\n",
      "   2. SeniorCitizen        (CAT): 0.1321\r\n",
      "   3. PhoneService         (CAT): 0.1225\r\n",
      "   4. Partner              (CAT): 0.1136\r\n",
      "   5. StreamingMovies      (CAT): 0.1049\r\n",
      "   6. Contract             (CAT): 0.0950\r\n",
      "   7. Dependents           (CAT): 0.0948\r\n",
      "   8. MonthlyCharges       (NUM): 0.0925\r\n",
      "   9. TechSupport          (CAT): 0.0912\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MultipleLines       : 0.1535\r\n",
      "   2. SeniorCitizen       : 0.1321\r\n",
      "   3. PhoneService        : 0.1225\r\n",
      "   4. Partner             : 0.1136\r\n",
      "   5. StreamingMovies     : 0.1049\r\n",
      "   6. Contract            : 0.0950\r\n",
      "   7. Dependents          : 0.0948\r\n",
      "   8. MonthlyCharges      : 0.0925\r\n",
      "   9. TechSupport         : 0.0912\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 339.4s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 62,497 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5971 | Val Loss: 0.5557 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5557)\r\n",
      "Epoch 001 | Train Loss: 0.5478 | Val Loss: 0.4999 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4999)\r\n",
      "Epoch 002 | Train Loss: 0.5032 | Val Loss: 0.4598 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4598)\r\n",
      "Epoch 003 | Train Loss: 0.4752 | Val Loss: 0.4484 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4484)\r\n",
      "Epoch 004 | Train Loss: 0.4580 | Val Loss: 0.4401 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4401)\r\n",
      "Epoch 005 | Train Loss: 0.4482 | Val Loss: 0.4364 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 006 | Train Loss: 0.4418 | Val Loss: 0.4342 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 007 | Train Loss: 0.4331 | Val Loss: 0.4314 | Time: 3.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4314)\r\n",
      "Epoch 008 | Train Loss: 0.4388 | Val Loss: 0.4296 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 009 | Train Loss: 0.4287 | Val Loss: 0.4283 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 010 | Train Loss: 0.4270 | Val Loss: 0.4277 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4277)\r\n",
      "Epoch 011 | Train Loss: 0.4253 | Val Loss: 0.4275 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4275)\r\n",
      "Epoch 012 | Train Loss: 0.4262 | Val Loss: 0.4258 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 013 | Train Loss: 0.4246 | Val Loss: 0.4237 | Time: 3.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4237)\r\n",
      "Epoch 014 | Train Loss: 0.4241 | Val Loss: 0.4229 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4229)\r\n",
      "Epoch 015 | Train Loss: 0.4212 | Val Loss: 0.4227 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 016 | Train Loss: 0.4223 | Val Loss: 0.4226 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4226)\r\n",
      "Epoch 017 | Train Loss: 0.4200 | Val Loss: 0.4214 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 018 | Train Loss: 0.4185 | Val Loss: 0.4221 | Time: 3.38s\r\n",
      "Epoch 019 | Train Loss: 0.4186 | Val Loss: 0.4219 | Time: 3.47s\r\n",
      "Epoch 020 | Train Loss: 0.4173 | Val Loss: 0.4216 | Time: 3.40s\r\n",
      "Epoch 021 | Train Loss: 0.4161 | Val Loss: 0.4217 | Time: 3.40s\r\n",
      "Epoch 022 | Train Loss: 0.4157 | Val Loss: 0.4211 | Time: 3.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4211)\r\n",
      "Epoch 023 | Train Loss: 0.4181 | Val Loss: 0.4195 | Time: 3.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4195)\r\n",
      "Epoch 024 | Train Loss: 0.4120 | Val Loss: 0.4202 | Time: 3.45s\r\n",
      "Epoch 025 | Train Loss: 0.4152 | Val Loss: 0.4194 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 026 | Train Loss: 0.4096 | Val Loss: 0.4196 | Time: 3.46s\r\n",
      "Epoch 027 | Train Loss: 0.4116 | Val Loss: 0.4197 | Time: 3.48s\r\n",
      "Epoch 028 | Train Loss: 0.4086 | Val Loss: 0.4193 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4193)\r\n",
      "Epoch 029 | Train Loss: 0.4112 | Val Loss: 0.4189 | Time: 3.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4189)\r\n",
      "Epoch 030 | Train Loss: 0.4116 | Val Loss: 0.4186 | Time: 3.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 031 | Train Loss: 0.4104 | Val Loss: 0.4199 | Time: 3.57s\r\n",
      "Epoch 032 | Train Loss: 0.4081 | Val Loss: 0.4186 | Time: 3.50s\r\n",
      "Epoch 033 | Train Loss: 0.4091 | Val Loss: 0.4189 | Time: 3.54s\r\n",
      "Epoch 034 | Train Loss: 0.4099 | Val Loss: 0.4172 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4172)\r\n",
      "Epoch 035 | Train Loss: 0.4091 | Val Loss: 0.4164 | Time: 3.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4164)\r\n",
      "Epoch 036 | Train Loss: 0.4071 | Val Loss: 0.4171 | Time: 3.55s\r\n",
      "Epoch 037 | Train Loss: 0.4070 | Val Loss: 0.4178 | Time: 3.50s\r\n",
      "Epoch 038 | Train Loss: 0.4050 | Val Loss: 0.4181 | Time: 3.47s\r\n",
      "Epoch 039 | Train Loss: 0.4070 | Val Loss: 0.4174 | Time: 3.54s\r\n",
      "Epoch 040 | Train Loss: 0.4090 | Val Loss: 0.4174 | Time: 3.53s\r\n",
      "Epoch 041 | Train Loss: 0.4087 | Val Loss: 0.4170 | Time: 3.48s\r\n",
      "Epoch 042 | Train Loss: 0.4038 | Val Loss: 0.4170 | Time: 3.49s\r\n",
      "Epoch 043 | Train Loss: 0.4050 | Val Loss: 0.4184 | Time: 3.50s\r\n",
      "Epoch 044 | Train Loss: 0.4011 | Val Loss: 0.4188 | Time: 3.45s\r\n",
      "Epoch 045 | Train Loss: 0.4027 | Val Loss: 0.4182 | Time: 3.53s\r\n",
      "Epoch 046 | Train Loss: 0.4038 | Val Loss: 0.4167 | Time: 3.52s\r\n",
      "Epoch 047 | Train Loss: 0.4061 | Val Loss: 0.4172 | Time: 3.48s\r\n",
      "Epoch 048 | Train Loss: 0.4035 | Val Loss: 0.4183 | Time: 3.53s\r\n",
      "Epoch 049 | Train Loss: 0.4045 | Val Loss: 0.4173 | Time: 3.59s\r\n",
      "✅ Meilleur modèle chargé (époque 35, val_loss: 0.4164)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.0785\r\n",
      "   2. PhoneService        : 0.0764\r\n",
      "   3. StreamingTV         : 0.0686\r\n",
      "   4. PaperlessBilling    : 0.0674\r\n",
      "   5. Partner             : 0.0660\r\n",
      "   6. MonthlyCharges      : 0.0596\r\n",
      "   7. TechSupport         : 0.0585\r\n",
      "   8. MultipleLines       : 0.0517\r\n",
      "   9. TotalCharges        : 0.0512\r\n",
      "  10. DeviceProtection    : 0.0493\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. tenure               (NUM): 0.0785\r\n",
      "   2. PhoneService         (CAT): 0.0764\r\n",
      "   3. StreamingTV          (CAT): 0.0686\r\n",
      "   4. PaperlessBilling     (CAT): 0.0674\r\n",
      "   5. Partner              (CAT): 0.0660\r\n",
      "   6. MonthlyCharges       (NUM): 0.0596\r\n",
      "   7. TechSupport          (CAT): 0.0585\r\n",
      "   8. MultipleLines        (CAT): 0.0517\r\n",
      "   9. TotalCharges         (NUM): 0.0512\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'MonthlyCharges', 'TotalCharges'] → indices [0, 1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['PhoneService', 'StreamingTV', 'PaperlessBilling', 'Partner', 'TechSupport', 'MultipleLines'] → indices [4, 11, 14, 2, 10, 5]\r\n",
      "📊 Features sélectionnées: 3 numériques, 6 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 334,977 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.00%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5789 | Val Loss: 0.4905 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4905)\r\n",
      "Epoch 001 | Train Loss: 0.4923 | Val Loss: 0.4639 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4639)\r\n",
      "Epoch 002 | Train Loss: 0.4742 | Val Loss: 0.4570 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4570)\r\n",
      "Epoch 003 | Train Loss: 0.4649 | Val Loss: 0.4531 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4531)\r\n",
      "Epoch 004 | Train Loss: 0.4607 | Val Loss: 0.4483 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4483)\r\n",
      "Epoch 005 | Train Loss: 0.4555 | Val Loss: 0.4450 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4450)\r\n",
      "Epoch 006 | Train Loss: 0.4531 | Val Loss: 0.4414 | Time: 3.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4414)\r\n",
      "Epoch 007 | Train Loss: 0.4547 | Val Loss: 0.4395 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4395)\r\n",
      "Epoch 008 | Train Loss: 0.4463 | Val Loss: 0.4385 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 009 | Train Loss: 0.4485 | Val Loss: 0.4366 | Time: 3.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4366)\r\n",
      "Epoch 010 | Train Loss: 0.4442 | Val Loss: 0.4354 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4354)\r\n",
      "Epoch 011 | Train Loss: 0.4469 | Val Loss: 0.4336 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4336)\r\n",
      "Epoch 012 | Train Loss: 0.4437 | Val Loss: 0.4335 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4335)\r\n",
      "Epoch 013 | Train Loss: 0.4409 | Val Loss: 0.4326 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4326)\r\n",
      "Epoch 014 | Train Loss: 0.4411 | Val Loss: 0.4332 | Time: 3.35s\r\n",
      "Epoch 015 | Train Loss: 0.4376 | Val Loss: 0.4328 | Time: 3.40s\r\n",
      "Epoch 016 | Train Loss: 0.4398 | Val Loss: 0.4321 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4321)\r\n",
      "Epoch 017 | Train Loss: 0.4375 | Val Loss: 0.4307 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 018 | Train Loss: 0.4396 | Val Loss: 0.4303 | Time: 3.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4303)\r\n",
      "Epoch 019 | Train Loss: 0.4389 | Val Loss: 0.4293 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 020 | Train Loss: 0.4399 | Val Loss: 0.4305 | Time: 3.34s\r\n",
      "Epoch 021 | Train Loss: 0.4376 | Val Loss: 0.4296 | Time: 3.39s\r\n",
      "Epoch 022 | Train Loss: 0.4388 | Val Loss: 0.4286 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4286)\r\n",
      "Epoch 023 | Train Loss: 0.4360 | Val Loss: 0.4284 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4284)\r\n",
      "Epoch 024 | Train Loss: 0.4366 | Val Loss: 0.4285 | Time: 3.37s\r\n",
      "Epoch 025 | Train Loss: 0.4356 | Val Loss: 0.4285 | Time: 3.35s\r\n",
      "Epoch 026 | Train Loss: 0.4353 | Val Loss: 0.4286 | Time: 3.28s\r\n",
      "Epoch 027 | Train Loss: 0.4361 | Val Loss: 0.4285 | Time: 3.42s\r\n",
      "Epoch 028 | Train Loss: 0.4323 | Val Loss: 0.4293 | Time: 3.42s\r\n",
      "Epoch 029 | Train Loss: 0.4336 | Val Loss: 0.4291 | Time: 3.33s\r\n",
      "Epoch 030 | Train Loss: 0.4353 | Val Loss: 0.4284 | Time: 3.34s\r\n",
      "Epoch 031 | Train Loss: 0.4307 | Val Loss: 0.4288 | Time: 3.33s\r\n",
      "Epoch 032 | Train Loss: 0.4321 | Val Loss: 0.4293 | Time: 3.29s\r\n",
      "Epoch 033 | Train Loss: 0.4348 | Val Loss: 0.4274 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4274)\r\n",
      "Epoch 034 | Train Loss: 0.4351 | Val Loss: 0.4268 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4268)\r\n",
      "Epoch 035 | Train Loss: 0.4322 | Val Loss: 0.4284 | Time: 3.37s\r\n",
      "Epoch 036 | Train Loss: 0.4337 | Val Loss: 0.4281 | Time: 3.38s\r\n",
      "Epoch 037 | Train Loss: 0.4323 | Val Loss: 0.4281 | Time: 3.45s\r\n",
      "Epoch 038 | Train Loss: 0.4319 | Val Loss: 0.4275 | Time: 3.35s\r\n",
      "Epoch 039 | Train Loss: 0.4345 | Val Loss: 0.4280 | Time: 3.38s\r\n",
      "Epoch 040 | Train Loss: 0.4299 | Val Loss: 0.4282 | Time: 3.34s\r\n",
      "Epoch 041 | Train Loss: 0.4310 | Val Loss: 0.4277 | Time: 3.31s\r\n",
      "Epoch 042 | Train Loss: 0.4320 | Val Loss: 0.4275 | Time: 3.38s\r\n",
      "Epoch 043 | Train Loss: 0.4325 | Val Loss: 0.4269 | Time: 3.33s\r\n",
      "Epoch 044 | Train Loss: 0.4320 | Val Loss: 0.4268 | Time: 3.28s\r\n",
      "Epoch 045 | Train Loss: 0.4311 | Val Loss: 0.4268 | Time: 3.34s\r\n",
      "Epoch 046 | Train Loss: 0.4321 | Val Loss: 0.4268 | Time: 3.47s\r\n",
      "Epoch 047 | Train Loss: 0.4332 | Val Loss: 0.4262 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4262)\r\n",
      "Epoch 048 | Train Loss: 0.4318 | Val Loss: 0.4263 | Time: 3.36s\r\n",
      "Epoch 049 | Train Loss: 0.4334 | Val Loss: 0.4252 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4252)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4252)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.2080\r\n",
      "   2. Partner              (CAT): 0.1410\r\n",
      "   3. MonthlyCharges       (NUM): 0.1016\r\n",
      "   4. PaperlessBilling     (CAT): 0.1013\r\n",
      "   5. TechSupport          (CAT): 0.1009\r\n",
      "   6. MultipleLines        (CAT): 0.0984\r\n",
      "   7. StreamingTV          (CAT): 0.0899\r\n",
      "   8. TotalCharges         (NUM): 0.0882\r\n",
      "   9. tenure               (NUM): 0.0707\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.2080\r\n",
      "   2. Partner             : 0.1410\r\n",
      "   3. MonthlyCharges      : 0.1016\r\n",
      "   4. PaperlessBilling    : 0.1013\r\n",
      "   5. TechSupport         : 0.1009\r\n",
      "   6. MultipleLines       : 0.0984\r\n",
      "   7. StreamingTV         : 0.0899\r\n",
      "   8. TotalCharges        : 0.0882\r\n",
      "   9. tenure              : 0.0707\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 344.8s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 62,497 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6132 | Val Loss: 0.5534 | Time: 3.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5534)\r\n",
      "Epoch 001 | Train Loss: 0.5502 | Val Loss: 0.5059 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5059)\r\n",
      "Epoch 002 | Train Loss: 0.5144 | Val Loss: 0.4614 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4614)\r\n",
      "Epoch 003 | Train Loss: 0.4815 | Val Loss: 0.4359 | Time: 3.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4359)\r\n",
      "Epoch 004 | Train Loss: 0.4709 | Val Loss: 0.4258 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 005 | Train Loss: 0.4554 | Val Loss: 0.4204 | Time: 3.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 006 | Train Loss: 0.4506 | Val Loss: 0.4180 | Time: 3.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 007 | Train Loss: 0.4488 | Val Loss: 0.4184 | Time: 3.49s\r\n",
      "Epoch 008 | Train Loss: 0.4431 | Val Loss: 0.4157 | Time: 3.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4157)\r\n",
      "Epoch 009 | Train Loss: 0.4413 | Val Loss: 0.4158 | Time: 3.52s\r\n",
      "Epoch 010 | Train Loss: 0.4394 | Val Loss: 0.4143 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4143)\r\n",
      "Epoch 011 | Train Loss: 0.4359 | Val Loss: 0.4127 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4127)\r\n",
      "Epoch 012 | Train Loss: 0.4333 | Val Loss: 0.4142 | Time: 3.50s\r\n",
      "Epoch 013 | Train Loss: 0.4327 | Val Loss: 0.4133 | Time: 3.44s\r\n",
      "Epoch 014 | Train Loss: 0.4345 | Val Loss: 0.4135 | Time: 3.59s\r\n",
      "Epoch 015 | Train Loss: 0.4292 | Val Loss: 0.4127 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4127)\r\n",
      "Epoch 016 | Train Loss: 0.4275 | Val Loss: 0.4115 | Time: 3.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4115)\r\n",
      "Epoch 017 | Train Loss: 0.4279 | Val Loss: 0.4108 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4108)\r\n",
      "Epoch 018 | Train Loss: 0.4293 | Val Loss: 0.4114 | Time: 3.48s\r\n",
      "Epoch 019 | Train Loss: 0.4254 | Val Loss: 0.4101 | Time: 3.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4101)\r\n",
      "Epoch 020 | Train Loss: 0.4288 | Val Loss: 0.4109 | Time: 3.51s\r\n",
      "Epoch 021 | Train Loss: 0.4247 | Val Loss: 0.4118 | Time: 3.51s\r\n",
      "Epoch 022 | Train Loss: 0.4244 | Val Loss: 0.4128 | Time: 3.50s\r\n",
      "Epoch 023 | Train Loss: 0.4244 | Val Loss: 0.4110 | Time: 3.68s\r\n",
      "Epoch 024 | Train Loss: 0.4239 | Val Loss: 0.4112 | Time: 3.49s\r\n",
      "Epoch 025 | Train Loss: 0.4246 | Val Loss: 0.4105 | Time: 3.50s\r\n",
      "Epoch 026 | Train Loss: 0.4237 | Val Loss: 0.4110 | Time: 3.55s\r\n",
      "Epoch 027 | Train Loss: 0.4222 | Val Loss: 0.4115 | Time: 3.50s\r\n",
      "Epoch 028 | Train Loss: 0.4242 | Val Loss: 0.4095 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4095)\r\n",
      "Epoch 029 | Train Loss: 0.4225 | Val Loss: 0.4089 | Time: 3.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4089)\r\n",
      "Epoch 030 | Train Loss: 0.4201 | Val Loss: 0.4096 | Time: 3.51s\r\n",
      "Epoch 031 | Train Loss: 0.4184 | Val Loss: 0.4094 | Time: 3.52s\r\n",
      "Epoch 032 | Train Loss: 0.4207 | Val Loss: 0.4110 | Time: 3.55s\r\n",
      "Epoch 033 | Train Loss: 0.4205 | Val Loss: 0.4086 | Time: 3.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4086)\r\n",
      "Epoch 034 | Train Loss: 0.4199 | Val Loss: 0.4083 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4083)\r\n",
      "Epoch 035 | Train Loss: 0.4188 | Val Loss: 0.4092 | Time: 3.45s\r\n",
      "Epoch 036 | Train Loss: 0.4204 | Val Loss: 0.4097 | Time: 3.45s\r\n",
      "Epoch 037 | Train Loss: 0.4177 | Val Loss: 0.4096 | Time: 3.53s\r\n",
      "Epoch 038 | Train Loss: 0.4169 | Val Loss: 0.4100 | Time: 3.46s\r\n",
      "Epoch 039 | Train Loss: 0.4151 | Val Loss: 0.4098 | Time: 3.51s\r\n",
      "Epoch 040 | Train Loss: 0.4206 | Val Loss: 0.4089 | Time: 3.52s\r\n",
      "Epoch 041 | Train Loss: 0.4183 | Val Loss: 0.4072 | Time: 3.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4072)\r\n",
      "Epoch 042 | Train Loss: 0.4167 | Val Loss: 0.4086 | Time: 3.50s\r\n",
      "Epoch 043 | Train Loss: 0.4159 | Val Loss: 0.4079 | Time: 3.51s\r\n",
      "Epoch 044 | Train Loss: 0.4140 | Val Loss: 0.4075 | Time: 3.48s\r\n",
      "Epoch 045 | Train Loss: 0.4155 | Val Loss: 0.4083 | Time: 3.49s\r\n",
      "Epoch 046 | Train Loss: 0.4156 | Val Loss: 0.4101 | Time: 3.49s\r\n",
      "Epoch 047 | Train Loss: 0.4123 | Val Loss: 0.4093 | Time: 3.47s\r\n",
      "Epoch 048 | Train Loss: 0.4131 | Val Loss: 0.4096 | Time: 3.46s\r\n",
      "Epoch 049 | Train Loss: 0.4140 | Val Loss: 0.4099 | Time: 3.57s\r\n",
      "✅ Meilleur modèle chargé (époque 41, val_loss: 0.4072)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.0775\r\n",
      "   2. OnlineSecurity      : 0.0772\r\n",
      "   3. TechSupport         : 0.0732\r\n",
      "   4. MonthlyCharges      : 0.0695\r\n",
      "   5. MultipleLines       : 0.0663\r\n",
      "   6. InternetService     : 0.0616\r\n",
      "   7. SeniorCitizen       : 0.0581\r\n",
      "   8. Dependents          : 0.0533\r\n",
      "   9. Contract            : 0.0532\r\n",
      "  10. PaperlessBilling    : 0.0510\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. DeviceProtection     (CAT): 0.0775\r\n",
      "   2. OnlineSecurity       (CAT): 0.0772\r\n",
      "   3. TechSupport          (CAT): 0.0732\r\n",
      "   4. MonthlyCharges       (NUM): 0.0695\r\n",
      "   5. MultipleLines        (CAT): 0.0663\r\n",
      "   6. InternetService      (CAT): 0.0616\r\n",
      "   7. SeniorCitizen        (CAT): 0.0581\r\n",
      "   8. Dependents           (CAT): 0.0533\r\n",
      "   9. Contract             (CAT): 0.0532\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['DeviceProtection', 'OnlineSecurity', 'TechSupport', 'MultipleLines', 'InternetService', 'SeniorCitizen', 'Dependents', 'Contract'] → indices [9, 7, 10, 5, 6, 1, 3, 13]\r\n",
      "📊 Features sélectionnées: 1 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 335,617 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.00%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6951 | Val Loss: 0.5303 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5303)\r\n",
      "Epoch 001 | Train Loss: 0.5292 | Val Loss: 0.4761 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4761)\r\n",
      "Epoch 002 | Train Loss: 0.4936 | Val Loss: 0.4569 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4569)\r\n",
      "Epoch 003 | Train Loss: 0.4785 | Val Loss: 0.4475 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4475)\r\n",
      "Epoch 004 | Train Loss: 0.4707 | Val Loss: 0.4447 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4447)\r\n",
      "Epoch 005 | Train Loss: 0.4676 | Val Loss: 0.4430 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4430)\r\n",
      "Epoch 006 | Train Loss: 0.4634 | Val Loss: 0.4419 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "Epoch 007 | Train Loss: 0.4656 | Val Loss: 0.4417 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 008 | Train Loss: 0.4580 | Val Loss: 0.4414 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4414)\r\n",
      "Epoch 009 | Train Loss: 0.4600 | Val Loss: 0.4406 | Time: 3.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4406)\r\n",
      "Epoch 010 | Train Loss: 0.4592 | Val Loss: 0.4401 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4401)\r\n",
      "Epoch 011 | Train Loss: 0.4602 | Val Loss: 0.4400 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4400)\r\n",
      "Epoch 012 | Train Loss: 0.4551 | Val Loss: 0.4405 | Time: 3.32s\r\n",
      "Epoch 013 | Train Loss: 0.4586 | Val Loss: 0.4404 | Time: 3.34s\r\n",
      "Epoch 014 | Train Loss: 0.4557 | Val Loss: 0.4412 | Time: 3.34s\r\n",
      "Epoch 015 | Train Loss: 0.4567 | Val Loss: 0.4410 | Time: 3.30s\r\n",
      "Epoch 016 | Train Loss: 0.4535 | Val Loss: 0.4410 | Time: 3.35s\r\n",
      "Epoch 017 | Train Loss: 0.4537 | Val Loss: 0.4404 | Time: 3.33s\r\n",
      "Epoch 018 | Train Loss: 0.4538 | Val Loss: 0.4397 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4397)\r\n",
      "Epoch 019 | Train Loss: 0.4552 | Val Loss: 0.4397 | Time: 3.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4397)\r\n",
      "Epoch 020 | Train Loss: 0.4530 | Val Loss: 0.4399 | Time: 3.35s\r\n",
      "Epoch 021 | Train Loss: 0.4529 | Val Loss: 0.4395 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4395)\r\n",
      "Epoch 022 | Train Loss: 0.4506 | Val Loss: 0.4403 | Time: 3.40s\r\n",
      "Epoch 023 | Train Loss: 0.4549 | Val Loss: 0.4393 | Time: 3.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4393)\r\n",
      "Epoch 024 | Train Loss: 0.4560 | Val Loss: 0.4400 | Time: 3.38s\r\n",
      "Epoch 025 | Train Loss: 0.4529 | Val Loss: 0.4394 | Time: 3.41s\r\n",
      "Epoch 026 | Train Loss: 0.4525 | Val Loss: 0.4400 | Time: 3.38s\r\n",
      "Epoch 027 | Train Loss: 0.4542 | Val Loss: 0.4407 | Time: 3.43s\r\n",
      "Epoch 028 | Train Loss: 0.4524 | Val Loss: 0.4403 | Time: 3.51s\r\n",
      "Epoch 029 | Train Loss: 0.4517 | Val Loss: 0.4410 | Time: 3.41s\r\n",
      "Epoch 030 | Train Loss: 0.4542 | Val Loss: 0.4402 | Time: 3.39s\r\n",
      "Epoch 031 | Train Loss: 0.4501 | Val Loss: 0.4399 | Time: 3.35s\r\n",
      "Epoch 032 | Train Loss: 0.4504 | Val Loss: 0.4400 | Time: 3.34s\r\n",
      "Epoch 033 | Train Loss: 0.4488 | Val Loss: 0.4389 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4389)\r\n",
      "Epoch 034 | Train Loss: 0.4528 | Val Loss: 0.4397 | Time: 3.40s\r\n",
      "Epoch 035 | Train Loss: 0.4480 | Val Loss: 0.4406 | Time: 3.36s\r\n",
      "Epoch 036 | Train Loss: 0.4516 | Val Loss: 0.4407 | Time: 3.36s\r\n",
      "Epoch 037 | Train Loss: 0.4489 | Val Loss: 0.4397 | Time: 3.46s\r\n",
      "Epoch 038 | Train Loss: 0.4501 | Val Loss: 0.4399 | Time: 3.29s\r\n",
      "Epoch 039 | Train Loss: 0.4504 | Val Loss: 0.4395 | Time: 3.34s\r\n",
      "Epoch 040 | Train Loss: 0.4489 | Val Loss: 0.4399 | Time: 3.34s\r\n",
      "Epoch 041 | Train Loss: 0.4507 | Val Loss: 0.4395 | Time: 3.34s\r\n",
      "Epoch 042 | Train Loss: 0.4489 | Val Loss: 0.4394 | Time: 3.35s\r\n",
      "Epoch 043 | Train Loss: 0.4496 | Val Loss: 0.4398 | Time: 3.39s\r\n",
      "Epoch 044 | Train Loss: 0.4498 | Val Loss: 0.4393 | Time: 3.34s\r\n",
      "Epoch 045 | Train Loss: 0.4491 | Val Loss: 0.4403 | Time: 3.30s\r\n",
      "Epoch 046 | Train Loss: 0.4511 | Val Loss: 0.4394 | Time: 3.36s\r\n",
      "Epoch 047 | Train Loss: 0.4494 | Val Loss: 0.4391 | Time: 3.43s\r\n",
      "Epoch 048 | Train Loss: 0.4463 | Val Loss: 0.4394 | Time: 3.35s\r\n",
      "Epoch 049 | Train Loss: 0.4474 | Val Loss: 0.4395 | Time: 3.36s\r\n",
      "\r\n",
      "Early stopping à l'époque 49 (patience: 16)\r\n",
      "✅ Meilleur modèle Random chargé (époque 33, val_loss: 0.4389)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. InternetService      (CAT): 0.1766\r\n",
      "   2. Contract             (CAT): 0.1359\r\n",
      "   3. OnlineSecurity       (CAT): 0.1347\r\n",
      "   4. TechSupport          (CAT): 0.1233\r\n",
      "   5. MultipleLines        (CAT): 0.1207\r\n",
      "   6. Dependents           (CAT): 0.1107\r\n",
      "   7. MonthlyCharges       (NUM): 0.0781\r\n",
      "   8. DeviceProtection     (CAT): 0.0623\r\n",
      "   9. SeniorCitizen        (CAT): 0.0577\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.1766\r\n",
      "   2. Contract            : 0.1359\r\n",
      "   3. OnlineSecurity      : 0.1347\r\n",
      "   4. TechSupport         : 0.1233\r\n",
      "   5. MultipleLines       : 0.1207\r\n",
      "   6. Dependents          : 0.1107\r\n",
      "   7. MonthlyCharges      : 0.0781\r\n",
      "   8. DeviceProtection    : 0.0623\r\n",
      "   9. SeniorCitizen       : 0.0577\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_0/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 347.1s ===\r\n",
      "\u001b[32m[I 2025-07-19 18:30:10,286]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 2, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.26648852816008434, 'ffn_dropout_stage1': 0.14246782213565523, 'residual_dropout_stage1': 0.11818249672071007, 'lr_stage1': 5.415244119402538e-05, 'weight_decay_stage1': 3.320559103751961e-05, 'd_token_stage2': 128, 'n_blocks_stage2': 2, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.10929008254399955, 'ffn_dropout_stage2': 0.22150897038028766, 'residual_dropout_stage2': 0.11705241236872915, 'lr_stage2': 1.8205657658407255e-05, 'weight_decay_stage2': 0.055517216852447225, 'batch_size': 32, 'patience': 16, 'embedding_type': 'T-LR', 'M': 9, 'k': 6}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:   4%|▍         | 1/25 [17:13<6:53:14, 1033.09s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 405,121 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4558 | Val Loss: 0.4339 | Time: 9.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4339)\r\n",
      "Epoch 001 | Train Loss: 0.4373 | Val Loss: 0.4229 | Time: 9.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4229)\r\n",
      "Epoch 002 | Train Loss: 0.4287 | Val Loss: 0.4152 | Time: 9.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4152)\r\n",
      "Epoch 003 | Train Loss: 0.4231 | Val Loss: 0.4129 | Time: 9.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 004 | Train Loss: 0.4228 | Val Loss: 0.4111 | Time: 9.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4111)\r\n",
      "Epoch 005 | Train Loss: 0.4171 | Val Loss: 0.4187 | Time: 9.86s\r\n",
      "Epoch 006 | Train Loss: 0.4200 | Val Loss: 0.4158 | Time: 9.76s\r\n",
      "Epoch 007 | Train Loss: 0.4173 | Val Loss: 0.4145 | Time: 9.76s\r\n",
      "Epoch 008 | Train Loss: 0.4143 | Val Loss: 0.4174 | Time: 9.92s\r\n",
      "Epoch 009 | Train Loss: 0.4147 | Val Loss: 0.4180 | Time: 9.73s\r\n",
      "Epoch 010 | Train Loss: 0.4143 | Val Loss: 0.4252 | Time: 9.68s\r\n",
      "Epoch 011 | Train Loss: 0.4114 | Val Loss: 0.4254 | Time: 9.86s\r\n",
      "Epoch 012 | Train Loss: 0.4094 | Val Loss: 0.4273 | Time: 9.73s\r\n",
      "Epoch 013 | Train Loss: 0.4099 | Val Loss: 0.4226 | Time: 9.70s\r\n",
      "Epoch 014 | Train Loss: 0.4106 | Val Loss: 0.4270 | Time: 9.72s\r\n",
      "Epoch 015 | Train Loss: 0.4078 | Val Loss: 0.4222 | Time: 10.02s\r\n",
      "Epoch 016 | Train Loss: 0.4065 | Val Loss: 0.4151 | Time: 9.70s\r\n",
      "Epoch 017 | Train Loss: 0.4051 | Val Loss: 0.4244 | Time: 9.77s\r\n",
      "Epoch 018 | Train Loss: 0.4029 | Val Loss: 0.4147 | Time: 9.93s\r\n",
      "Epoch 019 | Train Loss: 0.4018 | Val Loss: 0.4252 | Time: 9.76s\r\n",
      "Epoch 020 | Train Loss: 0.4000 | Val Loss: 0.4238 | Time: 9.62s\r\n",
      "Epoch 021 | Train Loss: 0.3973 | Val Loss: 0.4277 | Time: 9.68s\r\n",
      "Epoch 022 | Train Loss: 0.3975 | Val Loss: 0.4318 | Time: 9.75s\r\n",
      "Epoch 023 | Train Loss: 0.3989 | Val Loss: 0.4237 | Time: 9.86s\r\n",
      "Epoch 024 | Train Loss: 0.3959 | Val Loss: 0.4200 | Time: 9.87s\r\n",
      "\r\n",
      "Early stopping à l'époque 24 (patience: 20)\r\n",
      "✅ Meilleur modèle chargé (époque 4, val_loss: 0.4111)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.0539\r\n",
      "   2. OnlineSecurity      : 0.0537\r\n",
      "   3. MonthlyCharges      : 0.0536\r\n",
      "   4. DeviceProtection    : 0.0536\r\n",
      "   5. Contract            : 0.0534\r\n",
      "   6. MultipleLines       : 0.0532\r\n",
      "   7. TechSupport         : 0.0527\r\n",
      "   8. Partner             : 0.0527\r\n",
      "   9. TotalCharges        : 0.0526\r\n",
      "  10. PaperlessBilling    : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 17 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. InternetService      (CAT): 0.0539\r\n",
      "   2. OnlineSecurity       (CAT): 0.0537\r\n",
      "   3. MonthlyCharges       (NUM): 0.0536\r\n",
      "   4. DeviceProtection     (CAT): 0.0536\r\n",
      "   5. Contract             (CAT): 0.0534\r\n",
      "   6. MultipleLines        (CAT): 0.0532\r\n",
      "   7. TechSupport          (CAT): 0.0527\r\n",
      "   8. Partner              (CAT): 0.0527\r\n",
      "   9. TotalCharges         (NUM): 0.0526\r\n",
      "  10. PaperlessBilling     (CAT): 0.0525\r\n",
      "  11. PaymentMethod        (CAT): 0.0525\r\n",
      "  12. OnlineBackup         (CAT): 0.0524\r\n",
      "  13. gender               (CAT): 0.0524\r\n",
      "  14. Dependents           (CAT): 0.0523\r\n",
      "  15. StreamingTV          (CAT): 0.0522\r\n",
      "  16. tenure               (NUM): 0.0522\r\n",
      "  17. SeniorCitizen        (CAT): 0.0520\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges', 'tenure'] → indices [1, 2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['InternetService', 'OnlineSecurity', 'DeviceProtection', 'Contract', 'MultipleLines', 'TechSupport', 'Partner', 'PaperlessBilling', 'PaymentMethod', 'OnlineBackup', 'gender', 'Dependents', 'StreamingTV', 'SeniorCitizen'] → indices [6, 7, 9, 13, 5, 10, 2, 14, 15, 8, 0, 3, 11, 1]\r\n",
      "📊 Features sélectionnées: 3 numériques, 14 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 403,713 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.80%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5235 | Val Loss: 0.4920 | Time: 9.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4920)\r\n",
      "Epoch 001 | Train Loss: 0.5028 | Val Loss: 0.5142 | Time: 9.37s\r\n",
      "Epoch 002 | Train Loss: 0.5376 | Val Loss: 0.5689 | Time: 9.35s\r\n",
      "Epoch 003 | Train Loss: 0.4952 | Val Loss: 0.5116 | Time: 9.49s\r\n",
      "Epoch 004 | Train Loss: 0.4884 | Val Loss: 0.5076 | Time: 9.41s\r\n",
      "Epoch 005 | Train Loss: 0.4866 | Val Loss: 0.5200 | Time: 9.37s\r\n",
      "Epoch 006 | Train Loss: 0.4972 | Val Loss: 0.4965 | Time: 9.48s\r\n",
      "Epoch 007 | Train Loss: 0.4984 | Val Loss: 0.5052 | Time: 9.41s\r\n",
      "Epoch 008 | Train Loss: 0.4903 | Val Loss: 0.4880 | Time: 9.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4880)\r\n",
      "Epoch 009 | Train Loss: 0.4845 | Val Loss: 0.4823 | Time: 9.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4823)\r\n",
      "Epoch 010 | Train Loss: 0.4943 | Val Loss: 0.4939 | Time: 9.36s\r\n",
      "Epoch 011 | Train Loss: 0.4835 | Val Loss: 0.4801 | Time: 9.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4801)\r\n",
      "Epoch 012 | Train Loss: 0.4842 | Val Loss: 0.4819 | Time: 9.37s\r\n",
      "Epoch 013 | Train Loss: 0.4743 | Val Loss: 0.4672 | Time: 9.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4672)\r\n",
      "Epoch 014 | Train Loss: 0.4749 | Val Loss: 0.5060 | Time: 9.36s\r\n",
      "Epoch 015 | Train Loss: 0.4725 | Val Loss: 0.4795 | Time: 9.38s\r\n",
      "Epoch 016 | Train Loss: 0.4685 | Val Loss: 0.4633 | Time: 9.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4633)\r\n",
      "Epoch 017 | Train Loss: 0.4734 | Val Loss: 0.4824 | Time: 9.37s\r\n",
      "Epoch 018 | Train Loss: 0.4749 | Val Loss: 0.4788 | Time: 9.35s\r\n",
      "Epoch 019 | Train Loss: 0.5003 | Val Loss: 0.4993 | Time: 9.43s\r\n",
      "Epoch 020 | Train Loss: 0.4908 | Val Loss: 0.4881 | Time: 9.32s\r\n",
      "Epoch 021 | Train Loss: 0.4794 | Val Loss: 0.4875 | Time: 9.33s\r\n",
      "Epoch 022 | Train Loss: 0.4745 | Val Loss: 0.4743 | Time: 9.34s\r\n",
      "Epoch 023 | Train Loss: 0.4707 | Val Loss: 0.4570 | Time: 9.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4570)\r\n",
      "Epoch 024 | Train Loss: 0.4718 | Val Loss: 0.4685 | Time: 9.41s\r\n",
      "Epoch 025 | Train Loss: 0.4658 | Val Loss: 0.4760 | Time: 9.33s\r\n",
      "Epoch 026 | Train Loss: 0.4617 | Val Loss: 0.4616 | Time: 9.46s\r\n",
      "Epoch 027 | Train Loss: 0.4746 | Val Loss: 0.4792 | Time: 9.38s\r\n",
      "Epoch 028 | Train Loss: 0.4707 | Val Loss: 0.4775 | Time: 9.40s\r\n",
      "Epoch 029 | Train Loss: 0.4726 | Val Loss: 0.4752 | Time: 9.48s\r\n",
      "Epoch 030 | Train Loss: 0.4762 | Val Loss: 0.4696 | Time: 9.48s\r\n",
      "Epoch 031 | Train Loss: 0.4768 | Val Loss: 0.4736 | Time: 9.42s\r\n",
      "Epoch 032 | Train Loss: 0.4562 | Val Loss: 0.4707 | Time: 9.46s\r\n",
      "Epoch 033 | Train Loss: 0.4575 | Val Loss: 0.4649 | Time: 9.56s\r\n",
      "Epoch 034 | Train Loss: 0.4563 | Val Loss: 0.4685 | Time: 9.35s\r\n",
      "Epoch 035 | Train Loss: 0.4594 | Val Loss: 0.4529 | Time: 9.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4529)\r\n",
      "Epoch 036 | Train Loss: 0.4595 | Val Loss: 0.4702 | Time: 9.54s\r\n",
      "Epoch 037 | Train Loss: 0.4582 | Val Loss: 0.4583 | Time: 9.32s\r\n",
      "Epoch 038 | Train Loss: 0.4583 | Val Loss: 0.4635 | Time: 9.63s\r\n",
      "Epoch 039 | Train Loss: 0.4592 | Val Loss: 0.4753 | Time: 9.37s\r\n",
      "Epoch 040 | Train Loss: 0.4920 | Val Loss: 0.4701 | Time: 9.52s\r\n",
      "Epoch 041 | Train Loss: 0.4712 | Val Loss: 0.4685 | Time: 9.81s\r\n",
      "Epoch 042 | Train Loss: 0.4584 | Val Loss: 0.4827 | Time: 9.52s\r\n",
      "Epoch 043 | Train Loss: 0.4573 | Val Loss: 0.4576 | Time: 9.68s\r\n",
      "Epoch 044 | Train Loss: 0.4484 | Val Loss: 0.4527 | Time: 9.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4527)\r\n",
      "Epoch 045 | Train Loss: 0.4600 | Val Loss: 0.4711 | Time: 9.64s\r\n",
      "Epoch 046 | Train Loss: 0.4705 | Val Loss: 0.4618 | Time: 9.75s\r\n",
      "Epoch 047 | Train Loss: 0.4662 | Val Loss: 0.4653 | Time: 9.58s\r\n",
      "Epoch 048 | Train Loss: 0.4628 | Val Loss: 0.4605 | Time: 9.55s\r\n",
      "Epoch 049 | Train Loss: 0.4677 | Val Loss: 0.4599 | Time: 9.63s\r\n",
      "✅ Meilleur modèle Random chargé (époque 44, val_loss: 0.4527)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Partner              (CAT): 0.2176\r\n",
      "   2. SeniorCitizen        (CAT): 0.1649\r\n",
      "   3. tenure               (NUM): 0.1099\r\n",
      "   4. MultipleLines        (CAT): 0.0898\r\n",
      "   5. TotalCharges         (NUM): 0.0764\r\n",
      "   6. TechSupport          (CAT): 0.0638\r\n",
      "   7. DeviceProtection     (CAT): 0.0623\r\n",
      "   8. gender               (CAT): 0.0566\r\n",
      "   9. Contract             (CAT): 0.0323\r\n",
      "  10. OnlineSecurity       (CAT): 0.0299\r\n",
      "  11. Dependents           (CAT): 0.0289\r\n",
      "  12. MonthlyCharges       (NUM): 0.0260\r\n",
      "  13. PaperlessBilling     (CAT): 0.0160\r\n",
      "  14. OnlineBackup         (CAT): 0.0118\r\n",
      "  15. PaymentMethod        (CAT): 0.0095\r\n",
      "  16. StreamingTV          (CAT): 0.0033\r\n",
      "  17. InternetService      (CAT): 0.0012\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Partner             : 0.2176\r\n",
      "   2. SeniorCitizen       : 0.1649\r\n",
      "   3. tenure              : 0.1099\r\n",
      "   4. MultipleLines       : 0.0898\r\n",
      "   5. TotalCharges        : 0.0764\r\n",
      "   6. TechSupport         : 0.0638\r\n",
      "   7. DeviceProtection    : 0.0623\r\n",
      "   8. gender              : 0.0566\r\n",
      "   9. Contract            : 0.0323\r\n",
      "  10. OnlineSecurity      : 0.0299\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 721.7s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 405,121 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4699 | Val Loss: 0.4350 | Time: 9.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4350)\r\n",
      "Epoch 001 | Train Loss: 0.4316 | Val Loss: 0.4254 | Time: 9.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 002 | Train Loss: 0.4208 | Val Loss: 0.4204 | Time: 10.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 003 | Train Loss: 0.4167 | Val Loss: 0.4191 | Time: 9.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 004 | Train Loss: 0.4132 | Val Loss: 0.4207 | Time: 9.86s\r\n",
      "Epoch 005 | Train Loss: 0.4101 | Val Loss: 0.4182 | Time: 9.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4182)\r\n",
      "Epoch 006 | Train Loss: 0.4101 | Val Loss: 0.4189 | Time: 9.75s\r\n",
      "Epoch 007 | Train Loss: 0.4077 | Val Loss: 0.4143 | Time: 9.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4143)\r\n",
      "Epoch 008 | Train Loss: 0.4067 | Val Loss: 0.4172 | Time: 9.78s\r\n",
      "Epoch 009 | Train Loss: 0.4053 | Val Loss: 0.4137 | Time: 9.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 010 | Train Loss: 0.4027 | Val Loss: 0.4160 | Time: 9.74s\r\n",
      "Epoch 011 | Train Loss: 0.4045 | Val Loss: 0.4162 | Time: 9.81s\r\n",
      "Epoch 012 | Train Loss: 0.4011 | Val Loss: 0.4144 | Time: 9.86s\r\n",
      "Epoch 013 | Train Loss: 0.4003 | Val Loss: 0.4130 | Time: 9.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4130)\r\n",
      "Epoch 014 | Train Loss: 0.4001 | Val Loss: 0.4130 | Time: 9.72s\r\n",
      "Epoch 015 | Train Loss: 0.4005 | Val Loss: 0.4147 | Time: 9.89s\r\n",
      "Epoch 016 | Train Loss: 0.3998 | Val Loss: 0.4164 | Time: 9.80s\r\n",
      "Epoch 017 | Train Loss: 0.3980 | Val Loss: 0.4146 | Time: 9.81s\r\n",
      "Epoch 018 | Train Loss: 0.3960 | Val Loss: 0.4159 | Time: 9.88s\r\n",
      "Epoch 019 | Train Loss: 0.3941 | Val Loss: 0.4172 | Time: 9.76s\r\n",
      "Epoch 020 | Train Loss: 0.3952 | Val Loss: 0.4180 | Time: 9.74s\r\n",
      "Epoch 021 | Train Loss: 0.3930 | Val Loss: 0.4187 | Time: 9.75s\r\n",
      "Epoch 022 | Train Loss: 0.3930 | Val Loss: 0.4215 | Time: 9.87s\r\n",
      "Epoch 023 | Train Loss: 0.3888 | Val Loss: 0.4208 | Time: 9.78s\r\n",
      "Epoch 024 | Train Loss: 0.3883 | Val Loss: 0.4215 | Time: 9.82s\r\n",
      "Epoch 025 | Train Loss: 0.3899 | Val Loss: 0.4261 | Time: 9.93s\r\n",
      "Epoch 026 | Train Loss: 0.3851 | Val Loss: 0.4246 | Time: 9.81s\r\n",
      "Epoch 027 | Train Loss: 0.3828 | Val Loss: 0.4277 | Time: 9.79s\r\n",
      "Epoch 028 | Train Loss: 0.3855 | Val Loss: 0.4254 | Time: 9.84s\r\n",
      "Epoch 029 | Train Loss: 0.3839 | Val Loss: 0.4240 | Time: 9.82s\r\n",
      "Epoch 030 | Train Loss: 0.3842 | Val Loss: 0.4243 | Time: 9.76s\r\n",
      "Epoch 031 | Train Loss: 0.3791 | Val Loss: 0.4333 | Time: 9.89s\r\n",
      "Epoch 032 | Train Loss: 0.3807 | Val Loss: 0.4326 | Time: 9.79s\r\n",
      "Epoch 033 | Train Loss: 0.3772 | Val Loss: 0.4317 | Time: 9.89s\r\n",
      "\r\n",
      "Early stopping à l'époque 33 (patience: 20)\r\n",
      "✅ Meilleur modèle chargé (époque 13, val_loss: 0.4130)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Dependents          : 0.0540\r\n",
      "   2. SeniorCitizen       : 0.0539\r\n",
      "   3. PaymentMethod       : 0.0538\r\n",
      "   4. InternetService     : 0.0537\r\n",
      "   5. OnlineBackup        : 0.0536\r\n",
      "   6. StreamingTV         : 0.0533\r\n",
      "   7. StreamingMovies     : 0.0533\r\n",
      "   8. PhoneService        : 0.0532\r\n",
      "   9. TechSupport         : 0.0529\r\n",
      "  10. gender              : 0.0527\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 17 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Dependents           (CAT): 0.0540\r\n",
      "   2. SeniorCitizen        (CAT): 0.0539\r\n",
      "   3. PaymentMethod        (CAT): 0.0538\r\n",
      "   4. InternetService      (CAT): 0.0537\r\n",
      "   5. OnlineBackup         (CAT): 0.0536\r\n",
      "   6. StreamingTV          (CAT): 0.0533\r\n",
      "   7. StreamingMovies      (CAT): 0.0533\r\n",
      "   8. PhoneService         (CAT): 0.0532\r\n",
      "   9. TechSupport          (CAT): 0.0529\r\n",
      "  10. gender               (CAT): 0.0527\r\n",
      "  11. OnlineSecurity       (CAT): 0.0526\r\n",
      "  12. TotalCharges         (NUM): 0.0522\r\n",
      "  13. tenure               (NUM): 0.0522\r\n",
      "  14. MonthlyCharges       (NUM): 0.0522\r\n",
      "  15. Partner              (CAT): 0.0522\r\n",
      "  16. PaperlessBilling     (CAT): 0.0521\r\n",
      "  17. DeviceProtection     (CAT): 0.0514\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure', 'MonthlyCharges'] → indices [2, 0, 1]\r\n",
      "   - Catégorielles sélectionnées: ['Dependents', 'SeniorCitizen', 'PaymentMethod', 'InternetService', 'OnlineBackup', 'StreamingTV', 'StreamingMovies', 'PhoneService', 'TechSupport', 'gender', 'OnlineSecurity', 'Partner', 'PaperlessBilling', 'DeviceProtection'] → indices [3, 1, 15, 6, 8, 11, 12, 4, 10, 0, 7, 2, 14, 9]\r\n",
      "📊 Features sélectionnées: 3 numériques, 14 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 403,649 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.80%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5881 | Val Loss: 0.5865 | Time: 9.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5865)\r\n",
      "Epoch 001 | Train Loss: 0.5760 | Val Loss: 0.5647 | Time: 9.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5647)\r\n",
      "Epoch 002 | Train Loss: 0.5756 | Val Loss: 0.5783 | Time: 9.45s\r\n",
      "Epoch 003 | Train Loss: 0.5806 | Val Loss: 0.5792 | Time: 9.40s\r\n",
      "Epoch 004 | Train Loss: 0.5785 | Val Loss: 0.5776 | Time: 9.45s\r\n",
      "Epoch 005 | Train Loss: 0.5739 | Val Loss: 0.5727 | Time: 9.39s\r\n",
      "Epoch 006 | Train Loss: 0.5673 | Val Loss: 0.5739 | Time: 9.38s\r\n",
      "Epoch 007 | Train Loss: 0.5623 | Val Loss: 0.5420 | Time: 9.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5420)\r\n",
      "Epoch 008 | Train Loss: 0.5685 | Val Loss: 0.5793 | Time: 9.38s\r\n",
      "Epoch 009 | Train Loss: 0.5800 | Val Loss: 0.5819 | Time: 9.50s\r\n",
      "Epoch 010 | Train Loss: 0.5800 | Val Loss: 0.5804 | Time: 9.41s\r\n",
      "Epoch 011 | Train Loss: 0.5797 | Val Loss: 0.5797 | Time: 9.51s\r\n",
      "Epoch 012 | Train Loss: 0.5799 | Val Loss: 0.5795 | Time: 9.40s\r\n",
      "Epoch 013 | Train Loss: 0.5795 | Val Loss: 0.5791 | Time: 9.39s\r\n",
      "Epoch 014 | Train Loss: 0.5796 | Val Loss: 0.5791 | Time: 9.43s\r\n",
      "Epoch 015 | Train Loss: 0.5794 | Val Loss: 0.5789 | Time: 9.43s\r\n",
      "Epoch 016 | Train Loss: 0.5793 | Val Loss: 0.5789 | Time: 9.37s\r\n",
      "Epoch 017 | Train Loss: 0.5794 | Val Loss: 0.5789 | Time: 9.50s\r\n",
      "Epoch 018 | Train Loss: 0.5790 | Val Loss: 0.5805 | Time: 9.38s\r\n",
      "Epoch 019 | Train Loss: 0.5785 | Val Loss: 0.5787 | Time: 9.39s\r\n",
      "Epoch 020 | Train Loss: 0.5785 | Val Loss: 0.5788 | Time: 9.35s\r\n",
      "Epoch 021 | Train Loss: 0.5793 | Val Loss: 0.5788 | Time: 9.47s\r\n",
      "Epoch 022 | Train Loss: 0.5796 | Val Loss: 0.5788 | Time: 9.38s\r\n",
      "Epoch 023 | Train Loss: 0.5794 | Val Loss: 0.5788 | Time: 9.37s\r\n",
      "Epoch 024 | Train Loss: 0.5794 | Val Loss: 0.5788 | Time: 9.47s\r\n",
      "Epoch 025 | Train Loss: 0.5794 | Val Loss: 0.5788 | Time: 9.32s\r\n",
      "Epoch 026 | Train Loss: 0.5794 | Val Loss: 0.5788 | Time: 9.41s\r\n",
      "Epoch 027 | Train Loss: 0.5793 | Val Loss: 0.5788 | Time: 9.47s\r\n",
      "\r\n",
      "Early stopping à l'époque 27 (patience: 20)\r\n",
      "✅ Meilleur modèle Random chargé (époque 7, val_loss: 0.5420)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. MonthlyCharges       (NUM): 0.0848\r\n",
      "   2. StreamingTV          (CAT): 0.0693\r\n",
      "   3. OnlineBackup         (CAT): 0.0692\r\n",
      "   4. StreamingMovies      (CAT): 0.0627\r\n",
      "   5. InternetService      (CAT): 0.0595\r\n",
      "   6. PaperlessBilling     (CAT): 0.0586\r\n",
      "   7. TechSupport          (CAT): 0.0578\r\n",
      "   8. Dependents           (CAT): 0.0566\r\n",
      "   9. PaymentMethod        (CAT): 0.0562\r\n",
      "  10. gender               (CAT): 0.0532\r\n",
      "  11. tenure               (NUM): 0.0532\r\n",
      "  12. OnlineSecurity       (CAT): 0.0532\r\n",
      "  13. PhoneService         (CAT): 0.0532\r\n",
      "  14. DeviceProtection     (CAT): 0.0532\r\n",
      "  15. TotalCharges         (NUM): 0.0532\r\n",
      "  16. SeniorCitizen        (CAT): 0.0532\r\n",
      "  17. Partner              (CAT): 0.0531\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.0848\r\n",
      "   2. StreamingTV         : 0.0693\r\n",
      "   3. OnlineBackup        : 0.0692\r\n",
      "   4. StreamingMovies     : 0.0627\r\n",
      "   5. InternetService     : 0.0595\r\n",
      "   6. PaperlessBilling    : 0.0586\r\n",
      "   7. TechSupport         : 0.0578\r\n",
      "   8. Dependents          : 0.0566\r\n",
      "   9. PaymentMethod       : 0.0562\r\n",
      "  10. gender              : 0.0532\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 601.9s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 405,121 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4727 | Val Loss: 0.4518 | Time: 10.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4518)\r\n",
      "Epoch 001 | Train Loss: 0.4369 | Val Loss: 0.4314 | Time: 9.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4314)\r\n",
      "Epoch 002 | Train Loss: 0.4288 | Val Loss: 0.4314 | Time: 10.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4314)\r\n",
      "Epoch 003 | Train Loss: 0.4286 | Val Loss: 0.4239 | Time: 9.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "Epoch 004 | Train Loss: 0.4222 | Val Loss: 0.4319 | Time: 10.08s\r\n",
      "Epoch 005 | Train Loss: 0.4218 | Val Loss: 0.4280 | Time: 10.04s\r\n",
      "Epoch 006 | Train Loss: 0.4174 | Val Loss: 0.4390 | Time: 9.88s\r\n",
      "Epoch 007 | Train Loss: 0.4179 | Val Loss: 0.4252 | Time: 9.86s\r\n",
      "Epoch 008 | Train Loss: 0.4174 | Val Loss: 0.4315 | Time: 9.86s\r\n",
      "Epoch 009 | Train Loss: 0.4136 | Val Loss: 0.4210 | Time: 9.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4210)\r\n",
      "Epoch 010 | Train Loss: 0.4127 | Val Loss: 0.4186 | Time: 9.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 011 | Train Loss: 0.4110 | Val Loss: 0.4193 | Time: 9.82s\r\n",
      "Epoch 012 | Train Loss: 0.4107 | Val Loss: 0.4258 | Time: 9.97s\r\n",
      "Epoch 013 | Train Loss: 0.4084 | Val Loss: 0.4220 | Time: 9.78s\r\n",
      "Epoch 014 | Train Loss: 0.4061 | Val Loss: 0.4268 | Time: 9.90s\r\n",
      "Epoch 015 | Train Loss: 0.4054 | Val Loss: 0.4352 | Time: 9.96s\r\n",
      "Epoch 016 | Train Loss: 0.4035 | Val Loss: 0.4334 | Time: 9.85s\r\n",
      "Epoch 017 | Train Loss: 0.4020 | Val Loss: 0.4343 | Time: 9.86s\r\n",
      "Epoch 018 | Train Loss: 0.4007 | Val Loss: 0.4393 | Time: 10.00s\r\n",
      "Epoch 019 | Train Loss: 0.4054 | Val Loss: 0.4279 | Time: 9.76s\r\n",
      "Epoch 020 | Train Loss: 0.4002 | Val Loss: 0.4336 | Time: 9.93s\r\n",
      "Epoch 021 | Train Loss: 0.3991 | Val Loss: 0.4351 | Time: 10.03s\r\n",
      "Epoch 022 | Train Loss: 0.4000 | Val Loss: 0.4353 | Time: 9.87s\r\n",
      "Epoch 023 | Train Loss: 0.3951 | Val Loss: 0.4326 | Time: 9.85s\r\n",
      "Epoch 024 | Train Loss: 0.3969 | Val Loss: 0.4399 | Time: 9.85s\r\n",
      "Epoch 025 | Train Loss: 0.3945 | Val Loss: 0.4338 | Time: 9.89s\r\n",
      "Epoch 026 | Train Loss: 0.3947 | Val Loss: 0.4396 | Time: 9.74s\r\n",
      "Epoch 027 | Train Loss: 0.3894 | Val Loss: 0.4317 | Time: 9.76s\r\n",
      "Epoch 028 | Train Loss: 0.3902 | Val Loss: 0.4356 | Time: 9.93s\r\n",
      "Epoch 029 | Train Loss: 0.3889 | Val Loss: 0.4347 | Time: 9.83s\r\n",
      "Epoch 030 | Train Loss: 0.3882 | Val Loss: 0.4400 | Time: 9.82s\r\n",
      "\r\n",
      "Early stopping à l'époque 30 (patience: 20)\r\n",
      "✅ Meilleur modèle chargé (époque 10, val_loss: 0.4186)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0547\r\n",
      "   2. MonthlyCharges      : 0.0542\r\n",
      "   3. gender              : 0.0536\r\n",
      "   4. TotalCharges        : 0.0535\r\n",
      "   5. PaymentMethod       : 0.0534\r\n",
      "   6. OnlineBackup        : 0.0534\r\n",
      "   7. Contract            : 0.0531\r\n",
      "   8. PhoneService        : 0.0530\r\n",
      "   9. MultipleLines       : 0.0529\r\n",
      "  10. SeniorCitizen       : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 17 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0547\r\n",
      "   2. MonthlyCharges       (NUM): 0.0542\r\n",
      "   3. gender               (CAT): 0.0536\r\n",
      "   4. TotalCharges         (NUM): 0.0535\r\n",
      "   5. PaymentMethod        (CAT): 0.0534\r\n",
      "   6. OnlineBackup         (CAT): 0.0534\r\n",
      "   7. Contract             (CAT): 0.0531\r\n",
      "   8. PhoneService         (CAT): 0.0530\r\n",
      "   9. MultipleLines        (CAT): 0.0529\r\n",
      "  10. SeniorCitizen        (CAT): 0.0525\r\n",
      "  11. Partner              (CAT): 0.0524\r\n",
      "  12. StreamingTV          (CAT): 0.0523\r\n",
      "  13. OnlineSecurity       (CAT): 0.0523\r\n",
      "  14. Dependents           (CAT): 0.0521\r\n",
      "  15. TechSupport          (CAT): 0.0515\r\n",
      "  16. StreamingMovies      (CAT): 0.0515\r\n",
      "  17. tenure               (NUM): 0.0513\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges', 'tenure'] → indices [1, 2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'gender', 'PaymentMethod', 'OnlineBackup', 'Contract', 'PhoneService', 'MultipleLines', 'SeniorCitizen', 'Partner', 'StreamingTV', 'OnlineSecurity', 'Dependents', 'TechSupport', 'StreamingMovies'] → indices [14, 0, 15, 8, 13, 4, 5, 1, 2, 11, 7, 3, 10, 12]\r\n",
      "📊 Features sélectionnées: 3 numériques, 14 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 403,649 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.80%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5775 | Val Loss: 0.5910 | Time: 9.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5910)\r\n",
      "Epoch 001 | Train Loss: 0.5583 | Val Loss: 0.6034 | Time: 9.33s\r\n",
      "Epoch 002 | Train Loss: 0.5648 | Val Loss: 0.6025 | Time: 9.42s\r\n",
      "Epoch 003 | Train Loss: 0.5647 | Val Loss: 0.5551 | Time: 9.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5551)\r\n",
      "Epoch 004 | Train Loss: 0.5604 | Val Loss: 0.5938 | Time: 9.39s\r\n",
      "Epoch 005 | Train Loss: 0.5493 | Val Loss: 0.6304 | Time: 9.37s\r\n",
      "Epoch 006 | Train Loss: 0.5587 | Val Loss: 0.5841 | Time: 9.45s\r\n",
      "Epoch 007 | Train Loss: 0.5636 | Val Loss: 0.5669 | Time: 9.50s\r\n",
      "Epoch 008 | Train Loss: 0.5433 | Val Loss: 0.5994 | Time: 9.48s\r\n",
      "Epoch 009 | Train Loss: 0.5522 | Val Loss: 0.5886 | Time: 9.49s\r\n",
      "Epoch 010 | Train Loss: 0.5355 | Val Loss: 0.5260 | Time: 9.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5260)\r\n",
      "Epoch 011 | Train Loss: 0.5552 | Val Loss: 0.5991 | Time: 9.42s\r\n",
      "Epoch 012 | Train Loss: 0.5629 | Val Loss: 0.5772 | Time: 9.42s\r\n",
      "Epoch 013 | Train Loss: 0.5634 | Val Loss: 0.5781 | Time: 9.50s\r\n",
      "Epoch 014 | Train Loss: 0.5623 | Val Loss: 0.5681 | Time: 9.45s\r\n",
      "Epoch 015 | Train Loss: 0.5616 | Val Loss: 0.5663 | Time: 9.45s\r\n",
      "Epoch 016 | Train Loss: 0.5608 | Val Loss: 0.5702 | Time: 9.52s\r\n",
      "Epoch 017 | Train Loss: 0.5620 | Val Loss: 0.5726 | Time: 9.43s\r\n",
      "Epoch 018 | Train Loss: 0.5628 | Val Loss: 0.5706 | Time: 9.44s\r\n",
      "Epoch 019 | Train Loss: 0.5619 | Val Loss: 0.5764 | Time: 9.41s\r\n",
      "Epoch 020 | Train Loss: 0.5599 | Val Loss: 0.5776 | Time: 9.52s\r\n",
      "Epoch 021 | Train Loss: 0.5637 | Val Loss: 0.5713 | Time: 9.35s\r\n",
      "Epoch 022 | Train Loss: 0.5624 | Val Loss: 0.5670 | Time: 9.41s\r\n",
      "Epoch 023 | Train Loss: 0.5623 | Val Loss: 0.5680 | Time: 9.76s\r\n",
      "Epoch 024 | Train Loss: 0.5666 | Val Loss: 0.5820 | Time: 9.44s\r\n",
      "Epoch 025 | Train Loss: 0.5625 | Val Loss: 0.5748 | Time: 9.40s\r\n",
      "Epoch 026 | Train Loss: 0.5642 | Val Loss: 0.5687 | Time: 9.39s\r\n",
      "Epoch 027 | Train Loss: 0.5611 | Val Loss: 0.5862 | Time: 9.39s\r\n",
      "Epoch 028 | Train Loss: 0.5803 | Val Loss: 0.5788 | Time: 9.42s\r\n",
      "Epoch 029 | Train Loss: 0.5793 | Val Loss: 0.5790 | Time: 9.45s\r\n",
      "Epoch 030 | Train Loss: 0.5599 | Val Loss: 0.5621 | Time: 9.51s\r\n",
      "\r\n",
      "Early stopping à l'époque 30 (patience: 20)\r\n",
      "✅ Meilleur modèle Random chargé (époque 10, val_loss: 0.5260)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. MonthlyCharges       (NUM): 0.0659\r\n",
      "   2. TechSupport          (CAT): 0.0639\r\n",
      "   3. OnlineBackup         (CAT): 0.0637\r\n",
      "   4. StreamingTV          (CAT): 0.0630\r\n",
      "   5. PhoneService         (CAT): 0.0621\r\n",
      "   6. gender               (CAT): 0.0619\r\n",
      "   7. MultipleLines        (CAT): 0.0617\r\n",
      "   8. SeniorCitizen        (CAT): 0.0611\r\n",
      "   9. StreamingMovies      (CAT): 0.0597\r\n",
      "  10. OnlineSecurity       (CAT): 0.0583\r\n",
      "  11. Dependents           (CAT): 0.0557\r\n",
      "  12. tenure               (NUM): 0.0539\r\n",
      "  13. PaymentMethod        (CAT): 0.0539\r\n",
      "  14. PaperlessBilling     (CAT): 0.0538\r\n",
      "  15. Partner              (CAT): 0.0538\r\n",
      "  16. Contract             (CAT): 0.0538\r\n",
      "  17. TotalCharges         (NUM): 0.0538\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.0659\r\n",
      "   2. TechSupport         : 0.0639\r\n",
      "   3. OnlineBackup        : 0.0637\r\n",
      "   4. StreamingTV         : 0.0630\r\n",
      "   5. PhoneService        : 0.0621\r\n",
      "   6. gender              : 0.0619\r\n",
      "   7. MultipleLines       : 0.0617\r\n",
      "   8. SeniorCitizen       : 0.0611\r\n",
      "   9. StreamingMovies     : 0.0597\r\n",
      "  10. OnlineSecurity      : 0.0583\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_1/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 604.2s ===\r\n",
      "\u001b[32m[I 2025-07-19 19:02:18,724]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 6, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.1777354579378964, 'ffn_dropout_stage1': 0.1542698063547792, 'residual_dropout_stage1': 0.18287375091519295, 'lr_stage1': 0.00026730883107816685, 'weight_decay_stage1': 2.539057572102415e-05, 'd_token_stage2': 64, 'n_blocks_stage2': 6, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.11480893034681808, 'ffn_dropout_stage2': 0.17169314570885452, 'residual_dropout_stage2': 0.11158690595251297, 'lr_stage2': 0.028340904295147733, 'weight_decay_stage2': 0.0013076473382928546, 'batch_size': 32, 'patience': 20, 'embedding_type': 'Q', 'M': 17, 'k': 6}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:   8%|▊         | 2/25 [49:21<9:57:54, 1559.77s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6152 | Val Loss: 0.5564 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5564)\r\n",
      "Epoch 001 | Train Loss: 0.5404 | Val Loss: 0.5068 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5068)\r\n",
      "Epoch 002 | Train Loss: 0.5085 | Val Loss: 0.4828 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4828)\r\n",
      "Epoch 003 | Train Loss: 0.4919 | Val Loss: 0.4702 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4702)\r\n",
      "Epoch 004 | Train Loss: 0.4796 | Val Loss: 0.4618 | Time: 6.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4618)\r\n",
      "Epoch 005 | Train Loss: 0.4673 | Val Loss: 0.4556 | Time: 5.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4556)\r\n",
      "Epoch 006 | Train Loss: 0.4652 | Val Loss: 0.4493 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4493)\r\n",
      "Epoch 007 | Train Loss: 0.4613 | Val Loss: 0.4454 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4454)\r\n",
      "Epoch 008 | Train Loss: 0.4597 | Val Loss: 0.4429 | Time: 5.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4429)\r\n",
      "Epoch 009 | Train Loss: 0.4549 | Val Loss: 0.4411 | Time: 6.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4411)\r\n",
      "Epoch 010 | Train Loss: 0.4554 | Val Loss: 0.4374 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4374)\r\n",
      "Epoch 011 | Train Loss: 0.4504 | Val Loss: 0.4345 | Time: 5.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 012 | Train Loss: 0.4453 | Val Loss: 0.4320 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4320)\r\n",
      "Epoch 013 | Train Loss: 0.4462 | Val Loss: 0.4305 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4305)\r\n",
      "Epoch 014 | Train Loss: 0.4457 | Val Loss: 0.4290 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4290)\r\n",
      "Epoch 015 | Train Loss: 0.4457 | Val Loss: 0.4275 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4275)\r\n",
      "Epoch 016 | Train Loss: 0.4427 | Val Loss: 0.4269 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4269)\r\n",
      "Epoch 017 | Train Loss: 0.4396 | Val Loss: 0.4255 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 018 | Train Loss: 0.4362 | Val Loss: 0.4237 | Time: 5.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4237)\r\n",
      "Epoch 019 | Train Loss: 0.4372 | Val Loss: 0.4225 | Time: 6.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4225)\r\n",
      "Epoch 020 | Train Loss: 0.4388 | Val Loss: 0.4228 | Time: 6.13s\r\n",
      "Epoch 021 | Train Loss: 0.4359 | Val Loss: 0.4221 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4221)\r\n",
      "Epoch 022 | Train Loss: 0.4336 | Val Loss: 0.4222 | Time: 6.03s\r\n",
      "Epoch 023 | Train Loss: 0.4338 | Val Loss: 0.4208 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4208)\r\n",
      "Epoch 024 | Train Loss: 0.4327 | Val Loss: 0.4204 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 025 | Train Loss: 0.4319 | Val Loss: 0.4205 | Time: 6.03s\r\n",
      "Epoch 026 | Train Loss: 0.4305 | Val Loss: 0.4196 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4196)\r\n",
      "Epoch 027 | Train Loss: 0.4279 | Val Loss: 0.4190 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 028 | Train Loss: 0.4303 | Val Loss: 0.4201 | Time: 6.16s\r\n",
      "Epoch 029 | Train Loss: 0.4299 | Val Loss: 0.4197 | Time: 6.15s\r\n",
      "Epoch 030 | Train Loss: 0.4285 | Val Loss: 0.4193 | Time: 6.05s\r\n",
      "Epoch 031 | Train Loss: 0.4286 | Val Loss: 0.4194 | Time: 6.07s\r\n",
      "Epoch 032 | Train Loss: 0.4286 | Val Loss: 0.4183 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 033 | Train Loss: 0.4283 | Val Loss: 0.4188 | Time: 5.96s\r\n",
      "Epoch 034 | Train Loss: 0.4264 | Val Loss: 0.4182 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4182)\r\n",
      "Epoch 035 | Train Loss: 0.4223 | Val Loss: 0.4187 | Time: 6.08s\r\n",
      "Epoch 036 | Train Loss: 0.4284 | Val Loss: 0.4194 | Time: 6.03s\r\n",
      "Epoch 037 | Train Loss: 0.4249 | Val Loss: 0.4189 | Time: 5.99s\r\n",
      "Epoch 038 | Train Loss: 0.4260 | Val Loss: 0.4179 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4179)\r\n",
      "Epoch 039 | Train Loss: 0.4248 | Val Loss: 0.4174 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "Epoch 040 | Train Loss: 0.4250 | Val Loss: 0.4171 | Time: 6.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4171)\r\n",
      "Epoch 041 | Train Loss: 0.4236 | Val Loss: 0.4164 | Time: 6.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4164)\r\n",
      "Epoch 042 | Train Loss: 0.4220 | Val Loss: 0.4169 | Time: 6.00s\r\n",
      "Epoch 043 | Train Loss: 0.4250 | Val Loss: 0.4173 | Time: 6.00s\r\n",
      "Epoch 044 | Train Loss: 0.4236 | Val Loss: 0.4172 | Time: 6.03s\r\n",
      "Epoch 045 | Train Loss: 0.4233 | Val Loss: 0.4166 | Time: 5.95s\r\n",
      "Epoch 046 | Train Loss: 0.4238 | Val Loss: 0.4170 | Time: 5.97s\r\n",
      "Epoch 047 | Train Loss: 0.4246 | Val Loss: 0.4169 | Time: 5.91s\r\n",
      "Epoch 048 | Train Loss: 0.4213 | Val Loss: 0.4164 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4164)\r\n",
      "Epoch 049 | Train Loss: 0.4232 | Val Loss: 0.4170 | Time: 5.94s\r\n",
      "✅ Meilleur modèle chargé (époque 48, val_loss: 0.4164)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingMovies     : 0.0628\r\n",
      "   2. PaperlessBilling    : 0.0622\r\n",
      "   3. OnlineSecurity      : 0.0606\r\n",
      "   4. tenure              : 0.0604\r\n",
      "   5. OnlineBackup        : 0.0595\r\n",
      "   6. InternetService     : 0.0594\r\n",
      "   7. DeviceProtection    : 0.0584\r\n",
      "   8. TotalCharges        : 0.0550\r\n",
      "   9. Contract            : 0.0535\r\n",
      "  10. gender              : 0.0526\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. StreamingMovies      (CAT): 0.0628\r\n",
      "   2. PaperlessBilling     (CAT): 0.0622\r\n",
      "   3. OnlineSecurity       (CAT): 0.0606\r\n",
      "   4. tenure               (NUM): 0.0604\r\n",
      "   5. OnlineBackup         (CAT): 0.0595\r\n",
      "   6. InternetService      (CAT): 0.0594\r\n",
      "   7. DeviceProtection     (CAT): 0.0584\r\n",
      "   8. TotalCharges         (NUM): 0.0550\r\n",
      "   9. Contract             (CAT): 0.0535\r\n",
      "  10. gender               (CAT): 0.0526\r\n",
      "  11. Dependents           (CAT): 0.0503\r\n",
      "  12. StreamingTV          (CAT): 0.0495\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges'] → indices [0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['StreamingMovies', 'PaperlessBilling', 'OnlineSecurity', 'OnlineBackup', 'InternetService', 'DeviceProtection', 'Contract', 'gender', 'Dependents', 'StreamingTV'] → indices [12, 14, 7, 8, 6, 9, 13, 0, 3, 11]\r\n",
      "📊 Features sélectionnées: 2 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 61,633 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.07%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6518 | Val Loss: 0.5739 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5739)\r\n",
      "Epoch 001 | Train Loss: 0.5966 | Val Loss: 0.5404 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5404)\r\n",
      "Epoch 002 | Train Loss: 0.5712 | Val Loss: 0.5271 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5271)\r\n",
      "Epoch 003 | Train Loss: 0.5545 | Val Loss: 0.5166 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5166)\r\n",
      "Epoch 004 | Train Loss: 0.5421 | Val Loss: 0.5049 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5049)\r\n",
      "Epoch 005 | Train Loss: 0.5306 | Val Loss: 0.4927 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4927)\r\n",
      "Epoch 006 | Train Loss: 0.5201 | Val Loss: 0.4831 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4831)\r\n",
      "Epoch 007 | Train Loss: 0.5118 | Val Loss: 0.4739 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4739)\r\n",
      "Epoch 008 | Train Loss: 0.5084 | Val Loss: 0.4670 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4670)\r\n",
      "Epoch 009 | Train Loss: 0.5009 | Val Loss: 0.4613 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4613)\r\n",
      "Epoch 010 | Train Loss: 0.5014 | Val Loss: 0.4560 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4560)\r\n",
      "Epoch 011 | Train Loss: 0.4935 | Val Loss: 0.4521 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4521)\r\n",
      "Epoch 012 | Train Loss: 0.4940 | Val Loss: 0.4493 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4493)\r\n",
      "Epoch 013 | Train Loss: 0.4877 | Val Loss: 0.4466 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4466)\r\n",
      "Epoch 014 | Train Loss: 0.4830 | Val Loss: 0.4448 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4448)\r\n",
      "Epoch 015 | Train Loss: 0.4871 | Val Loss: 0.4439 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4439)\r\n",
      "Epoch 016 | Train Loss: 0.4806 | Val Loss: 0.4429 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4429)\r\n",
      "Epoch 017 | Train Loss: 0.4729 | Val Loss: 0.4417 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 018 | Train Loss: 0.4756 | Val Loss: 0.4411 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4411)\r\n",
      "Epoch 019 | Train Loss: 0.4757 | Val Loss: 0.4409 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4409)\r\n",
      "Epoch 020 | Train Loss: 0.4741 | Val Loss: 0.4407 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4407)\r\n",
      "Epoch 021 | Train Loss: 0.4699 | Val Loss: 0.4407 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4407)\r\n",
      "Epoch 022 | Train Loss: 0.4712 | Val Loss: 0.4404 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4404)\r\n",
      "Epoch 023 | Train Loss: 0.4726 | Val Loss: 0.4406 | Time: 3.33s\r\n",
      "Epoch 024 | Train Loss: 0.4710 | Val Loss: 0.4399 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4399)\r\n",
      "Epoch 025 | Train Loss: 0.4680 | Val Loss: 0.4398 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4398)\r\n",
      "Epoch 026 | Train Loss: 0.4690 | Val Loss: 0.4393 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4393)\r\n",
      "Epoch 027 | Train Loss: 0.4688 | Val Loss: 0.4384 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4384)\r\n",
      "Epoch 028 | Train Loss: 0.4710 | Val Loss: 0.4385 | Time: 3.29s\r\n",
      "Epoch 029 | Train Loss: 0.4648 | Val Loss: 0.4376 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4376)\r\n",
      "Epoch 030 | Train Loss: 0.4671 | Val Loss: 0.4391 | Time: 3.31s\r\n",
      "Epoch 031 | Train Loss: 0.4656 | Val Loss: 0.4382 | Time: 3.52s\r\n",
      "Epoch 032 | Train Loss: 0.4664 | Val Loss: 0.4368 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 033 | Train Loss: 0.4643 | Val Loss: 0.4368 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 034 | Train Loss: 0.4648 | Val Loss: 0.4363 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4363)\r\n",
      "Epoch 035 | Train Loss: 0.4686 | Val Loss: 0.4353 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4353)\r\n",
      "Epoch 036 | Train Loss: 0.4624 | Val Loss: 0.4351 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4351)\r\n",
      "Epoch 037 | Train Loss: 0.4604 | Val Loss: 0.4354 | Time: 3.34s\r\n",
      "Epoch 038 | Train Loss: 0.4640 | Val Loss: 0.4348 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4348)\r\n",
      "Epoch 039 | Train Loss: 0.4618 | Val Loss: 0.4351 | Time: 3.33s\r\n",
      "Epoch 040 | Train Loss: 0.4637 | Val Loss: 0.4347 | Time: 3.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4347)\r\n",
      "Epoch 041 | Train Loss: 0.4650 | Val Loss: 0.4342 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 042 | Train Loss: 0.4691 | Val Loss: 0.4342 | Time: 3.27s\r\n",
      "Epoch 043 | Train Loss: 0.4591 | Val Loss: 0.4338 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4338)\r\n",
      "Epoch 044 | Train Loss: 0.4556 | Val Loss: 0.4336 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4336)\r\n",
      "Epoch 045 | Train Loss: 0.4548 | Val Loss: 0.4337 | Time: 3.30s\r\n",
      "Epoch 046 | Train Loss: 0.4567 | Val Loss: 0.4332 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4332)\r\n",
      "Epoch 047 | Train Loss: 0.4590 | Val Loss: 0.4329 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4329)\r\n",
      "Epoch 048 | Train Loss: 0.4555 | Val Loss: 0.4328 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4328)\r\n",
      "Epoch 049 | Train Loss: 0.4639 | Val Loss: 0.4322 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4322)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4322)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. DeviceProtection     (CAT): 0.2075\r\n",
      "   2. Contract             (CAT): 0.1417\r\n",
      "   3. StreamingMovies      (CAT): 0.0931\r\n",
      "   4. StreamingTV          (CAT): 0.0800\r\n",
      "   5. gender               (CAT): 0.0737\r\n",
      "   6. tenure               (NUM): 0.0597\r\n",
      "   7. OnlineBackup         (CAT): 0.0587\r\n",
      "   8. PaperlessBilling     (CAT): 0.0575\r\n",
      "   9. InternetService      (CAT): 0.0575\r\n",
      "  10. OnlineSecurity       (CAT): 0.0573\r\n",
      "  11. TotalCharges         (NUM): 0.0567\r\n",
      "  12. Dependents           (CAT): 0.0565\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.2075\r\n",
      "   2. Contract            : 0.1417\r\n",
      "   3. StreamingMovies     : 0.0931\r\n",
      "   4. StreamingTV         : 0.0800\r\n",
      "   5. gender              : 0.0737\r\n",
      "   6. tenure              : 0.0597\r\n",
      "   7. OnlineBackup        : 0.0587\r\n",
      "   8. PaperlessBilling    : 0.0575\r\n",
      "   9. InternetService     : 0.0575\r\n",
      "  10. OnlineSecurity      : 0.0573\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 469.6s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6004 | Val Loss: 0.5103 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5103)\r\n",
      "Epoch 001 | Train Loss: 0.5222 | Val Loss: 0.4864 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4864)\r\n",
      "Epoch 002 | Train Loss: 0.5020 | Val Loss: 0.4743 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4743)\r\n",
      "Epoch 003 | Train Loss: 0.4893 | Val Loss: 0.4668 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4668)\r\n",
      "Epoch 004 | Train Loss: 0.4806 | Val Loss: 0.4591 | Time: 5.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4591)\r\n",
      "Epoch 005 | Train Loss: 0.4709 | Val Loss: 0.4547 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4547)\r\n",
      "Epoch 006 | Train Loss: 0.4666 | Val Loss: 0.4491 | Time: 5.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4491)\r\n",
      "Epoch 007 | Train Loss: 0.4619 | Val Loss: 0.4470 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4470)\r\n",
      "Epoch 008 | Train Loss: 0.4592 | Val Loss: 0.4441 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4441)\r\n",
      "Epoch 009 | Train Loss: 0.4522 | Val Loss: 0.4413 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4413)\r\n",
      "Epoch 010 | Train Loss: 0.4518 | Val Loss: 0.4405 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4405)\r\n",
      "Epoch 011 | Train Loss: 0.4477 | Val Loss: 0.4382 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4382)\r\n",
      "Epoch 012 | Train Loss: 0.4459 | Val Loss: 0.4372 | Time: 5.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4372)\r\n",
      "Epoch 013 | Train Loss: 0.4408 | Val Loss: 0.4364 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 014 | Train Loss: 0.4434 | Val Loss: 0.4350 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4350)\r\n",
      "Epoch 015 | Train Loss: 0.4417 | Val Loss: 0.4354 | Time: 6.03s\r\n",
      "Epoch 016 | Train Loss: 0.4399 | Val Loss: 0.4350 | Time: 5.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4350)\r\n",
      "Epoch 017 | Train Loss: 0.4304 | Val Loss: 0.4332 | Time: 6.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4332)\r\n",
      "Epoch 018 | Train Loss: 0.4294 | Val Loss: 0.4328 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4328)\r\n",
      "Epoch 019 | Train Loss: 0.4334 | Val Loss: 0.4318 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4318)\r\n",
      "Epoch 020 | Train Loss: 0.4346 | Val Loss: 0.4309 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4309)\r\n",
      "Epoch 021 | Train Loss: 0.4337 | Val Loss: 0.4302 | Time: 6.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4302)\r\n",
      "Epoch 022 | Train Loss: 0.4315 | Val Loss: 0.4289 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 023 | Train Loss: 0.4291 | Val Loss: 0.4279 | Time: 5.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4279)\r\n",
      "Epoch 024 | Train Loss: 0.4291 | Val Loss: 0.4285 | Time: 5.93s\r\n",
      "Epoch 025 | Train Loss: 0.4269 | Val Loss: 0.4285 | Time: 5.91s\r\n",
      "Epoch 026 | Train Loss: 0.4267 | Val Loss: 0.4281 | Time: 6.03s\r\n",
      "Epoch 027 | Train Loss: 0.4271 | Val Loss: 0.4269 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4269)\r\n",
      "Epoch 028 | Train Loss: 0.4259 | Val Loss: 0.4253 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4253)\r\n",
      "Epoch 029 | Train Loss: 0.4226 | Val Loss: 0.4256 | Time: 5.96s\r\n",
      "Epoch 030 | Train Loss: 0.4228 | Val Loss: 0.4244 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4244)\r\n",
      "Epoch 031 | Train Loss: 0.4227 | Val Loss: 0.4248 | Time: 6.19s\r\n",
      "Epoch 032 | Train Loss: 0.4207 | Val Loss: 0.4239 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "Epoch 033 | Train Loss: 0.4197 | Val Loss: 0.4235 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 034 | Train Loss: 0.4216 | Val Loss: 0.4230 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4230)\r\n",
      "Epoch 035 | Train Loss: 0.4219 | Val Loss: 0.4218 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4218)\r\n",
      "Epoch 036 | Train Loss: 0.4189 | Val Loss: 0.4221 | Time: 5.84s\r\n",
      "Epoch 037 | Train Loss: 0.4211 | Val Loss: 0.4217 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 038 | Train Loss: 0.4168 | Val Loss: 0.4211 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4211)\r\n",
      "Epoch 039 | Train Loss: 0.4177 | Val Loss: 0.4216 | Time: 5.85s\r\n",
      "Epoch 040 | Train Loss: 0.4172 | Val Loss: 0.4203 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4203)\r\n",
      "Epoch 041 | Train Loss: 0.4172 | Val Loss: 0.4205 | Time: 5.88s\r\n",
      "Epoch 042 | Train Loss: 0.4178 | Val Loss: 0.4201 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4201)\r\n",
      "Epoch 043 | Train Loss: 0.4185 | Val Loss: 0.4189 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4189)\r\n",
      "Epoch 044 | Train Loss: 0.4165 | Val Loss: 0.4189 | Time: 5.98s\r\n",
      "Epoch 045 | Train Loss: 0.4156 | Val Loss: 0.4184 | Time: 6.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 046 | Train Loss: 0.4134 | Val Loss: 0.4182 | Time: 5.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4182)\r\n",
      "Epoch 047 | Train Loss: 0.4146 | Val Loss: 0.4181 | Time: 5.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4181)\r\n",
      "Epoch 048 | Train Loss: 0.4136 | Val Loss: 0.4177 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4177)\r\n",
      "Epoch 049 | Train Loss: 0.4124 | Val Loss: 0.4174 | Time: 6.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4174)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.0878\r\n",
      "   2. OnlineBackup        : 0.0770\r\n",
      "   3. Dependents          : 0.0733\r\n",
      "   4. OnlineSecurity      : 0.0722\r\n",
      "   5. tenure              : 0.0704\r\n",
      "   6. PaperlessBilling    : 0.0598\r\n",
      "   7. MultipleLines       : 0.0508\r\n",
      "   8. InternetService     : 0.0503\r\n",
      "   9. Contract            : 0.0469\r\n",
      "  10. MonthlyCharges      : 0.0448\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. gender               (CAT): 0.0878\r\n",
      "   2. OnlineBackup         (CAT): 0.0770\r\n",
      "   3. Dependents           (CAT): 0.0733\r\n",
      "   4. OnlineSecurity       (CAT): 0.0722\r\n",
      "   5. tenure               (NUM): 0.0704\r\n",
      "   6. PaperlessBilling     (CAT): 0.0598\r\n",
      "   7. MultipleLines        (CAT): 0.0508\r\n",
      "   8. InternetService      (CAT): 0.0503\r\n",
      "   9. Contract             (CAT): 0.0469\r\n",
      "  10. MonthlyCharges       (NUM): 0.0448\r\n",
      "  11. PhoneService         (CAT): 0.0446\r\n",
      "  12. StreamingMovies      (CAT): 0.0439\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'MonthlyCharges'] → indices [0, 1]\r\n",
      "   - Catégorielles sélectionnées: ['gender', 'OnlineBackup', 'Dependents', 'OnlineSecurity', 'PaperlessBilling', 'MultipleLines', 'InternetService', 'Contract', 'PhoneService', 'StreamingMovies'] → indices [0, 8, 3, 7, 14, 5, 6, 13, 4, 12]\r\n",
      "📊 Features sélectionnées: 2 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 61,569 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.07%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6509 | Val Loss: 0.5816 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5816)\r\n",
      "Epoch 001 | Train Loss: 0.5896 | Val Loss: 0.5411 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5411)\r\n",
      "Epoch 002 | Train Loss: 0.5570 | Val Loss: 0.5218 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5218)\r\n",
      "Epoch 003 | Train Loss: 0.5376 | Val Loss: 0.5103 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5103)\r\n",
      "Epoch 004 | Train Loss: 0.5249 | Val Loss: 0.5018 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5018)\r\n",
      "Epoch 005 | Train Loss: 0.5172 | Val Loss: 0.4939 | Time: 3.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4939)\r\n",
      "Epoch 006 | Train Loss: 0.5062 | Val Loss: 0.4868 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4868)\r\n",
      "Epoch 007 | Train Loss: 0.5005 | Val Loss: 0.4820 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4820)\r\n",
      "Epoch 008 | Train Loss: 0.4965 | Val Loss: 0.4783 | Time: 3.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4783)\r\n",
      "Epoch 009 | Train Loss: 0.4934 | Val Loss: 0.4761 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4761)\r\n",
      "Epoch 010 | Train Loss: 0.4864 | Val Loss: 0.4741 | Time: 3.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4741)\r\n",
      "Epoch 011 | Train Loss: 0.4865 | Val Loss: 0.4725 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4725)\r\n",
      "Epoch 012 | Train Loss: 0.4799 | Val Loss: 0.4715 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4715)\r\n",
      "Epoch 013 | Train Loss: 0.4799 | Val Loss: 0.4698 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4698)\r\n",
      "Epoch 014 | Train Loss: 0.4823 | Val Loss: 0.4685 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4685)\r\n",
      "Epoch 015 | Train Loss: 0.4786 | Val Loss: 0.4666 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4666)\r\n",
      "Epoch 016 | Train Loss: 0.4744 | Val Loss: 0.4654 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4654)\r\n",
      "Epoch 017 | Train Loss: 0.4695 | Val Loss: 0.4638 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4638)\r\n",
      "Epoch 018 | Train Loss: 0.4721 | Val Loss: 0.4616 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4616)\r\n",
      "Epoch 019 | Train Loss: 0.4688 | Val Loss: 0.4604 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4604)\r\n",
      "Epoch 020 | Train Loss: 0.4694 | Val Loss: 0.4592 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4592)\r\n",
      "Epoch 021 | Train Loss: 0.4641 | Val Loss: 0.4579 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4579)\r\n",
      "Epoch 022 | Train Loss: 0.4653 | Val Loss: 0.4561 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4561)\r\n",
      "Epoch 023 | Train Loss: 0.4653 | Val Loss: 0.4547 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4547)\r\n",
      "Epoch 024 | Train Loss: 0.4663 | Val Loss: 0.4531 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4531)\r\n",
      "Epoch 025 | Train Loss: 0.4658 | Val Loss: 0.4517 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4517)\r\n",
      "Epoch 026 | Train Loss: 0.4619 | Val Loss: 0.4508 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4508)\r\n",
      "Epoch 027 | Train Loss: 0.4632 | Val Loss: 0.4499 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4499)\r\n",
      "Epoch 028 | Train Loss: 0.4556 | Val Loss: 0.4489 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4489)\r\n",
      "Epoch 029 | Train Loss: 0.4587 | Val Loss: 0.4485 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4485)\r\n",
      "Epoch 030 | Train Loss: 0.4571 | Val Loss: 0.4480 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4480)\r\n",
      "Epoch 031 | Train Loss: 0.4625 | Val Loss: 0.4475 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4475)\r\n",
      "Epoch 032 | Train Loss: 0.4527 | Val Loss: 0.4467 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4467)\r\n",
      "Epoch 033 | Train Loss: 0.4544 | Val Loss: 0.4465 | Time: 3.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4465)\r\n",
      "Epoch 034 | Train Loss: 0.4536 | Val Loss: 0.4459 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4459)\r\n",
      "Epoch 035 | Train Loss: 0.4558 | Val Loss: 0.4447 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4447)\r\n",
      "Epoch 036 | Train Loss: 0.4512 | Val Loss: 0.4429 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4429)\r\n",
      "Epoch 037 | Train Loss: 0.4552 | Val Loss: 0.4417 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 038 | Train Loss: 0.4521 | Val Loss: 0.4408 | Time: 3.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4408)\r\n",
      "Epoch 039 | Train Loss: 0.4562 | Val Loss: 0.4406 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4406)\r\n",
      "Epoch 040 | Train Loss: 0.4457 | Val Loss: 0.4400 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4400)\r\n",
      "Epoch 041 | Train Loss: 0.4491 | Val Loss: 0.4392 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4392)\r\n",
      "Epoch 042 | Train Loss: 0.4487 | Val Loss: 0.4392 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4392)\r\n",
      "Epoch 043 | Train Loss: 0.4473 | Val Loss: 0.4386 | Time: 3.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4386)\r\n",
      "Epoch 044 | Train Loss: 0.4479 | Val Loss: 0.4394 | Time: 3.37s\r\n",
      "Epoch 045 | Train Loss: 0.4506 | Val Loss: 0.4384 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4384)\r\n",
      "Epoch 046 | Train Loss: 0.4452 | Val Loss: 0.4380 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4380)\r\n",
      "Epoch 047 | Train Loss: 0.4475 | Val Loss: 0.4386 | Time: 3.36s\r\n",
      "Epoch 048 | Train Loss: 0.4470 | Val Loss: 0.4386 | Time: 3.27s\r\n",
      "Epoch 049 | Train Loss: 0.4449 | Val Loss: 0.4376 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4376)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4376)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PaperlessBilling     (CAT): 0.2220\r\n",
      "   2. StreamingMovies      (CAT): 0.1212\r\n",
      "   3. MonthlyCharges       (NUM): 0.1035\r\n",
      "   4. OnlineSecurity       (CAT): 0.0953\r\n",
      "   5. OnlineBackup         (CAT): 0.0741\r\n",
      "   6. Dependents           (CAT): 0.0641\r\n",
      "   7. MultipleLines        (CAT): 0.0633\r\n",
      "   8. InternetService      (CAT): 0.0523\r\n",
      "   9. PhoneService         (CAT): 0.0515\r\n",
      "  10. gender               (CAT): 0.0515\r\n",
      "  11. Contract             (CAT): 0.0509\r\n",
      "  12. tenure               (NUM): 0.0502\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.2220\r\n",
      "   2. StreamingMovies     : 0.1212\r\n",
      "   3. MonthlyCharges      : 0.1035\r\n",
      "   4. OnlineSecurity      : 0.0953\r\n",
      "   5. OnlineBackup        : 0.0741\r\n",
      "   6. Dependents          : 0.0641\r\n",
      "   7. MultipleLines       : 0.0633\r\n",
      "   8. InternetService     : 0.0523\r\n",
      "   9. PhoneService        : 0.0515\r\n",
      "  10. gender              : 0.0515\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 467.6s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6673 | Val Loss: 0.5776 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5776)\r\n",
      "Epoch 001 | Train Loss: 0.5690 | Val Loss: 0.5420 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5420)\r\n",
      "Epoch 002 | Train Loss: 0.5384 | Val Loss: 0.5058 | Time: 5.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5058)\r\n",
      "Epoch 003 | Train Loss: 0.5125 | Val Loss: 0.4838 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4838)\r\n",
      "Epoch 004 | Train Loss: 0.4967 | Val Loss: 0.4722 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4722)\r\n",
      "Epoch 005 | Train Loss: 0.4832 | Val Loss: 0.4650 | Time: 5.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4650)\r\n",
      "Epoch 006 | Train Loss: 0.4759 | Val Loss: 0.4604 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4604)\r\n",
      "Epoch 007 | Train Loss: 0.4726 | Val Loss: 0.4557 | Time: 5.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4557)\r\n",
      "Epoch 008 | Train Loss: 0.4630 | Val Loss: 0.4510 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4510)\r\n",
      "Epoch 009 | Train Loss: 0.4631 | Val Loss: 0.4457 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4457)\r\n",
      "Epoch 010 | Train Loss: 0.4590 | Val Loss: 0.4430 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4430)\r\n",
      "Epoch 011 | Train Loss: 0.4498 | Val Loss: 0.4399 | Time: 6.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4399)\r\n",
      "Epoch 012 | Train Loss: 0.4515 | Val Loss: 0.4373 | Time: 5.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4373)\r\n",
      "Epoch 013 | Train Loss: 0.4473 | Val Loss: 0.4356 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4356)\r\n",
      "Epoch 014 | Train Loss: 0.4480 | Val Loss: 0.4327 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 015 | Train Loss: 0.4401 | Val Loss: 0.4315 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4315)\r\n",
      "Epoch 016 | Train Loss: 0.4408 | Val Loss: 0.4310 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4310)\r\n",
      "Epoch 017 | Train Loss: 0.4418 | Val Loss: 0.4292 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4292)\r\n",
      "Epoch 018 | Train Loss: 0.4373 | Val Loss: 0.4283 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 019 | Train Loss: 0.4360 | Val Loss: 0.4262 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4262)\r\n",
      "Epoch 020 | Train Loss: 0.4341 | Val Loss: 0.4253 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4253)\r\n",
      "Epoch 021 | Train Loss: 0.4328 | Val Loss: 0.4248 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4248)\r\n",
      "Epoch 022 | Train Loss: 0.4349 | Val Loss: 0.4222 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 023 | Train Loss: 0.4331 | Val Loss: 0.4214 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 024 | Train Loss: 0.4315 | Val Loss: 0.4216 | Time: 5.85s\r\n",
      "Epoch 025 | Train Loss: 0.4289 | Val Loss: 0.4209 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 026 | Train Loss: 0.4291 | Val Loss: 0.4197 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 027 | Train Loss: 0.4273 | Val Loss: 0.4191 | Time: 6.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 028 | Train Loss: 0.4274 | Val Loss: 0.4180 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 029 | Train Loss: 0.4266 | Val Loss: 0.4184 | Time: 5.84s\r\n",
      "Epoch 030 | Train Loss: 0.4277 | Val Loss: 0.4168 | Time: 5.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4168)\r\n",
      "Epoch 031 | Train Loss: 0.4230 | Val Loss: 0.4167 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4167)\r\n",
      "Epoch 032 | Train Loss: 0.4255 | Val Loss: 0.4163 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4163)\r\n",
      "Epoch 033 | Train Loss: 0.4265 | Val Loss: 0.4164 | Time: 6.06s\r\n",
      "Epoch 034 | Train Loss: 0.4251 | Val Loss: 0.4159 | Time: 5.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 035 | Train Loss: 0.4240 | Val Loss: 0.4150 | Time: 5.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4150)\r\n",
      "Epoch 036 | Train Loss: 0.4227 | Val Loss: 0.4150 | Time: 5.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4150)\r\n",
      "Epoch 037 | Train Loss: 0.4221 | Val Loss: 0.4145 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4145)\r\n",
      "Epoch 038 | Train Loss: 0.4237 | Val Loss: 0.4131 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4131)\r\n",
      "Epoch 039 | Train Loss: 0.4212 | Val Loss: 0.4129 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 040 | Train Loss: 0.4208 | Val Loss: 0.4141 | Time: 5.84s\r\n",
      "Epoch 041 | Train Loss: 0.4211 | Val Loss: 0.4132 | Time: 5.85s\r\n",
      "Epoch 042 | Train Loss: 0.4205 | Val Loss: 0.4141 | Time: 5.88s\r\n",
      "Epoch 043 | Train Loss: 0.4179 | Val Loss: 0.4140 | Time: 5.90s\r\n",
      "Epoch 044 | Train Loss: 0.4175 | Val Loss: 0.4123 | Time: 6.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4123)\r\n",
      "Epoch 045 | Train Loss: 0.4177 | Val Loss: 0.4137 | Time: 5.85s\r\n",
      "Epoch 046 | Train Loss: 0.4194 | Val Loss: 0.4122 | Time: 5.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4122)\r\n",
      "Epoch 047 | Train Loss: 0.4190 | Val Loss: 0.4122 | Time: 5.90s\r\n",
      "Epoch 048 | Train Loss: 0.4213 | Val Loss: 0.4118 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4118)\r\n",
      "Epoch 049 | Train Loss: 0.4178 | Val Loss: 0.4120 | Time: 5.90s\r\n",
      "✅ Meilleur modèle chargé (époque 48, val_loss: 0.4118)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0659\r\n",
      "   2. PhoneService        : 0.0632\r\n",
      "   3. OnlineBackup        : 0.0578\r\n",
      "   4. tenure              : 0.0577\r\n",
      "   5. InternetService     : 0.0548\r\n",
      "   6. StreamingMovies     : 0.0546\r\n",
      "   7. MonthlyCharges      : 0.0534\r\n",
      "   8. TotalCharges        : 0.0516\r\n",
      "   9. MultipleLines       : 0.0514\r\n",
      "  10. OnlineSecurity      : 0.0504\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0659\r\n",
      "   2. PhoneService         (CAT): 0.0632\r\n",
      "   3. OnlineBackup         (CAT): 0.0578\r\n",
      "   4. tenure               (NUM): 0.0577\r\n",
      "   5. InternetService      (CAT): 0.0548\r\n",
      "   6. StreamingMovies      (CAT): 0.0546\r\n",
      "   7. MonthlyCharges       (NUM): 0.0534\r\n",
      "   8. TotalCharges         (NUM): 0.0516\r\n",
      "   9. MultipleLines        (CAT): 0.0514\r\n",
      "  10. OnlineSecurity       (CAT): 0.0504\r\n",
      "  11. SeniorCitizen        (CAT): 0.0504\r\n",
      "  12. gender               (CAT): 0.0502\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'MonthlyCharges', 'TotalCharges'] → indices [0, 1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'PhoneService', 'OnlineBackup', 'InternetService', 'StreamingMovies', 'MultipleLines', 'OnlineSecurity', 'SeniorCitizen', 'gender'] → indices [15, 4, 8, 6, 12, 5, 7, 1, 0]\r\n",
      "📊 Features sélectionnées: 3 numériques, 9 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 61,569 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.07%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6490 | Val Loss: 0.5950 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5950)\r\n",
      "Epoch 001 | Train Loss: 0.6039 | Val Loss: 0.5510 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5510)\r\n",
      "Epoch 002 | Train Loss: 0.5761 | Val Loss: 0.5283 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5283)\r\n",
      "Epoch 003 | Train Loss: 0.5636 | Val Loss: 0.5115 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5115)\r\n",
      "Epoch 004 | Train Loss: 0.5513 | Val Loss: 0.4966 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4966)\r\n",
      "Epoch 005 | Train Loss: 0.5401 | Val Loss: 0.4830 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4830)\r\n",
      "Epoch 006 | Train Loss: 0.5344 | Val Loss: 0.4714 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4714)\r\n",
      "Epoch 007 | Train Loss: 0.5277 | Val Loss: 0.4618 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4618)\r\n",
      "Epoch 008 | Train Loss: 0.5228 | Val Loss: 0.4557 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4557)\r\n",
      "Epoch 009 | Train Loss: 0.5205 | Val Loss: 0.4512 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4512)\r\n",
      "Epoch 010 | Train Loss: 0.5158 | Val Loss: 0.4478 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4478)\r\n",
      "Epoch 011 | Train Loss: 0.5075 | Val Loss: 0.4449 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4449)\r\n",
      "Epoch 012 | Train Loss: 0.5056 | Val Loss: 0.4424 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4424)\r\n",
      "Epoch 013 | Train Loss: 0.5018 | Val Loss: 0.4402 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4402)\r\n",
      "Epoch 014 | Train Loss: 0.4996 | Val Loss: 0.4377 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4377)\r\n",
      "Epoch 015 | Train Loss: 0.4988 | Val Loss: 0.4350 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4350)\r\n",
      "Epoch 016 | Train Loss: 0.4963 | Val Loss: 0.4332 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4332)\r\n",
      "Epoch 017 | Train Loss: 0.4956 | Val Loss: 0.4318 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4318)\r\n",
      "Epoch 018 | Train Loss: 0.4937 | Val Loss: 0.4307 | Time: 3.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 019 | Train Loss: 0.4887 | Val Loss: 0.4297 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 020 | Train Loss: 0.4862 | Val Loss: 0.4296 | Time: 3.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 021 | Train Loss: 0.4869 | Val Loss: 0.4293 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 022 | Train Loss: 0.4832 | Val Loss: 0.4294 | Time: 3.30s\r\n",
      "Epoch 023 | Train Loss: 0.4831 | Val Loss: 0.4300 | Time: 3.30s\r\n",
      "Epoch 024 | Train Loss: 0.4803 | Val Loss: 0.4302 | Time: 3.35s\r\n",
      "Epoch 025 | Train Loss: 0.4859 | Val Loss: 0.4297 | Time: 3.30s\r\n",
      "Epoch 026 | Train Loss: 0.4820 | Val Loss: 0.4296 | Time: 3.30s\r\n",
      "Epoch 027 | Train Loss: 0.4743 | Val Loss: 0.4300 | Time: 3.50s\r\n",
      "Epoch 028 | Train Loss: 0.4772 | Val Loss: 0.4304 | Time: 3.30s\r\n",
      "Epoch 029 | Train Loss: 0.4685 | Val Loss: 0.4312 | Time: 3.31s\r\n",
      "Epoch 030 | Train Loss: 0.4734 | Val Loss: 0.4316 | Time: 3.30s\r\n",
      "Epoch 031 | Train Loss: 0.4710 | Val Loss: 0.4317 | Time: 3.30s\r\n",
      "Epoch 032 | Train Loss: 0.4733 | Val Loss: 0.4324 | Time: 3.30s\r\n",
      "Epoch 033 | Train Loss: 0.4730 | Val Loss: 0.4324 | Time: 3.33s\r\n",
      "Epoch 034 | Train Loss: 0.4672 | Val Loss: 0.4319 | Time: 3.31s\r\n",
      "Epoch 035 | Train Loss: 0.4729 | Val Loss: 0.4325 | Time: 3.31s\r\n",
      "Epoch 036 | Train Loss: 0.4641 | Val Loss: 0.4333 | Time: 3.44s\r\n",
      "Epoch 037 | Train Loss: 0.4695 | Val Loss: 0.4324 | Time: 3.35s\r\n",
      "Epoch 038 | Train Loss: 0.4652 | Val Loss: 0.4328 | Time: 3.28s\r\n",
      "Epoch 039 | Train Loss: 0.4652 | Val Loss: 0.4331 | Time: 3.30s\r\n",
      "Epoch 040 | Train Loss: 0.4657 | Val Loss: 0.4335 | Time: 3.26s\r\n",
      "Epoch 041 | Train Loss: 0.4711 | Val Loss: 0.4332 | Time: 3.30s\r\n",
      "\r\n",
      "Early stopping à l'époque 41 (patience: 20)\r\n",
      "✅ Meilleur modèle Random chargé (époque 21, val_loss: 0.4293)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. InternetService      (CAT): 0.1761\r\n",
      "   2. PhoneService         (CAT): 0.1493\r\n",
      "   3. gender               (CAT): 0.1347\r\n",
      "   4. MultipleLines        (CAT): 0.1005\r\n",
      "   5. OnlineBackup         (CAT): 0.0753\r\n",
      "   6. tenure               (NUM): 0.0557\r\n",
      "   7. OnlineSecurity       (CAT): 0.0537\r\n",
      "   8. MonthlyCharges       (NUM): 0.0517\r\n",
      "   9. SeniorCitizen        (CAT): 0.0511\r\n",
      "  10. TotalCharges         (NUM): 0.0508\r\n",
      "  11. PaymentMethod        (CAT): 0.0507\r\n",
      "  12. StreamingMovies      (CAT): 0.0502\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.1761\r\n",
      "   2. PhoneService        : 0.1493\r\n",
      "   3. gender              : 0.1347\r\n",
      "   4. MultipleLines       : 0.1005\r\n",
      "   5. OnlineBackup        : 0.0753\r\n",
      "   6. tenure              : 0.0557\r\n",
      "   7. OnlineSecurity      : 0.0537\r\n",
      "   8. MonthlyCharges      : 0.0517\r\n",
      "   9. SeniorCitizen       : 0.0511\r\n",
      "  10. TotalCharges        : 0.0508\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_2/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 438.1s ===\r\n",
      "\u001b[32m[I 2025-07-19 19:25:14,579]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 2, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.1457596330983245, 'ffn_dropout_stage1': 0.1153959819657586, 'residual_dropout_stage1': 0.12897514529137682, 'lr_stage1': 4.414536876494478e-05, 'weight_decay_stage1': 0.0445131431564921, 'd_token_stage2': 64, 'n_blocks_stage2': 2, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.18542155772525126, 'ffn_dropout_stage2': 0.2636029531844986, 'residual_dropout_stage2': 0.18607305832563437, 'lr_stage2': 1.0661259689433896e-05, 'weight_decay_stage2': 0.00035787924535023234, 'batch_size': 32, 'patience': 20, 'embedding_type': 'T-LR', 'M': 12, 'k': 4}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  12%|▉       | 3/25 [1:12:17<9:01:07, 1475.79s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: L\r\n",
      "Modèle FTT+ créé avec 88,001 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5003 | Val Loss: 0.5027 | Time: 2.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5027)\r\n",
      "Epoch 001 | Train Loss: 0.5033 | Val Loss: 0.4883 | Time: 2.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4883)\r\n",
      "Epoch 002 | Train Loss: 0.4853 | Val Loss: 0.5033 | Time: 2.20s\r\n",
      "Epoch 003 | Train Loss: 0.4865 | Val Loss: 0.4886 | Time: 2.25s\r\n",
      "Epoch 004 | Train Loss: 0.4842 | Val Loss: 0.4830 | Time: 2.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4830)\r\n",
      "Epoch 005 | Train Loss: 0.4821 | Val Loss: 0.4737 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4737)\r\n",
      "Epoch 006 | Train Loss: 0.4795 | Val Loss: 0.5020 | Time: 2.21s\r\n",
      "Epoch 007 | Train Loss: 0.4936 | Val Loss: 0.4850 | Time: 2.19s\r\n",
      "Epoch 008 | Train Loss: 0.4880 | Val Loss: 0.4994 | Time: 2.20s\r\n",
      "Epoch 009 | Train Loss: 0.4966 | Val Loss: 0.4807 | Time: 2.23s\r\n",
      "Epoch 010 | Train Loss: 0.4904 | Val Loss: 0.4737 | Time: 2.20s\r\n",
      "Epoch 011 | Train Loss: 0.4838 | Val Loss: 0.4919 | Time: 2.23s\r\n",
      "Epoch 012 | Train Loss: 0.4859 | Val Loss: 0.4762 | Time: 2.25s\r\n",
      "Epoch 013 | Train Loss: 0.4831 | Val Loss: 0.4859 | Time: 2.27s\r\n",
      "Epoch 014 | Train Loss: 0.4847 | Val Loss: 0.4837 | Time: 2.21s\r\n",
      "Epoch 015 | Train Loss: 0.4689 | Val Loss: 0.4503 | Time: 2.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4503)\r\n",
      "Epoch 016 | Train Loss: 0.4699 | Val Loss: 0.4534 | Time: 2.20s\r\n",
      "Epoch 017 | Train Loss: 0.4865 | Val Loss: 0.4656 | Time: 2.21s\r\n",
      "Epoch 018 | Train Loss: 0.5022 | Val Loss: 0.4685 | Time: 2.23s\r\n",
      "Epoch 019 | Train Loss: 0.4892 | Val Loss: 0.4465 | Time: 2.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4465)\r\n",
      "Epoch 020 | Train Loss: 0.4888 | Val Loss: 0.4425 | Time: 2.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 021 | Train Loss: 0.4836 | Val Loss: 0.4644 | Time: 2.21s\r\n",
      "Epoch 022 | Train Loss: 0.4814 | Val Loss: 0.4747 | Time: 2.29s\r\n",
      "Epoch 023 | Train Loss: 0.4802 | Val Loss: 0.4618 | Time: 2.22s\r\n",
      "Epoch 024 | Train Loss: 0.4777 | Val Loss: 0.4477 | Time: 2.21s\r\n",
      "Epoch 025 | Train Loss: 0.4772 | Val Loss: 0.4597 | Time: 2.17s\r\n",
      "Epoch 026 | Train Loss: 0.4724 | Val Loss: 0.4547 | Time: 2.19s\r\n",
      "Epoch 027 | Train Loss: 0.4723 | Val Loss: 0.4493 | Time: 2.19s\r\n",
      "Epoch 028 | Train Loss: 0.4665 | Val Loss: 0.4542 | Time: 2.16s\r\n",
      "Epoch 029 | Train Loss: 0.4779 | Val Loss: 0.4525 | Time: 2.20s\r\n",
      "Epoch 030 | Train Loss: 0.4709 | Val Loss: 0.4734 | Time: 2.21s\r\n",
      "Epoch 031 | Train Loss: 0.4928 | Val Loss: 0.4970 | Time: 2.22s\r\n",
      "Epoch 032 | Train Loss: 0.4840 | Val Loss: 0.4925 | Time: 2.21s\r\n",
      "Epoch 033 | Train Loss: 0.5104 | Val Loss: 0.4809 | Time: 2.20s\r\n",
      "Epoch 034 | Train Loss: 0.5066 | Val Loss: 0.4777 | Time: 2.39s\r\n",
      "Epoch 035 | Train Loss: 0.4916 | Val Loss: 0.4838 | Time: 2.20s\r\n",
      "Epoch 036 | Train Loss: 0.4936 | Val Loss: 0.4927 | Time: 2.25s\r\n",
      "\r\n",
      "Early stopping à l'époque 36 (patience: 16)\r\n",
      "✅ Meilleur modèle chargé (époque 20, val_loss: 0.4425)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.3918\r\n",
      "   2. StreamingMovies     : 0.1232\r\n",
      "   3. Dependents          : 0.1039\r\n",
      "   4. PaperlessBilling    : 0.0834\r\n",
      "   5. SeniorCitizen       : 0.0772\r\n",
      "   6. gender              : 0.0432\r\n",
      "   7. Contract            : 0.0220\r\n",
      "   8. PhoneService        : 0.0200\r\n",
      "   9. DeviceProtection    : 0.0198\r\n",
      "  10. Partner             : 0.0193\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 6 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. InternetService      (CAT): 0.3918\r\n",
      "   2. StreamingMovies      (CAT): 0.1232\r\n",
      "   3. Dependents           (CAT): 0.1039\r\n",
      "   4. PaperlessBilling     (CAT): 0.0834\r\n",
      "   5. SeniorCitizen        (CAT): 0.0772\r\n",
      "   6. gender               (CAT): 0.0432\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['InternetService', 'StreamingMovies', 'Dependents', 'PaperlessBilling', 'SeniorCitizen', 'gender'] → indices [6, 12, 3, 14, 1, 0]\r\n",
      "📊 Features sélectionnées: 0 numériques, 6 catégorielles\r\n",
      "🎲 Interactions aléatoires: 10 paires\r\n",
      "Modèle Random créé avec 64,705 paramètres\r\n",
      "🔗 Sparsité d'attention: 34.69%\r\n",
      "   - Connexions feature-feature: 20\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5441 | Val Loss: 0.5373 | Time: 9.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5373)\r\n",
      "Epoch 001 | Train Loss: 0.5301 | Val Loss: 0.5423 | Time: 9.30s\r\n",
      "Epoch 002 | Train Loss: 0.5236 | Val Loss: 0.5398 | Time: 9.41s\r\n",
      "Epoch 003 | Train Loss: 0.5238 | Val Loss: 0.5294 | Time: 9.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5294)\r\n",
      "Epoch 004 | Train Loss: 0.5214 | Val Loss: 0.5171 | Time: 9.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5171)\r\n",
      "Epoch 005 | Train Loss: 0.5221 | Val Loss: 0.5151 | Time: 9.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5151)\r\n",
      "Epoch 006 | Train Loss: 0.5193 | Val Loss: 0.5314 | Time: 9.44s\r\n",
      "Epoch 007 | Train Loss: 0.5188 | Val Loss: 0.5172 | Time: 9.32s\r\n",
      "Epoch 008 | Train Loss: 0.5205 | Val Loss: 0.5199 | Time: 9.30s\r\n",
      "Epoch 009 | Train Loss: 0.5202 | Val Loss: 0.5151 | Time: 9.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5151)\r\n",
      "Epoch 010 | Train Loss: 0.5180 | Val Loss: 0.5254 | Time: 9.24s\r\n",
      "Epoch 011 | Train Loss: 0.5188 | Val Loss: 0.5228 | Time: 9.18s\r\n",
      "Epoch 012 | Train Loss: 0.5208 | Val Loss: 0.5200 | Time: 9.34s\r\n",
      "Epoch 013 | Train Loss: 0.5193 | Val Loss: 0.5183 | Time: 9.25s\r\n",
      "Epoch 014 | Train Loss: 0.5172 | Val Loss: 0.5317 | Time: 9.24s\r\n",
      "Epoch 015 | Train Loss: 0.5197 | Val Loss: 0.5308 | Time: 9.29s\r\n",
      "Epoch 016 | Train Loss: 0.5195 | Val Loss: 0.5218 | Time: 9.35s\r\n",
      "Epoch 017 | Train Loss: 0.5169 | Val Loss: 0.5213 | Time: 9.33s\r\n",
      "Epoch 018 | Train Loss: 0.5189 | Val Loss: 0.5186 | Time: 9.16s\r\n",
      "Epoch 019 | Train Loss: 0.5180 | Val Loss: 0.5207 | Time: 9.35s\r\n",
      "Epoch 020 | Train Loss: 0.5170 | Val Loss: 0.5252 | Time: 9.28s\r\n",
      "Epoch 021 | Train Loss: 0.5203 | Val Loss: 0.5207 | Time: 9.19s\r\n",
      "Epoch 022 | Train Loss: 0.5162 | Val Loss: 0.5157 | Time: 9.32s\r\n",
      "Epoch 023 | Train Loss: 0.5197 | Val Loss: 0.5140 | Time: 9.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5140)\r\n",
      "Epoch 024 | Train Loss: 0.5272 | Val Loss: 0.5334 | Time: 9.27s\r\n",
      "Epoch 025 | Train Loss: 0.5327 | Val Loss: 0.5207 | Time: 9.30s\r\n",
      "Epoch 026 | Train Loss: 0.5260 | Val Loss: 0.5185 | Time: 9.48s\r\n",
      "Epoch 027 | Train Loss: 0.5263 | Val Loss: 0.5151 | Time: 9.32s\r\n",
      "Epoch 028 | Train Loss: 0.5192 | Val Loss: 0.5123 | Time: 9.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5123)\r\n",
      "Epoch 029 | Train Loss: 0.5189 | Val Loss: 0.5265 | Time: 9.42s\r\n",
      "Epoch 030 | Train Loss: 0.5200 | Val Loss: 0.5287 | Time: 9.17s\r\n",
      "Epoch 031 | Train Loss: 0.5209 | Val Loss: 0.5143 | Time: 9.29s\r\n",
      "Epoch 032 | Train Loss: 0.5208 | Val Loss: 0.5139 | Time: 9.21s\r\n",
      "Epoch 033 | Train Loss: 0.5189 | Val Loss: 0.5187 | Time: 9.35s\r\n",
      "Epoch 034 | Train Loss: 0.5189 | Val Loss: 0.5135 | Time: 9.31s\r\n",
      "Epoch 035 | Train Loss: 0.5175 | Val Loss: 0.5137 | Time: 9.32s\r\n",
      "Epoch 036 | Train Loss: 0.5179 | Val Loss: 0.5114 | Time: 9.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5114)\r\n",
      "Epoch 037 | Train Loss: 0.5168 | Val Loss: 0.5128 | Time: 9.28s\r\n",
      "Epoch 038 | Train Loss: 0.5170 | Val Loss: 0.5149 | Time: 9.30s\r\n",
      "Epoch 039 | Train Loss: 0.5172 | Val Loss: 0.5128 | Time: 9.29s\r\n",
      "Epoch 040 | Train Loss: 0.5194 | Val Loss: 0.5097 | Time: 9.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5097)\r\n",
      "Epoch 041 | Train Loss: 0.5179 | Val Loss: 0.5083 | Time: 9.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5083)\r\n",
      "Epoch 042 | Train Loss: 0.5209 | Val Loss: 0.5093 | Time: 9.30s\r\n",
      "Epoch 043 | Train Loss: 0.5210 | Val Loss: 0.5121 | Time: 9.46s\r\n",
      "Epoch 044 | Train Loss: 0.5177 | Val Loss: 0.5123 | Time: 9.31s\r\n",
      "Epoch 045 | Train Loss: 0.5195 | Val Loss: 0.5189 | Time: 9.28s\r\n",
      "Epoch 046 | Train Loss: 0.5197 | Val Loss: 0.5120 | Time: 9.45s\r\n",
      "Epoch 047 | Train Loss: 0.5189 | Val Loss: 0.5157 | Time: 9.16s\r\n",
      "Epoch 048 | Train Loss: 0.5194 | Val Loss: 0.5288 | Time: 9.33s\r\n",
      "Epoch 049 | Train Loss: 0.5239 | Val Loss: 0.5166 | Time: 9.34s\r\n",
      "✅ Meilleur modèle Random chargé (époque 41, val_loss: 0.5083)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.2330\r\n",
      "   2. PaperlessBilling     (CAT): 0.2284\r\n",
      "   3. InternetService      (CAT): 0.1925\r\n",
      "   4. Dependents           (CAT): 0.1880\r\n",
      "   5. StreamingMovies      (CAT): 0.0855\r\n",
      "   6. gender               (CAT): 0.0727\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.2330\r\n",
      "   2. PaperlessBilling    : 0.2284\r\n",
      "   3. InternetService     : 0.1925\r\n",
      "   4. Dependents          : 0.1880\r\n",
      "   5. StreamingMovies     : 0.0855\r\n",
      "   6. gender              : 0.0727\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 551.2s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: L\r\n",
      "Modèle FTT+ créé avec 88,001 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5171 | Val Loss: 0.5195 | Time: 2.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5195)\r\n",
      "Epoch 001 | Train Loss: 0.4973 | Val Loss: 0.4975 | Time: 2.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4975)\r\n",
      "Epoch 002 | Train Loss: 0.4838 | Val Loss: 0.4562 | Time: 2.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4562)\r\n",
      "Epoch 003 | Train Loss: 0.5129 | Val Loss: 0.5009 | Time: 2.20s\r\n",
      "Epoch 004 | Train Loss: 0.4960 | Val Loss: 0.4704 | Time: 2.26s\r\n",
      "Epoch 005 | Train Loss: 0.4838 | Val Loss: 0.4795 | Time: 2.20s\r\n",
      "Epoch 006 | Train Loss: 0.4904 | Val Loss: 0.4749 | Time: 2.22s\r\n",
      "Epoch 007 | Train Loss: 0.4848 | Val Loss: 0.4741 | Time: 2.19s\r\n",
      "Epoch 008 | Train Loss: 0.4869 | Val Loss: 0.4921 | Time: 2.24s\r\n",
      "Epoch 009 | Train Loss: 0.5186 | Val Loss: 0.5094 | Time: 2.22s\r\n",
      "Epoch 010 | Train Loss: 0.5025 | Val Loss: 0.4834 | Time: 2.23s\r\n",
      "Epoch 011 | Train Loss: 0.4824 | Val Loss: 0.4517 | Time: 2.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4517)\r\n",
      "Epoch 012 | Train Loss: 0.4690 | Val Loss: 0.4698 | Time: 2.23s\r\n",
      "Epoch 013 | Train Loss: 0.4773 | Val Loss: 0.4613 | Time: 2.24s\r\n",
      "Epoch 014 | Train Loss: 0.4774 | Val Loss: 0.4660 | Time: 2.34s\r\n",
      "Epoch 015 | Train Loss: 0.4747 | Val Loss: 0.4620 | Time: 2.21s\r\n",
      "Epoch 016 | Train Loss: 0.4759 | Val Loss: 0.4558 | Time: 2.17s\r\n",
      "Epoch 017 | Train Loss: 0.4839 | Val Loss: 0.4724 | Time: 2.24s\r\n",
      "Epoch 018 | Train Loss: 0.4903 | Val Loss: 0.4933 | Time: 2.24s\r\n",
      "Epoch 019 | Train Loss: 0.4791 | Val Loss: 0.4749 | Time: 2.19s\r\n",
      "Epoch 020 | Train Loss: 0.4863 | Val Loss: 0.4774 | Time: 2.18s\r\n",
      "Epoch 021 | Train Loss: 0.4846 | Val Loss: 0.4768 | Time: 2.18s\r\n",
      "Epoch 022 | Train Loss: 0.4881 | Val Loss: 0.4793 | Time: 2.26s\r\n",
      "Epoch 023 | Train Loss: 0.4883 | Val Loss: 0.4858 | Time: 2.21s\r\n",
      "Epoch 024 | Train Loss: 0.4895 | Val Loss: 0.4785 | Time: 2.22s\r\n",
      "Epoch 025 | Train Loss: 0.4834 | Val Loss: 0.4791 | Time: 2.22s\r\n",
      "Epoch 026 | Train Loss: 0.4945 | Val Loss: 0.4869 | Time: 2.24s\r\n",
      "Epoch 027 | Train Loss: 0.4961 | Val Loss: 0.4911 | Time: 2.20s\r\n",
      "\r\n",
      "Early stopping à l'époque 27 (patience: 16)\r\n",
      "✅ Meilleur modèle chargé (époque 11, val_loss: 0.4517)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.1881\r\n",
      "   2. StreamingTV         : 0.1746\r\n",
      "   3. SeniorCitizen       : 0.1185\r\n",
      "   4. PaperlessBilling    : 0.0841\r\n",
      "   5. DeviceProtection    : 0.0762\r\n",
      "   6. StreamingMovies     : 0.0555\r\n",
      "   7. MultipleLines       : 0.0479\r\n",
      "   8. Contract            : 0.0428\r\n",
      "   9. tenure              : 0.0372\r\n",
      "  10. PhoneService        : 0.0367\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 6 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.1881\r\n",
      "   2. StreamingTV          (CAT): 0.1746\r\n",
      "   3. SeniorCitizen        (CAT): 0.1185\r\n",
      "   4. PaperlessBilling     (CAT): 0.0841\r\n",
      "   5. DeviceProtection     (CAT): 0.0762\r\n",
      "   6. StreamingMovies      (CAT): 0.0555\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'StreamingTV', 'SeniorCitizen', 'PaperlessBilling', 'DeviceProtection', 'StreamingMovies'] → indices [15, 11, 1, 14, 9, 12]\r\n",
      "📊 Features sélectionnées: 0 numériques, 6 catégorielles\r\n",
      "🎲 Interactions aléatoires: 10 paires\r\n",
      "Modèle Random créé avec 64,801 paramètres\r\n",
      "🔗 Sparsité d'attention: 34.69%\r\n",
      "   - Connexions feature-feature: 20\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5418 | Val Loss: 0.5232 | Time: 9.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5232)\r\n",
      "Epoch 001 | Train Loss: 0.5174 | Val Loss: 0.5103 | Time: 9.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5103)\r\n",
      "Epoch 002 | Train Loss: 0.5136 | Val Loss: 0.5158 | Time: 9.18s\r\n",
      "Epoch 003 | Train Loss: 0.5096 | Val Loss: 0.5152 | Time: 9.37s\r\n",
      "Epoch 004 | Train Loss: 0.5090 | Val Loss: 0.5242 | Time: 9.42s\r\n",
      "Epoch 005 | Train Loss: 0.5081 | Val Loss: 0.5302 | Time: 9.24s\r\n",
      "Epoch 006 | Train Loss: 0.5097 | Val Loss: 0.5179 | Time: 9.39s\r\n",
      "Epoch 007 | Train Loss: 0.5091 | Val Loss: 0.5202 | Time: 9.29s\r\n",
      "Epoch 008 | Train Loss: 0.5075 | Val Loss: 0.5203 | Time: 9.31s\r\n",
      "Epoch 009 | Train Loss: 0.5074 | Val Loss: 0.5200 | Time: 9.33s\r\n",
      "Epoch 010 | Train Loss: 0.5103 | Val Loss: 0.5155 | Time: 9.46s\r\n",
      "Epoch 011 | Train Loss: 0.5098 | Val Loss: 0.5183 | Time: 9.33s\r\n",
      "Epoch 012 | Train Loss: 0.5116 | Val Loss: 0.5193 | Time: 9.31s\r\n",
      "Epoch 013 | Train Loss: 0.5127 | Val Loss: 0.5231 | Time: 9.45s\r\n",
      "Epoch 014 | Train Loss: 0.5110 | Val Loss: 0.5225 | Time: 9.20s\r\n",
      "Epoch 015 | Train Loss: 0.5108 | Val Loss: 0.5257 | Time: 9.22s\r\n",
      "Epoch 016 | Train Loss: 0.5111 | Val Loss: 0.5170 | Time: 9.25s\r\n",
      "Epoch 017 | Train Loss: 0.5119 | Val Loss: 0.5263 | Time: 9.36s\r\n",
      "\r\n",
      "Early stopping à l'époque 17 (patience: 16)\r\n",
      "✅ Meilleur modèle Random chargé (époque 1, val_loss: 0.5103)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PaymentMethod        (CAT): 0.2358\r\n",
      "   2. DeviceProtection     (CAT): 0.2193\r\n",
      "   3. SeniorCitizen        (CAT): 0.2083\r\n",
      "   4. StreamingMovies      (CAT): 0.1492\r\n",
      "   5. StreamingTV          (CAT): 0.1055\r\n",
      "   6. PaperlessBilling     (CAT): 0.0819\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.2358\r\n",
      "   2. DeviceProtection    : 0.2193\r\n",
      "   3. SeniorCitizen       : 0.2083\r\n",
      "   4. StreamingMovies     : 0.1492\r\n",
      "   5. StreamingTV         : 0.1055\r\n",
      "   6. PaperlessBilling    : 0.0819\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 232.9s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: L\r\n",
      "Modèle FTT+ créé avec 88,001 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5117 | Val Loss: 0.4864 | Time: 2.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4864)\r\n",
      "Epoch 001 | Train Loss: 0.5083 | Val Loss: 0.4963 | Time: 2.18s\r\n",
      "Epoch 002 | Train Loss: 0.5077 | Val Loss: 0.5530 | Time: 2.22s\r\n",
      "Epoch 003 | Train Loss: 0.4982 | Val Loss: 0.5040 | Time: 2.22s\r\n",
      "Epoch 004 | Train Loss: 0.4892 | Val Loss: 0.5151 | Time: 2.21s\r\n",
      "Epoch 005 | Train Loss: 0.4848 | Val Loss: 0.5160 | Time: 2.22s\r\n",
      "Epoch 006 | Train Loss: 0.4773 | Val Loss: 0.5031 | Time: 2.23s\r\n",
      "Epoch 007 | Train Loss: 0.4892 | Val Loss: 0.5192 | Time: 2.25s\r\n",
      "Epoch 008 | Train Loss: 0.4826 | Val Loss: 0.5028 | Time: 2.19s\r\n",
      "Epoch 009 | Train Loss: 0.4888 | Val Loss: 0.5165 | Time: 2.41s\r\n",
      "Epoch 010 | Train Loss: 0.4830 | Val Loss: 0.4972 | Time: 2.21s\r\n",
      "Epoch 011 | Train Loss: 0.4842 | Val Loss: 0.4939 | Time: 2.27s\r\n",
      "Epoch 012 | Train Loss: 0.4834 | Val Loss: 0.4987 | Time: 2.19s\r\n",
      "Epoch 013 | Train Loss: 0.4975 | Val Loss: 0.4848 | Time: 2.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4848)\r\n",
      "Epoch 014 | Train Loss: 0.4993 | Val Loss: 0.4881 | Time: 2.18s\r\n",
      "Epoch 015 | Train Loss: 0.5002 | Val Loss: 0.4819 | Time: 2.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4819)\r\n",
      "Epoch 016 | Train Loss: 0.5017 | Val Loss: 0.4935 | Time: 2.25s\r\n",
      "Epoch 017 | Train Loss: 0.5095 | Val Loss: 0.4786 | Time: 2.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4786)\r\n",
      "Epoch 018 | Train Loss: 0.4933 | Val Loss: 0.4679 | Time: 2.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4679)\r\n",
      "Epoch 019 | Train Loss: 0.4870 | Val Loss: 0.4703 | Time: 2.18s\r\n",
      "Epoch 020 | Train Loss: 0.4890 | Val Loss: 0.4873 | Time: 2.25s\r\n",
      "Epoch 021 | Train Loss: 0.4895 | Val Loss: 0.4741 | Time: 2.19s\r\n",
      "Epoch 022 | Train Loss: 0.4913 | Val Loss: 0.4896 | Time: 2.17s\r\n",
      "Epoch 023 | Train Loss: 0.4947 | Val Loss: 0.5043 | Time: 2.35s\r\n",
      "Epoch 024 | Train Loss: 0.4960 | Val Loss: 0.4868 | Time: 2.24s\r\n",
      "Epoch 025 | Train Loss: 0.4800 | Val Loss: 0.4623 | Time: 2.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4623)\r\n",
      "Epoch 026 | Train Loss: 0.4952 | Val Loss: 0.4836 | Time: 2.27s\r\n",
      "Epoch 027 | Train Loss: 0.5119 | Val Loss: 0.5522 | Time: 2.23s\r\n",
      "Epoch 028 | Train Loss: 0.5024 | Val Loss: 0.4898 | Time: 2.23s\r\n",
      "Epoch 029 | Train Loss: 0.4951 | Val Loss: 0.5010 | Time: 2.25s\r\n",
      "Epoch 030 | Train Loss: 0.5020 | Val Loss: 0.4719 | Time: 2.22s\r\n",
      "Epoch 031 | Train Loss: 0.4943 | Val Loss: 0.4942 | Time: 2.23s\r\n",
      "Epoch 032 | Train Loss: 0.5042 | Val Loss: 0.4797 | Time: 2.23s\r\n",
      "Epoch 033 | Train Loss: 0.4860 | Val Loss: 0.4553 | Time: 2.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4553)\r\n",
      "Epoch 034 | Train Loss: 0.4902 | Val Loss: 0.4784 | Time: 2.26s\r\n",
      "Epoch 035 | Train Loss: 0.5006 | Val Loss: 0.4698 | Time: 2.23s\r\n",
      "Epoch 036 | Train Loss: 0.5009 | Val Loss: 0.4690 | Time: 2.22s\r\n",
      "Epoch 037 | Train Loss: 0.4985 | Val Loss: 0.4749 | Time: 2.28s\r\n",
      "Epoch 038 | Train Loss: 0.5074 | Val Loss: 0.4816 | Time: 2.33s\r\n",
      "Epoch 039 | Train Loss: 0.4935 | Val Loss: 0.4629 | Time: 2.21s\r\n",
      "Epoch 040 | Train Loss: 0.4896 | Val Loss: 0.4606 | Time: 2.19s\r\n",
      "Epoch 041 | Train Loss: 0.4826 | Val Loss: 0.4572 | Time: 2.23s\r\n",
      "Epoch 042 | Train Loss: 0.4835 | Val Loss: 0.4691 | Time: 2.23s\r\n",
      "Epoch 043 | Train Loss: 0.4859 | Val Loss: 0.4678 | Time: 2.25s\r\n",
      "Epoch 044 | Train Loss: 0.4891 | Val Loss: 0.4626 | Time: 2.22s\r\n",
      "Epoch 045 | Train Loss: 0.4861 | Val Loss: 0.4657 | Time: 2.21s\r\n",
      "Epoch 046 | Train Loss: 0.4858 | Val Loss: 0.4640 | Time: 2.21s\r\n",
      "Epoch 047 | Train Loss: 0.4912 | Val Loss: 0.4718 | Time: 2.25s\r\n",
      "Epoch 048 | Train Loss: 0.4970 | Val Loss: 0.4550 | Time: 2.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4550)\r\n",
      "Epoch 049 | Train Loss: 0.4984 | Val Loss: 0.4565 | Time: 2.21s\r\n",
      "✅ Meilleur modèle chargé (époque 48, val_loss: 0.4550)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.2719\r\n",
      "   2. gender              : 0.2448\r\n",
      "   3. OnlineSecurity      : 0.2143\r\n",
      "   4. MultipleLines       : 0.1307\r\n",
      "   5. Contract            : 0.0903\r\n",
      "   6. StreamingMovies     : 0.0140\r\n",
      "   7. MonthlyCharges      : 0.0098\r\n",
      "   8. PaperlessBilling    : 0.0090\r\n",
      "   9. TotalCharges        : 0.0065\r\n",
      "  10. tenure              : 0.0052\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 6 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PhoneService         (CAT): 0.2719\r\n",
      "   2. gender               (CAT): 0.2448\r\n",
      "   3. OnlineSecurity       (CAT): 0.2143\r\n",
      "   4. MultipleLines        (CAT): 0.1307\r\n",
      "   5. Contract             (CAT): 0.0903\r\n",
      "   6. StreamingMovies      (CAT): 0.0140\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['PhoneService', 'gender', 'OnlineSecurity', 'MultipleLines', 'Contract', 'StreamingMovies'] → indices [4, 0, 7, 5, 13, 12]\r\n",
      "📊 Features sélectionnées: 0 numériques, 6 catégorielles\r\n",
      "🎲 Interactions aléatoires: 10 paires\r\n",
      "Modèle Random créé avec 64,769 paramètres\r\n",
      "🔗 Sparsité d'attention: 34.69%\r\n",
      "   - Connexions feature-feature: 20\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5006 | Val Loss: 0.4766 | Time: 9.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4766)\r\n",
      "Epoch 001 | Train Loss: 0.4841 | Val Loss: 0.4689 | Time: 9.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4689)\r\n",
      "Epoch 002 | Train Loss: 0.4737 | Val Loss: 0.4663 | Time: 9.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4663)\r\n",
      "Epoch 003 | Train Loss: 0.4741 | Val Loss: 0.4683 | Time: 9.54s\r\n",
      "Epoch 004 | Train Loss: 0.4749 | Val Loss: 0.4637 | Time: 9.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4637)\r\n",
      "Epoch 005 | Train Loss: 0.4782 | Val Loss: 0.4700 | Time: 9.20s\r\n",
      "Epoch 006 | Train Loss: 0.4767 | Val Loss: 0.4644 | Time: 9.31s\r\n",
      "Epoch 007 | Train Loss: 0.4729 | Val Loss: 0.4559 | Time: 9.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4559)\r\n",
      "Epoch 008 | Train Loss: 0.4735 | Val Loss: 0.4570 | Time: 9.16s\r\n",
      "Epoch 009 | Train Loss: 0.4690 | Val Loss: 0.4547 | Time: 9.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4547)\r\n",
      "Epoch 010 | Train Loss: 0.4663 | Val Loss: 0.4573 | Time: 9.45s\r\n",
      "Epoch 011 | Train Loss: 0.4673 | Val Loss: 0.4556 | Time: 9.24s\r\n",
      "Epoch 012 | Train Loss: 0.4652 | Val Loss: 0.4571 | Time: 9.24s\r\n",
      "Epoch 013 | Train Loss: 0.4674 | Val Loss: 0.4550 | Time: 9.25s\r\n",
      "Epoch 014 | Train Loss: 0.4694 | Val Loss: 0.4521 | Time: 9.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4521)\r\n",
      "Epoch 015 | Train Loss: 0.4724 | Val Loss: 0.4570 | Time: 9.16s\r\n",
      "Epoch 016 | Train Loss: 0.4882 | Val Loss: 0.4679 | Time: 9.22s\r\n",
      "Epoch 017 | Train Loss: 0.4759 | Val Loss: 0.4606 | Time: 9.38s\r\n",
      "Epoch 018 | Train Loss: 0.4697 | Val Loss: 0.4639 | Time: 9.37s\r\n",
      "Epoch 019 | Train Loss: 0.4686 | Val Loss: 0.4617 | Time: 9.36s\r\n",
      "Epoch 020 | Train Loss: 0.4755 | Val Loss: 0.4756 | Time: 9.49s\r\n",
      "Epoch 021 | Train Loss: 0.4731 | Val Loss: 0.4573 | Time: 9.28s\r\n",
      "Epoch 022 | Train Loss: 0.4711 | Val Loss: 0.4557 | Time: 9.36s\r\n",
      "Epoch 023 | Train Loss: 0.4690 | Val Loss: 0.4578 | Time: 9.30s\r\n",
      "Epoch 024 | Train Loss: 0.4687 | Val Loss: 0.4540 | Time: 9.39s\r\n",
      "Epoch 025 | Train Loss: 0.4692 | Val Loss: 0.4676 | Time: 9.35s\r\n",
      "Epoch 026 | Train Loss: 0.4737 | Val Loss: 0.4532 | Time: 9.41s\r\n",
      "Epoch 027 | Train Loss: 0.4699 | Val Loss: 0.4536 | Time: 9.44s\r\n",
      "Epoch 028 | Train Loss: 0.4705 | Val Loss: 0.4529 | Time: 9.34s\r\n",
      "Epoch 029 | Train Loss: 0.4689 | Val Loss: 0.4537 | Time: 9.33s\r\n",
      "Epoch 030 | Train Loss: 0.4682 | Val Loss: 0.4558 | Time: 9.35s\r\n",
      "\r\n",
      "Early stopping à l'époque 30 (patience: 16)\r\n",
      "✅ Meilleur modèle Random chargé (époque 14, val_loss: 0.4521)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.4037\r\n",
      "   2. Contract             (CAT): 0.1855\r\n",
      "   3. OnlineSecurity       (CAT): 0.1816\r\n",
      "   4. MultipleLines        (CAT): 0.1241\r\n",
      "   5. gender               (CAT): 0.0675\r\n",
      "   6. StreamingMovies      (CAT): 0.0376\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.4037\r\n",
      "   2. Contract            : 0.1855\r\n",
      "   3. OnlineSecurity      : 0.1816\r\n",
      "   4. MultipleLines       : 0.1241\r\n",
      "   5. gender              : 0.0675\r\n",
      "   6. StreamingMovies     : 0.0376\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_3/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 403.4s ===\r\n",
      "\u001b[32m[I 2025-07-19 19:45:02,632]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 2, 'n_heads_stage1': 4, 'ffn_hidden_stage1': 128, 'attention_dropout_stage1': 0.2344271094811757, 'ffn_dropout_stage1': 0.25232392306574347, 'residual_dropout_stage1': 0.12376375439923998, 'lr_stage1': 0.008182111518618418, 'weight_decay_stage1': 6.901057779813318e-05, 'd_token_stage2': 32, 'n_blocks_stage2': 6, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.1452991550395876, 'ffn_dropout_stage2': 0.22903455808188997, 'residual_dropout_stage2': 0.11743664290049916, 'lr_stage2': 0.0058043147174140705, 'weight_decay_stage2': 8.583743499363294e-05, 'batch_size': 32, 'patience': 16, 'embedding_type': 'L', 'M': 6, 'k': 10}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  16%|█▎      | 4/25 [1:32:05<7:56:46, 1362.19s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: L\r\n",
      "Modèle FTT+ créé avec 70,945 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4869 | Val Loss: 0.5215 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5215)\r\n",
      "Epoch 001 | Train Loss: 0.4687 | Val Loss: 0.4409 | Time: 3.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4409)\r\n",
      "Epoch 002 | Train Loss: 0.4480 | Val Loss: 0.4356 | Time: 3.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4356)\r\n",
      "Epoch 003 | Train Loss: 0.4382 | Val Loss: 0.4252 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4252)\r\n",
      "Epoch 004 | Train Loss: 0.4346 | Val Loss: 0.4362 | Time: 3.54s\r\n",
      "Epoch 005 | Train Loss: 0.4349 | Val Loss: 0.4296 | Time: 3.50s\r\n",
      "Epoch 006 | Train Loss: 0.4322 | Val Loss: 0.4378 | Time: 3.51s\r\n",
      "Epoch 007 | Train Loss: 0.4291 | Val Loss: 0.4302 | Time: 3.45s\r\n",
      "Epoch 008 | Train Loss: 0.4332 | Val Loss: 0.4374 | Time: 3.53s\r\n",
      "Epoch 009 | Train Loss: 0.4318 | Val Loss: 0.4397 | Time: 3.55s\r\n",
      "Epoch 010 | Train Loss: 0.4285 | Val Loss: 0.4277 | Time: 3.52s\r\n",
      "Epoch 011 | Train Loss: 0.4274 | Val Loss: 0.4285 | Time: 3.52s\r\n",
      "Epoch 012 | Train Loss: 0.4272 | Val Loss: 0.4253 | Time: 3.58s\r\n",
      "Epoch 013 | Train Loss: 0.4248 | Val Loss: 0.4280 | Time: 3.52s\r\n",
      "Epoch 014 | Train Loss: 0.4234 | Val Loss: 0.4355 | Time: 3.53s\r\n",
      "Epoch 015 | Train Loss: 0.4336 | Val Loss: 0.4417 | Time: 3.54s\r\n",
      "Epoch 016 | Train Loss: 0.4278 | Val Loss: 0.4177 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4177)\r\n",
      "Epoch 017 | Train Loss: 0.4191 | Val Loss: 0.4106 | Time: 3.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 018 | Train Loss: 0.4232 | Val Loss: 0.4181 | Time: 3.55s\r\n",
      "Epoch 019 | Train Loss: 0.4254 | Val Loss: 0.4148 | Time: 3.52s\r\n",
      "Epoch 020 | Train Loss: 0.4206 | Val Loss: 0.4165 | Time: 3.53s\r\n",
      "Epoch 021 | Train Loss: 0.4220 | Val Loss: 0.4205 | Time: 3.52s\r\n",
      "Epoch 022 | Train Loss: 0.4215 | Val Loss: 0.4160 | Time: 3.56s\r\n",
      "Epoch 023 | Train Loss: 0.4174 | Val Loss: 0.4224 | Time: 3.55s\r\n",
      "Epoch 024 | Train Loss: 0.4180 | Val Loss: 0.4208 | Time: 3.46s\r\n",
      "Epoch 025 | Train Loss: 0.4171 | Val Loss: 0.4259 | Time: 3.49s\r\n",
      "Epoch 026 | Train Loss: 0.4248 | Val Loss: 0.4218 | Time: 3.67s\r\n",
      "Epoch 027 | Train Loss: 0.4209 | Val Loss: 0.4264 | Time: 3.50s\r\n",
      "Epoch 028 | Train Loss: 0.4241 | Val Loss: 0.4231 | Time: 3.54s\r\n",
      "Epoch 029 | Train Loss: 0.4226 | Val Loss: 0.4186 | Time: 3.55s\r\n",
      "Epoch 030 | Train Loss: 0.4186 | Val Loss: 0.4259 | Time: 3.47s\r\n",
      "Epoch 031 | Train Loss: 0.4216 | Val Loss: 0.4184 | Time: 3.55s\r\n",
      "Epoch 032 | Train Loss: 0.4161 | Val Loss: 0.4242 | Time: 3.54s\r\n",
      "Epoch 033 | Train Loss: 0.4184 | Val Loss: 0.4203 | Time: 3.53s\r\n",
      "Epoch 034 | Train Loss: 0.4186 | Val Loss: 0.4169 | Time: 3.56s\r\n",
      "Epoch 035 | Train Loss: 0.4193 | Val Loss: 0.4214 | Time: 3.58s\r\n",
      "Epoch 036 | Train Loss: 0.4156 | Val Loss: 0.4206 | Time: 3.52s\r\n",
      "Epoch 037 | Train Loss: 0.4178 | Val Loss: 0.4191 | Time: 3.58s\r\n",
      "Epoch 038 | Train Loss: 0.4161 | Val Loss: 0.4261 | Time: 3.54s\r\n",
      "Epoch 039 | Train Loss: 0.4168 | Val Loss: 0.4159 | Time: 3.53s\r\n",
      "Epoch 040 | Train Loss: 0.4149 | Val Loss: 0.4123 | Time: 3.52s\r\n",
      "Epoch 041 | Train Loss: 0.4190 | Val Loss: 0.4159 | Time: 3.53s\r\n",
      "Epoch 042 | Train Loss: 0.4148 | Val Loss: 0.4202 | Time: 3.48s\r\n",
      "\r\n",
      "Early stopping à l'époque 42 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 17, val_loss: 0.4106)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.0847\r\n",
      "   2. MultipleLines       : 0.0821\r\n",
      "   3. OnlineBackup        : 0.0678\r\n",
      "   4. DeviceProtection    : 0.0654\r\n",
      "   5. StreamingTV         : 0.0567\r\n",
      "   6. Dependents          : 0.0535\r\n",
      "   7. PaymentMethod       : 0.0523\r\n",
      "   8. Contract            : 0.0494\r\n",
      "   9. gender              : 0.0491\r\n",
      "  10. Partner             : 0.0485\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 15 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. SeniorCitizen        (CAT): 0.0847\r\n",
      "   2. MultipleLines        (CAT): 0.0821\r\n",
      "   3. OnlineBackup         (CAT): 0.0678\r\n",
      "   4. DeviceProtection     (CAT): 0.0654\r\n",
      "   5. StreamingTV          (CAT): 0.0567\r\n",
      "   6. Dependents           (CAT): 0.0535\r\n",
      "   7. PaymentMethod        (CAT): 0.0523\r\n",
      "   8. Contract             (CAT): 0.0494\r\n",
      "   9. gender               (CAT): 0.0491\r\n",
      "  10. Partner              (CAT): 0.0485\r\n",
      "  11. OnlineSecurity       (CAT): 0.0471\r\n",
      "  12. TechSupport          (CAT): 0.0460\r\n",
      "  13. PaperlessBilling     (CAT): 0.0460\r\n",
      "  14. tenure               (NUM): 0.0435\r\n",
      "  15. TotalCharges         (NUM): 0.0427\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges'] → indices [0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['SeniorCitizen', 'MultipleLines', 'OnlineBackup', 'DeviceProtection', 'StreamingTV', 'Dependents', 'PaymentMethod', 'Contract', 'gender', 'Partner', 'OnlineSecurity', 'TechSupport', 'PaperlessBilling'] → indices [1, 5, 8, 9, 11, 3, 15, 13, 0, 2, 7, 10, 14]\r\n",
      "📊 Features sélectionnées: 2 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 3 paires\r\n",
      "Modèle Random créé avec 128,961 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.94%\r\n",
      "   - Connexions feature-feature: 6\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5544 | Val Loss: 0.4637 | Time: 4.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4637)\r\n",
      "Epoch 001 | Train Loss: 0.4806 | Val Loss: 0.4505 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4505)\r\n",
      "Epoch 002 | Train Loss: 0.4597 | Val Loss: 0.4488 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4488)\r\n",
      "Epoch 003 | Train Loss: 0.4572 | Val Loss: 0.4454 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4454)\r\n",
      "Epoch 004 | Train Loss: 0.4504 | Val Loss: 0.4456 | Time: 4.81s\r\n",
      "Epoch 005 | Train Loss: 0.4526 | Val Loss: 0.4443 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4443)\r\n",
      "Epoch 006 | Train Loss: 0.4518 | Val Loss: 0.4426 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4426)\r\n",
      "Epoch 007 | Train Loss: 0.4488 | Val Loss: 0.4384 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4384)\r\n",
      "Epoch 008 | Train Loss: 0.4428 | Val Loss: 0.4322 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4322)\r\n",
      "Epoch 009 | Train Loss: 0.4472 | Val Loss: 0.4345 | Time: 4.79s\r\n",
      "Epoch 010 | Train Loss: 0.4438 | Val Loss: 0.4317 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 011 | Train Loss: 0.4380 | Val Loss: 0.4353 | Time: 4.71s\r\n",
      "Epoch 012 | Train Loss: 0.4448 | Val Loss: 0.4304 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4304)\r\n",
      "Epoch 013 | Train Loss: 0.4415 | Val Loss: 0.4308 | Time: 4.84s\r\n",
      "Epoch 014 | Train Loss: 0.4337 | Val Loss: 0.4309 | Time: 4.84s\r\n",
      "Epoch 015 | Train Loss: 0.4377 | Val Loss: 0.4289 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 016 | Train Loss: 0.4428 | Val Loss: 0.4276 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4276)\r\n",
      "Epoch 017 | Train Loss: 0.4414 | Val Loss: 0.4258 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 018 | Train Loss: 0.4404 | Val Loss: 0.4279 | Time: 4.82s\r\n",
      "Epoch 019 | Train Loss: 0.4383 | Val Loss: 0.4289 | Time: 4.77s\r\n",
      "Epoch 020 | Train Loss: 0.4379 | Val Loss: 0.4274 | Time: 4.86s\r\n",
      "Epoch 021 | Train Loss: 0.4328 | Val Loss: 0.4279 | Time: 4.73s\r\n",
      "Epoch 022 | Train Loss: 0.4332 | Val Loss: 0.4271 | Time: 4.72s\r\n",
      "Epoch 023 | Train Loss: 0.4328 | Val Loss: 0.4260 | Time: 4.78s\r\n",
      "Epoch 024 | Train Loss: 0.4347 | Val Loss: 0.4256 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4256)\r\n",
      "Epoch 025 | Train Loss: 0.4344 | Val Loss: 0.4231 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4231)\r\n",
      "Epoch 026 | Train Loss: 0.4296 | Val Loss: 0.4287 | Time: 4.76s\r\n",
      "Epoch 027 | Train Loss: 0.4307 | Val Loss: 0.4277 | Time: 4.86s\r\n",
      "Epoch 028 | Train Loss: 0.4339 | Val Loss: 0.4263 | Time: 4.74s\r\n",
      "Epoch 029 | Train Loss: 0.4321 | Val Loss: 0.4268 | Time: 4.80s\r\n",
      "Epoch 030 | Train Loss: 0.4346 | Val Loss: 0.4274 | Time: 4.77s\r\n",
      "Epoch 031 | Train Loss: 0.4283 | Val Loss: 0.4254 | Time: 4.79s\r\n",
      "Epoch 032 | Train Loss: 0.4301 | Val Loss: 0.4254 | Time: 4.77s\r\n",
      "Epoch 033 | Train Loss: 0.4325 | Val Loss: 0.4228 | Time: 4.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4228)\r\n",
      "Epoch 034 | Train Loss: 0.4298 | Val Loss: 0.4235 | Time: 4.71s\r\n",
      "Epoch 035 | Train Loss: 0.4315 | Val Loss: 0.4266 | Time: 4.73s\r\n",
      "Epoch 036 | Train Loss: 0.4285 | Val Loss: 0.4248 | Time: 4.75s\r\n",
      "Epoch 037 | Train Loss: 0.4295 | Val Loss: 0.4236 | Time: 4.80s\r\n",
      "Epoch 038 | Train Loss: 0.4276 | Val Loss: 0.4238 | Time: 4.76s\r\n",
      "Epoch 039 | Train Loss: 0.4300 | Val Loss: 0.4245 | Time: 4.82s\r\n",
      "Epoch 040 | Train Loss: 0.4311 | Val Loss: 0.4233 | Time: 4.83s\r\n",
      "Epoch 041 | Train Loss: 0.4291 | Val Loss: 0.4253 | Time: 4.74s\r\n",
      "Epoch 042 | Train Loss: 0.4264 | Val Loss: 0.4234 | Time: 4.71s\r\n",
      "Epoch 043 | Train Loss: 0.4259 | Val Loss: 0.4249 | Time: 4.78s\r\n",
      "Epoch 044 | Train Loss: 0.4306 | Val Loss: 0.4234 | Time: 4.80s\r\n",
      "Epoch 045 | Train Loss: 0.4273 | Val Loss: 0.4261 | Time: 4.80s\r\n",
      "Epoch 046 | Train Loss: 0.4250 | Val Loss: 0.4245 | Time: 4.76s\r\n",
      "Epoch 047 | Train Loss: 0.4249 | Val Loss: 0.4239 | Time: 4.97s\r\n",
      "Epoch 048 | Train Loss: 0.4244 | Val Loss: 0.4245 | Time: 4.76s\r\n",
      "Epoch 049 | Train Loss: 0.4248 | Val Loss: 0.4232 | Time: 4.74s\r\n",
      "✅ Meilleur modèle Random chargé (époque 33, val_loss: 0.4228)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineSecurity       (CAT): 0.1435\r\n",
      "   2. Contract             (CAT): 0.0772\r\n",
      "   3. tenure               (NUM): 0.0731\r\n",
      "   4. Dependents           (CAT): 0.0689\r\n",
      "   5. PaperlessBilling     (CAT): 0.0648\r\n",
      "   6. OnlineBackup         (CAT): 0.0635\r\n",
      "   7. TotalCharges         (NUM): 0.0593\r\n",
      "   8. Partner              (CAT): 0.0591\r\n",
      "   9. gender               (CAT): 0.0561\r\n",
      "  10. DeviceProtection     (CAT): 0.0561\r\n",
      "  11. MultipleLines        (CAT): 0.0561\r\n",
      "  12. StreamingTV          (CAT): 0.0558\r\n",
      "  13. TechSupport          (CAT): 0.0558\r\n",
      "  14. PaymentMethod        (CAT): 0.0557\r\n",
      "  15. SeniorCitizen        (CAT): 0.0549\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.1435\r\n",
      "   2. Contract            : 0.0772\r\n",
      "   3. tenure              : 0.0731\r\n",
      "   4. Dependents          : 0.0689\r\n",
      "   5. PaperlessBilling    : 0.0648\r\n",
      "   6. OnlineBackup        : 0.0635\r\n",
      "   7. TotalCharges        : 0.0593\r\n",
      "   8. Partner             : 0.0591\r\n",
      "   9. gender              : 0.0561\r\n",
      "  10. DeviceProtection    : 0.0561\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 395.0s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: L\r\n",
      "Modèle FTT+ créé avec 70,945 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4843 | Val Loss: 0.4648 | Time: 3.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4648)\r\n",
      "Epoch 001 | Train Loss: 0.4475 | Val Loss: 0.4393 | Time: 3.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4393)\r\n",
      "Epoch 002 | Train Loss: 0.4496 | Val Loss: 0.4444 | Time: 3.60s\r\n",
      "Epoch 003 | Train Loss: 0.4345 | Val Loss: 0.4353 | Time: 3.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4353)\r\n",
      "Epoch 004 | Train Loss: 0.4370 | Val Loss: 0.4333 | Time: 3.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4333)\r\n",
      "Epoch 005 | Train Loss: 0.4385 | Val Loss: 0.4242 | Time: 3.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4242)\r\n",
      "Epoch 006 | Train Loss: 0.4308 | Val Loss: 0.4335 | Time: 3.48s\r\n",
      "Epoch 007 | Train Loss: 0.4278 | Val Loss: 0.4244 | Time: 3.52s\r\n",
      "Epoch 008 | Train Loss: 0.4263 | Val Loss: 0.4203 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4203)\r\n",
      "Epoch 009 | Train Loss: 0.4201 | Val Loss: 0.4207 | Time: 3.54s\r\n",
      "Epoch 010 | Train Loss: 0.4208 | Val Loss: 0.4227 | Time: 3.58s\r\n",
      "Epoch 011 | Train Loss: 0.4187 | Val Loss: 0.4175 | Time: 3.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4175)\r\n",
      "Epoch 012 | Train Loss: 0.4155 | Val Loss: 0.4166 | Time: 3.57s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4166)\r\n",
      "Epoch 013 | Train Loss: 0.4169 | Val Loss: 0.4230 | Time: 3.61s\r\n",
      "Epoch 014 | Train Loss: 0.4146 | Val Loss: 0.4155 | Time: 3.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4155)\r\n",
      "Epoch 015 | Train Loss: 0.4134 | Val Loss: 0.4195 | Time: 3.51s\r\n",
      "Epoch 016 | Train Loss: 0.4173 | Val Loss: 0.4214 | Time: 3.58s\r\n",
      "Epoch 017 | Train Loss: 0.4156 | Val Loss: 0.4231 | Time: 3.57s\r\n",
      "Epoch 018 | Train Loss: 0.4147 | Val Loss: 0.4239 | Time: 3.55s\r\n",
      "Epoch 019 | Train Loss: 0.4149 | Val Loss: 0.4281 | Time: 3.60s\r\n",
      "Epoch 020 | Train Loss: 0.4188 | Val Loss: 0.4254 | Time: 3.52s\r\n",
      "Epoch 021 | Train Loss: 0.4121 | Val Loss: 0.4255 | Time: 3.62s\r\n",
      "Epoch 022 | Train Loss: 0.4134 | Val Loss: 0.4175 | Time: 3.54s\r\n",
      "Epoch 023 | Train Loss: 0.4141 | Val Loss: 0.4177 | Time: 3.54s\r\n",
      "Epoch 024 | Train Loss: 0.4116 | Val Loss: 0.4250 | Time: 3.57s\r\n",
      "Epoch 025 | Train Loss: 0.4126 | Val Loss: 0.4190 | Time: 3.54s\r\n",
      "Epoch 026 | Train Loss: 0.4122 | Val Loss: 0.4147 | Time: 3.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "Epoch 027 | Train Loss: 0.4102 | Val Loss: 0.4177 | Time: 3.57s\r\n",
      "Epoch 028 | Train Loss: 0.4135 | Val Loss: 0.4219 | Time: 3.57s\r\n",
      "Epoch 029 | Train Loss: 0.4123 | Val Loss: 0.4215 | Time: 3.55s\r\n",
      "Epoch 030 | Train Loss: 0.4085 | Val Loss: 0.4247 | Time: 3.67s\r\n",
      "Epoch 031 | Train Loss: 0.4088 | Val Loss: 0.4227 | Time: 3.55s\r\n",
      "Epoch 032 | Train Loss: 0.4130 | Val Loss: 0.4244 | Time: 3.54s\r\n",
      "Epoch 033 | Train Loss: 0.4106 | Val Loss: 0.4268 | Time: 3.52s\r\n",
      "Epoch 034 | Train Loss: 0.4094 | Val Loss: 0.4315 | Time: 3.51s\r\n",
      "Epoch 035 | Train Loss: 0.4112 | Val Loss: 0.4280 | Time: 3.62s\r\n",
      "Epoch 036 | Train Loss: 0.4124 | Val Loss: 0.4251 | Time: 3.54s\r\n",
      "Epoch 037 | Train Loss: 0.4091 | Val Loss: 0.4239 | Time: 3.55s\r\n",
      "Epoch 038 | Train Loss: 0.4111 | Val Loss: 0.4259 | Time: 3.56s\r\n",
      "Epoch 039 | Train Loss: 0.4080 | Val Loss: 0.4263 | Time: 3.62s\r\n",
      "Epoch 040 | Train Loss: 0.4076 | Val Loss: 0.4266 | Time: 3.48s\r\n",
      "Epoch 041 | Train Loss: 0.4073 | Val Loss: 0.4268 | Time: 3.54s\r\n",
      "Epoch 042 | Train Loss: 0.4087 | Val Loss: 0.4251 | Time: 3.55s\r\n",
      "Epoch 043 | Train Loss: 0.4103 | Val Loss: 0.4215 | Time: 3.54s\r\n",
      "Epoch 044 | Train Loss: 0.4122 | Val Loss: 0.4194 | Time: 3.59s\r\n",
      "Epoch 045 | Train Loss: 0.4053 | Val Loss: 0.4240 | Time: 3.54s\r\n",
      "Epoch 046 | Train Loss: 0.4099 | Val Loss: 0.4246 | Time: 3.53s\r\n",
      "Epoch 047 | Train Loss: 0.4070 | Val Loss: 0.4278 | Time: 3.54s\r\n",
      "Epoch 048 | Train Loss: 0.4114 | Val Loss: 0.4229 | Time: 3.73s\r\n",
      "Epoch 049 | Train Loss: 0.4082 | Val Loss: 0.4295 | Time: 3.56s\r\n",
      "✅ Meilleur modèle chargé (époque 26, val_loss: 0.4147)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.0911\r\n",
      "   2. gender              : 0.0700\r\n",
      "   3. MultipleLines       : 0.0665\r\n",
      "   4. MonthlyCharges      : 0.0627\r\n",
      "   5. Contract            : 0.0617\r\n",
      "   6. TotalCharges        : 0.0587\r\n",
      "   7. StreamingMovies     : 0.0584\r\n",
      "   8. OnlineBackup        : 0.0534\r\n",
      "   9. PhoneService        : 0.0526\r\n",
      "  10. InternetService     : 0.0508\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 15 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. SeniorCitizen        (CAT): 0.0911\r\n",
      "   2. gender               (CAT): 0.0700\r\n",
      "   3. MultipleLines        (CAT): 0.0665\r\n",
      "   4. MonthlyCharges       (NUM): 0.0627\r\n",
      "   5. Contract             (CAT): 0.0617\r\n",
      "   6. TotalCharges         (NUM): 0.0587\r\n",
      "   7. StreamingMovies      (CAT): 0.0584\r\n",
      "   8. OnlineBackup         (CAT): 0.0534\r\n",
      "   9. PhoneService         (CAT): 0.0526\r\n",
      "  10. InternetService      (CAT): 0.0508\r\n",
      "  11. PaymentMethod        (CAT): 0.0505\r\n",
      "  12. DeviceProtection     (CAT): 0.0486\r\n",
      "  13. TechSupport          (CAT): 0.0472\r\n",
      "  14. StreamingTV          (CAT): 0.0418\r\n",
      "  15. OnlineSecurity       (CAT): 0.0401\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges'] → indices [1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['SeniorCitizen', 'gender', 'MultipleLines', 'Contract', 'StreamingMovies', 'OnlineBackup', 'PhoneService', 'InternetService', 'PaymentMethod', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'OnlineSecurity'] → indices [1, 0, 5, 13, 12, 8, 4, 6, 15, 9, 10, 11, 7]\r\n",
      "📊 Features sélectionnées: 2 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 3 paires\r\n",
      "Modèle Random créé avec 129,089 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.94%\r\n",
      "   - Connexions feature-feature: 6\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5630 | Val Loss: 0.4635 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4635)\r\n",
      "Epoch 001 | Train Loss: 0.4846 | Val Loss: 0.4342 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 002 | Train Loss: 0.4653 | Val Loss: 0.4327 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 003 | Train Loss: 0.4566 | Val Loss: 0.4290 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4290)\r\n",
      "Epoch 004 | Train Loss: 0.4512 | Val Loss: 0.4302 | Time: 4.69s\r\n",
      "Epoch 005 | Train Loss: 0.4510 | Val Loss: 0.4257 | Time: 5.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4257)\r\n",
      "Epoch 006 | Train Loss: 0.4458 | Val Loss: 0.4235 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 007 | Train Loss: 0.4418 | Val Loss: 0.4220 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4220)\r\n",
      "Epoch 008 | Train Loss: 0.4425 | Val Loss: 0.4209 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 009 | Train Loss: 0.4349 | Val Loss: 0.4213 | Time: 4.72s\r\n",
      "Epoch 010 | Train Loss: 0.4298 | Val Loss: 0.4204 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 011 | Train Loss: 0.4369 | Val Loss: 0.4209 | Time: 4.82s\r\n",
      "Epoch 012 | Train Loss: 0.4414 | Val Loss: 0.4193 | Time: 4.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4193)\r\n",
      "Epoch 013 | Train Loss: 0.4334 | Val Loss: 0.4200 | Time: 4.77s\r\n",
      "Epoch 014 | Train Loss: 0.4341 | Val Loss: 0.4189 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4189)\r\n",
      "Epoch 015 | Train Loss: 0.4303 | Val Loss: 0.4198 | Time: 4.79s\r\n",
      "Epoch 016 | Train Loss: 0.4296 | Val Loss: 0.4203 | Time: 4.77s\r\n",
      "Epoch 017 | Train Loss: 0.4296 | Val Loss: 0.4168 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4168)\r\n",
      "Epoch 018 | Train Loss: 0.4279 | Val Loss: 0.4178 | Time: 4.83s\r\n",
      "Epoch 019 | Train Loss: 0.4330 | Val Loss: 0.4179 | Time: 4.68s\r\n",
      "Epoch 020 | Train Loss: 0.4297 | Val Loss: 0.4185 | Time: 4.81s\r\n",
      "Epoch 021 | Train Loss: 0.4279 | Val Loss: 0.4158 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4158)\r\n",
      "Epoch 022 | Train Loss: 0.4270 | Val Loss: 0.4174 | Time: 4.81s\r\n",
      "Epoch 023 | Train Loss: 0.4263 | Val Loss: 0.4166 | Time: 4.79s\r\n",
      "Epoch 024 | Train Loss: 0.4318 | Val Loss: 0.4147 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "Epoch 025 | Train Loss: 0.4254 | Val Loss: 0.4147 | Time: 5.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "Epoch 026 | Train Loss: 0.4246 | Val Loss: 0.4163 | Time: 4.85s\r\n",
      "Epoch 027 | Train Loss: 0.4219 | Val Loss: 0.4144 | Time: 4.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4144)\r\n",
      "Epoch 028 | Train Loss: 0.4251 | Val Loss: 0.4183 | Time: 4.83s\r\n",
      "Epoch 029 | Train Loss: 0.4255 | Val Loss: 0.4155 | Time: 4.74s\r\n",
      "Epoch 030 | Train Loss: 0.4269 | Val Loss: 0.4157 | Time: 4.78s\r\n",
      "Epoch 031 | Train Loss: 0.4176 | Val Loss: 0.4151 | Time: 4.85s\r\n",
      "Epoch 032 | Train Loss: 0.4263 | Val Loss: 0.4133 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4133)\r\n",
      "Epoch 033 | Train Loss: 0.4229 | Val Loss: 0.4121 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4121)\r\n",
      "Epoch 034 | Train Loss: 0.4248 | Val Loss: 0.4124 | Time: 4.79s\r\n",
      "Epoch 035 | Train Loss: 0.4197 | Val Loss: 0.4112 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4112)\r\n",
      "Epoch 036 | Train Loss: 0.4245 | Val Loss: 0.4094 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4094)\r\n",
      "Epoch 037 | Train Loss: 0.4236 | Val Loss: 0.4086 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4086)\r\n",
      "Epoch 038 | Train Loss: 0.4239 | Val Loss: 0.4097 | Time: 4.80s\r\n",
      "Epoch 039 | Train Loss: 0.4246 | Val Loss: 0.4091 | Time: 4.83s\r\n",
      "Epoch 040 | Train Loss: 0.4222 | Val Loss: 0.4110 | Time: 4.74s\r\n",
      "Epoch 041 | Train Loss: 0.4198 | Val Loss: 0.4083 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4083)\r\n",
      "Epoch 042 | Train Loss: 0.4173 | Val Loss: 0.4097 | Time: 4.68s\r\n",
      "Epoch 043 | Train Loss: 0.4186 | Val Loss: 0.4110 | Time: 4.73s\r\n",
      "Epoch 044 | Train Loss: 0.4204 | Val Loss: 0.4095 | Time: 4.79s\r\n",
      "Epoch 045 | Train Loss: 0.4210 | Val Loss: 0.4090 | Time: 4.84s\r\n",
      "Epoch 046 | Train Loss: 0.4208 | Val Loss: 0.4075 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4075)\r\n",
      "Epoch 047 | Train Loss: 0.4208 | Val Loss: 0.4069 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4069)\r\n",
      "Epoch 048 | Train Loss: 0.4135 | Val Loss: 0.4096 | Time: 4.68s\r\n",
      "Epoch 049 | Train Loss: 0.4169 | Val Loss: 0.4101 | Time: 4.79s\r\n",
      "✅ Meilleur modèle Random chargé (époque 47, val_loss: 0.4069)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PaymentMethod        (CAT): 0.1087\r\n",
      "   2. MultipleLines        (CAT): 0.0897\r\n",
      "   3. OnlineBackup         (CAT): 0.0809\r\n",
      "   4. TotalCharges         (NUM): 0.0759\r\n",
      "   5. OnlineSecurity       (CAT): 0.0696\r\n",
      "   6. gender               (CAT): 0.0660\r\n",
      "   7. InternetService      (CAT): 0.0638\r\n",
      "   8. PhoneService         (CAT): 0.0582\r\n",
      "   9. SeniorCitizen        (CAT): 0.0573\r\n",
      "  10. StreamingTV          (CAT): 0.0556\r\n",
      "  11. Contract             (CAT): 0.0554\r\n",
      "  12. DeviceProtection     (CAT): 0.0548\r\n",
      "  13. MonthlyCharges       (NUM): 0.0547\r\n",
      "  14. TechSupport          (CAT): 0.0547\r\n",
      "  15. StreamingMovies      (CAT): 0.0546\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.1087\r\n",
      "   2. MultipleLines       : 0.0897\r\n",
      "   3. OnlineBackup        : 0.0809\r\n",
      "   4. TotalCharges        : 0.0759\r\n",
      "   5. OnlineSecurity      : 0.0696\r\n",
      "   6. gender              : 0.0660\r\n",
      "   7. InternetService     : 0.0638\r\n",
      "   8. PhoneService        : 0.0582\r\n",
      "   9. SeniorCitizen       : 0.0573\r\n",
      "  10. StreamingTV         : 0.0556\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 421.3s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: L\r\n",
      "Modèle FTT+ créé avec 70,945 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4979 | Val Loss: 0.4852 | Time: 3.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4852)\r\n",
      "Epoch 001 | Train Loss: 0.4585 | Val Loss: 0.4565 | Time: 3.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4565)\r\n",
      "Epoch 002 | Train Loss: 0.4556 | Val Loss: 0.4671 | Time: 3.51s\r\n",
      "Epoch 003 | Train Loss: 0.4433 | Val Loss: 0.4622 | Time: 3.52s\r\n",
      "Epoch 004 | Train Loss: 0.4442 | Val Loss: 0.4533 | Time: 3.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4533)\r\n",
      "Epoch 005 | Train Loss: 0.4447 | Val Loss: 0.4560 | Time: 3.52s\r\n",
      "Epoch 006 | Train Loss: 0.4453 | Val Loss: 0.4542 | Time: 3.53s\r\n",
      "Epoch 007 | Train Loss: 0.4385 | Val Loss: 0.4521 | Time: 3.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4521)\r\n",
      "Epoch 008 | Train Loss: 0.4432 | Val Loss: 0.4566 | Time: 3.59s\r\n",
      "Epoch 009 | Train Loss: 0.4344 | Val Loss: 0.4420 | Time: 3.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4420)\r\n",
      "Epoch 010 | Train Loss: 0.4317 | Val Loss: 0.4621 | Time: 3.79s\r\n",
      "Epoch 011 | Train Loss: 0.4311 | Val Loss: 0.4406 | Time: 3.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4406)\r\n",
      "Epoch 012 | Train Loss: 0.4310 | Val Loss: 0.4360 | Time: 3.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4360)\r\n",
      "Epoch 013 | Train Loss: 0.4297 | Val Loss: 0.4478 | Time: 3.53s\r\n",
      "Epoch 014 | Train Loss: 0.4272 | Val Loss: 0.4369 | Time: 3.54s\r\n",
      "Epoch 015 | Train Loss: 0.4300 | Val Loss: 0.4461 | Time: 3.57s\r\n",
      "Epoch 016 | Train Loss: 0.4302 | Val Loss: 0.4391 | Time: 3.53s\r\n",
      "Epoch 017 | Train Loss: 0.4280 | Val Loss: 0.4240 | Time: 3.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4240)\r\n",
      "Epoch 018 | Train Loss: 0.4256 | Val Loss: 0.4375 | Time: 3.57s\r\n",
      "Epoch 019 | Train Loss: 0.4306 | Val Loss: 0.4296 | Time: 3.69s\r\n",
      "Epoch 020 | Train Loss: 0.4263 | Val Loss: 0.4440 | Time: 3.45s\r\n",
      "Epoch 021 | Train Loss: 0.4237 | Val Loss: 0.4227 | Time: 3.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 022 | Train Loss: 0.4254 | Val Loss: 0.4339 | Time: 3.47s\r\n",
      "Epoch 023 | Train Loss: 0.4248 | Val Loss: 0.4430 | Time: 3.50s\r\n",
      "Epoch 024 | Train Loss: 0.4226 | Val Loss: 0.4326 | Time: 3.52s\r\n",
      "Epoch 025 | Train Loss: 0.4250 | Val Loss: 0.4229 | Time: 3.52s\r\n",
      "Epoch 026 | Train Loss: 0.4258 | Val Loss: 0.4376 | Time: 3.52s\r\n",
      "Epoch 027 | Train Loss: 0.4248 | Val Loss: 0.4388 | Time: 3.54s\r\n",
      "Epoch 028 | Train Loss: 0.4253 | Val Loss: 0.4336 | Time: 3.54s\r\n",
      "Epoch 029 | Train Loss: 0.4249 | Val Loss: 0.4252 | Time: 3.51s\r\n",
      "Epoch 030 | Train Loss: 0.4246 | Val Loss: 0.4358 | Time: 3.48s\r\n",
      "Epoch 031 | Train Loss: 0.4233 | Val Loss: 0.4412 | Time: 3.53s\r\n",
      "Epoch 032 | Train Loss: 0.4262 | Val Loss: 0.4169 | Time: 3.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4169)\r\n",
      "Epoch 033 | Train Loss: 0.4259 | Val Loss: 0.4343 | Time: 3.55s\r\n",
      "Epoch 034 | Train Loss: 0.4248 | Val Loss: 0.4340 | Time: 3.56s\r\n",
      "Epoch 035 | Train Loss: 0.4226 | Val Loss: 0.4408 | Time: 3.58s\r\n",
      "Epoch 036 | Train Loss: 0.4250 | Val Loss: 0.4342 | Time: 3.47s\r\n",
      "Epoch 037 | Train Loss: 0.4238 | Val Loss: 0.4454 | Time: 3.70s\r\n",
      "Epoch 038 | Train Loss: 0.4228 | Val Loss: 0.4449 | Time: 3.54s\r\n",
      "Epoch 039 | Train Loss: 0.4230 | Val Loss: 0.4327 | Time: 3.52s\r\n",
      "Epoch 040 | Train Loss: 0.4248 | Val Loss: 0.4369 | Time: 3.52s\r\n",
      "Epoch 041 | Train Loss: 0.4223 | Val Loss: 0.4397 | Time: 3.56s\r\n",
      "Epoch 042 | Train Loss: 0.4260 | Val Loss: 0.4256 | Time: 3.52s\r\n",
      "Epoch 043 | Train Loss: 0.4214 | Val Loss: 0.4209 | Time: 3.51s\r\n",
      "Epoch 044 | Train Loss: 0.4254 | Val Loss: 0.4221 | Time: 3.57s\r\n",
      "Epoch 045 | Train Loss: 0.4210 | Val Loss: 0.4316 | Time: 3.53s\r\n",
      "Epoch 046 | Train Loss: 0.4235 | Val Loss: 0.4284 | Time: 3.72s\r\n",
      "Epoch 047 | Train Loss: 0.4197 | Val Loss: 0.4235 | Time: 3.51s\r\n",
      "Epoch 048 | Train Loss: 0.4239 | Val Loss: 0.4282 | Time: 3.54s\r\n",
      "Epoch 049 | Train Loss: 0.4207 | Val Loss: 0.4250 | Time: 3.53s\r\n",
      "✅ Meilleur modèle chargé (époque 32, val_loss: 0.4169)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0811\r\n",
      "   2. StreamingMovies     : 0.0757\r\n",
      "   3. SeniorCitizen       : 0.0651\r\n",
      "   4. gender              : 0.0646\r\n",
      "   5. PaymentMethod       : 0.0632\r\n",
      "   6. Dependents          : 0.0615\r\n",
      "   7. MonthlyCharges      : 0.0582\r\n",
      "   8. TechSupport         : 0.0581\r\n",
      "   9. Partner             : 0.0553\r\n",
      "  10. OnlineBackup        : 0.0533\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 15 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0811\r\n",
      "   2. StreamingMovies      (CAT): 0.0757\r\n",
      "   3. SeniorCitizen        (CAT): 0.0651\r\n",
      "   4. gender               (CAT): 0.0646\r\n",
      "   5. PaymentMethod        (CAT): 0.0632\r\n",
      "   6. Dependents           (CAT): 0.0615\r\n",
      "   7. MonthlyCharges       (NUM): 0.0582\r\n",
      "   8. TechSupport          (CAT): 0.0581\r\n",
      "   9. Partner              (CAT): 0.0553\r\n",
      "  10. OnlineBackup         (CAT): 0.0533\r\n",
      "  11. PhoneService         (CAT): 0.0482\r\n",
      "  12. OnlineSecurity       (CAT): 0.0451\r\n",
      "  13. tenure               (NUM): 0.0416\r\n",
      "  14. InternetService      (CAT): 0.0412\r\n",
      "  15. TotalCharges         (NUM): 0.0385\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure', 'TotalCharges'] → indices [1, 0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'StreamingMovies', 'SeniorCitizen', 'gender', 'PaymentMethod', 'Dependents', 'TechSupport', 'Partner', 'OnlineBackup', 'PhoneService', 'OnlineSecurity', 'InternetService'] → indices [14, 12, 1, 0, 15, 3, 10, 2, 8, 4, 7, 6]\r\n",
      "📊 Features sélectionnées: 3 numériques, 12 catégorielles\r\n",
      "🎲 Interactions aléatoires: 3 paires\r\n",
      "Modèle Random créé avec 128,769 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.94%\r\n",
      "   - Connexions feature-feature: 6\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5428 | Val Loss: 0.4645 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4645)\r\n",
      "Epoch 001 | Train Loss: 0.4867 | Val Loss: 0.4391 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4391)\r\n",
      "Epoch 002 | Train Loss: 0.4716 | Val Loss: 0.4361 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4361)\r\n",
      "Epoch 003 | Train Loss: 0.4692 | Val Loss: 0.4369 | Time: 4.96s\r\n",
      "Epoch 004 | Train Loss: 0.4634 | Val Loss: 0.4430 | Time: 4.73s\r\n",
      "Epoch 005 | Train Loss: 0.4569 | Val Loss: 0.4436 | Time: 4.85s\r\n",
      "Epoch 006 | Train Loss: 0.4548 | Val Loss: 0.4401 | Time: 4.77s\r\n",
      "Epoch 007 | Train Loss: 0.4655 | Val Loss: 0.4332 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4332)\r\n",
      "Epoch 008 | Train Loss: 0.4602 | Val Loss: 0.4340 | Time: 4.71s\r\n",
      "Epoch 009 | Train Loss: 0.4538 | Val Loss: 0.4322 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4322)\r\n",
      "Epoch 010 | Train Loss: 0.4511 | Val Loss: 0.4230 | Time: 4.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4230)\r\n",
      "Epoch 011 | Train Loss: 0.4529 | Val Loss: 0.4287 | Time: 4.75s\r\n",
      "Epoch 012 | Train Loss: 0.4543 | Val Loss: 0.4283 | Time: 4.78s\r\n",
      "Epoch 013 | Train Loss: 0.4515 | Val Loss: 0.4324 | Time: 4.70s\r\n",
      "Epoch 014 | Train Loss: 0.4452 | Val Loss: 0.4313 | Time: 4.77s\r\n",
      "Epoch 015 | Train Loss: 0.4482 | Val Loss: 0.4348 | Time: 4.74s\r\n",
      "Epoch 016 | Train Loss: 0.4408 | Val Loss: 0.4322 | Time: 4.73s\r\n",
      "Epoch 017 | Train Loss: 0.4473 | Val Loss: 0.4276 | Time: 4.87s\r\n",
      "Epoch 018 | Train Loss: 0.4515 | Val Loss: 0.4271 | Time: 4.77s\r\n",
      "Epoch 019 | Train Loss: 0.4456 | Val Loss: 0.4292 | Time: 4.74s\r\n",
      "Epoch 020 | Train Loss: 0.4443 | Val Loss: 0.4226 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4226)\r\n",
      "Epoch 021 | Train Loss: 0.4426 | Val Loss: 0.4258 | Time: 4.72s\r\n",
      "Epoch 022 | Train Loss: 0.4388 | Val Loss: 0.4269 | Time: 4.78s\r\n",
      "Epoch 023 | Train Loss: 0.4340 | Val Loss: 0.4288 | Time: 4.93s\r\n",
      "Epoch 024 | Train Loss: 0.4388 | Val Loss: 0.4280 | Time: 4.77s\r\n",
      "Epoch 025 | Train Loss: 0.4421 | Val Loss: 0.4281 | Time: 4.72s\r\n",
      "Epoch 026 | Train Loss: 0.4391 | Val Loss: 0.4282 | Time: 4.78s\r\n",
      "Epoch 027 | Train Loss: 0.4395 | Val Loss: 0.4275 | Time: 4.73s\r\n",
      "Epoch 028 | Train Loss: 0.4397 | Val Loss: 0.4272 | Time: 4.75s\r\n",
      "Epoch 029 | Train Loss: 0.4364 | Val Loss: 0.4223 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4223)\r\n",
      "Epoch 030 | Train Loss: 0.4348 | Val Loss: 0.4271 | Time: 4.99s\r\n",
      "Epoch 031 | Train Loss: 0.4343 | Val Loss: 0.4241 | Time: 4.74s\r\n",
      "Epoch 032 | Train Loss: 0.4360 | Val Loss: 0.4279 | Time: 4.76s\r\n",
      "Epoch 033 | Train Loss: 0.4372 | Val Loss: 0.4274 | Time: 4.77s\r\n",
      "Epoch 034 | Train Loss: 0.4408 | Val Loss: 0.4230 | Time: 4.73s\r\n",
      "Epoch 035 | Train Loss: 0.4346 | Val Loss: 0.4262 | Time: 4.77s\r\n",
      "Epoch 036 | Train Loss: 0.4353 | Val Loss: 0.4222 | Time: 4.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 037 | Train Loss: 0.4325 | Val Loss: 0.4213 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4213)\r\n",
      "Epoch 038 | Train Loss: 0.4334 | Val Loss: 0.4250 | Time: 4.73s\r\n",
      "Epoch 039 | Train Loss: 0.4340 | Val Loss: 0.4191 | Time: 4.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 040 | Train Loss: 0.4348 | Val Loss: 0.4199 | Time: 4.73s\r\n",
      "Epoch 041 | Train Loss: 0.4352 | Val Loss: 0.4218 | Time: 4.75s\r\n",
      "Epoch 042 | Train Loss: 0.4314 | Val Loss: 0.4186 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 043 | Train Loss: 0.4262 | Val Loss: 0.4195 | Time: 4.88s\r\n",
      "Epoch 044 | Train Loss: 0.4322 | Val Loss: 0.4252 | Time: 4.77s\r\n",
      "Epoch 045 | Train Loss: 0.4362 | Val Loss: 0.4240 | Time: 4.73s\r\n",
      "Epoch 046 | Train Loss: 0.4315 | Val Loss: 0.4220 | Time: 4.73s\r\n",
      "Epoch 047 | Train Loss: 0.4332 | Val Loss: 0.4238 | Time: 4.74s\r\n",
      "Epoch 048 | Train Loss: 0.4276 | Val Loss: 0.4202 | Time: 4.68s\r\n",
      "Epoch 049 | Train Loss: 0.4317 | Val Loss: 0.4275 | Time: 4.71s\r\n",
      "✅ Meilleur modèle Random chargé (époque 42, val_loss: 0.4186)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingMovies      (CAT): 0.0923\r\n",
      "   2. Partner              (CAT): 0.0755\r\n",
      "   3. PhoneService         (CAT): 0.0733\r\n",
      "   4. tenure               (NUM): 0.0703\r\n",
      "   5. PaperlessBilling     (CAT): 0.0673\r\n",
      "   6. Dependents           (CAT): 0.0647\r\n",
      "   7. TotalCharges         (NUM): 0.0627\r\n",
      "   8. OnlineSecurity       (CAT): 0.0624\r\n",
      "   9. OnlineBackup         (CAT): 0.0624\r\n",
      "  10. SeniorCitizen        (CAT): 0.0618\r\n",
      "  11. gender               (CAT): 0.0617\r\n",
      "  12. PaymentMethod        (CAT): 0.0617\r\n",
      "  13. InternetService      (CAT): 0.0615\r\n",
      "  14. TechSupport          (CAT): 0.0614\r\n",
      "  15. MonthlyCharges       (NUM): 0.0609\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingMovies     : 0.0923\r\n",
      "   2. Partner             : 0.0755\r\n",
      "   3. PhoneService        : 0.0733\r\n",
      "   4. tenure              : 0.0703\r\n",
      "   5. PaperlessBilling    : 0.0673\r\n",
      "   6. Dependents          : 0.0647\r\n",
      "   7. TotalCharges        : 0.0627\r\n",
      "   8. OnlineSecurity      : 0.0624\r\n",
      "   9. OnlineBackup        : 0.0624\r\n",
      "  10. SeniorCitizen       : 0.0618\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_4/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 419.3s ===\r\n",
      "\u001b[32m[I 2025-07-19 20:05:38,807]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 5, 'n_heads_stage1': 2, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.221285811931918, 'ffn_dropout_stage1': 0.10183941032332594, 'residual_dropout_stage1': 0.11014715428660321, 'lr_stage1': 0.00450824050248068, 'weight_decay_stage1': 1.0600050132100276e-06, 'd_token_stage2': 64, 'n_blocks_stage2': 3, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 128, 'attention_dropout_stage2': 0.21366172066709432, 'ffn_dropout_stage2': 0.1187349535656185, 'residual_dropout_stage2': 0.13677158030594336, 'lr_stage2': 0.00011502956321912731, 'weight_decay_stage2': 1.6593890383815802e-05, 'batch_size': 32, 'patience': 25, 'embedding_type': 'L', 'M': 15, 'k': 3}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  20%|█▌      | 5/25 [1:52:41<7:18:55, 1316.75s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 24,001 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5256 | Val Loss: 0.4564 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4564)\r\n",
      "Epoch 001 | Train Loss: 0.4558 | Val Loss: 0.4286 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4286)\r\n",
      "Epoch 002 | Train Loss: 0.4416 | Val Loss: 0.4178 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 003 | Train Loss: 0.4367 | Val Loss: 0.4125 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4125)\r\n",
      "Epoch 004 | Train Loss: 0.4387 | Val Loss: 0.4151 | Time: 0.87s\r\n",
      "Epoch 005 | Train Loss: 0.4306 | Val Loss: 0.4144 | Time: 0.88s\r\n",
      "Epoch 006 | Train Loss: 0.4269 | Val Loss: 0.4141 | Time: 0.88s\r\n",
      "Epoch 007 | Train Loss: 0.4269 | Val Loss: 0.4149 | Time: 0.93s\r\n",
      "Epoch 008 | Train Loss: 0.4255 | Val Loss: 0.4075 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4075)\r\n",
      "Epoch 009 | Train Loss: 0.4203 | Val Loss: 0.4120 | Time: 0.89s\r\n",
      "Epoch 010 | Train Loss: 0.4207 | Val Loss: 0.4081 | Time: 0.88s\r\n",
      "Epoch 011 | Train Loss: 0.4225 | Val Loss: 0.4082 | Time: 0.89s\r\n",
      "Epoch 012 | Train Loss: 0.4222 | Val Loss: 0.4071 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4071)\r\n",
      "Epoch 013 | Train Loss: 0.4212 | Val Loss: 0.4073 | Time: 0.88s\r\n",
      "Epoch 014 | Train Loss: 0.4176 | Val Loss: 0.4087 | Time: 0.89s\r\n",
      "Epoch 015 | Train Loss: 0.4205 | Val Loss: 0.4129 | Time: 0.88s\r\n",
      "Epoch 016 | Train Loss: 0.4195 | Val Loss: 0.4118 | Time: 0.94s\r\n",
      "Epoch 017 | Train Loss: 0.4178 | Val Loss: 0.4076 | Time: 0.91s\r\n",
      "Epoch 018 | Train Loss: 0.4167 | Val Loss: 0.4114 | Time: 0.91s\r\n",
      "Epoch 019 | Train Loss: 0.4172 | Val Loss: 0.4112 | Time: 0.87s\r\n",
      "Epoch 020 | Train Loss: 0.4139 | Val Loss: 0.4108 | Time: 0.87s\r\n",
      "Epoch 021 | Train Loss: 0.4133 | Val Loss: 0.4136 | Time: 0.88s\r\n",
      "Epoch 022 | Train Loss: 0.4129 | Val Loss: 0.4120 | Time: 0.88s\r\n",
      "Epoch 023 | Train Loss: 0.4137 | Val Loss: 0.4079 | Time: 0.88s\r\n",
      "Epoch 024 | Train Loss: 0.4161 | Val Loss: 0.4141 | Time: 0.88s\r\n",
      "Epoch 025 | Train Loss: 0.4133 | Val Loss: 0.4130 | Time: 0.88s\r\n",
      "Epoch 026 | Train Loss: 0.4124 | Val Loss: 0.4125 | Time: 0.88s\r\n",
      "Epoch 027 | Train Loss: 0.4115 | Val Loss: 0.4102 | Time: 0.88s\r\n",
      "Epoch 028 | Train Loss: 0.4125 | Val Loss: 0.4085 | Time: 0.88s\r\n",
      "Epoch 029 | Train Loss: 0.4103 | Val Loss: 0.4090 | Time: 0.89s\r\n",
      "Epoch 030 | Train Loss: 0.4086 | Val Loss: 0.4137 | Time: 0.91s\r\n",
      "Epoch 031 | Train Loss: 0.4082 | Val Loss: 0.4139 | Time: 0.89s\r\n",
      "Epoch 032 | Train Loss: 0.4105 | Val Loss: 0.4104 | Time: 0.89s\r\n",
      "Epoch 033 | Train Loss: 0.4064 | Val Loss: 0.4138 | Time: 0.89s\r\n",
      "Epoch 034 | Train Loss: 0.4098 | Val Loss: 0.4142 | Time: 1.03s\r\n",
      "Epoch 035 | Train Loss: 0.4098 | Val Loss: 0.4124 | Time: 0.95s\r\n",
      "Epoch 036 | Train Loss: 0.4095 | Val Loss: 0.4199 | Time: 0.90s\r\n",
      "Epoch 037 | Train Loss: 0.4088 | Val Loss: 0.4193 | Time: 0.88s\r\n",
      "\r\n",
      "Early stopping à l'époque 37 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 12, val_loss: 0.4071)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.0758\r\n",
      "   2. InternetService     : 0.0684\r\n",
      "   3. SeniorCitizen       : 0.0649\r\n",
      "   4. TotalCharges        : 0.0590\r\n",
      "   5. Contract            : 0.0579\r\n",
      "   6. Partner             : 0.0574\r\n",
      "   7. OnlineSecurity      : 0.0529\r\n",
      "   8. MultipleLines       : 0.0526\r\n",
      "   9. TechSupport         : 0.0525\r\n",
      "  10. PaperlessBilling    : 0.0511\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MonthlyCharges       (NUM): 0.0758\r\n",
      "   2. InternetService      (CAT): 0.0684\r\n",
      "   3. SeniorCitizen        (CAT): 0.0649\r\n",
      "   4. TotalCharges         (NUM): 0.0590\r\n",
      "   5. Contract             (CAT): 0.0579\r\n",
      "   6. Partner              (CAT): 0.0574\r\n",
      "   7. OnlineSecurity       (CAT): 0.0529\r\n",
      "   8. MultipleLines        (CAT): 0.0526\r\n",
      "   9. TechSupport          (CAT): 0.0525\r\n",
      "  10. PaperlessBilling     (CAT): 0.0511\r\n",
      "  11. DeviceProtection     (CAT): 0.0502\r\n",
      "  12. PhoneService         (CAT): 0.0498\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges'] → indices [1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['InternetService', 'SeniorCitizen', 'Contract', 'Partner', 'OnlineSecurity', 'MultipleLines', 'TechSupport', 'PaperlessBilling', 'DeviceProtection', 'PhoneService'] → indices [6, 1, 13, 2, 7, 5, 10, 14, 9, 4]\r\n",
      "📊 Features sélectionnées: 2 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 46,113 paramètres\r\n",
      "🔗 Sparsité d'attention: 78.70%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5510 | Val Loss: 0.4825 | Time: 2.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4825)\r\n",
      "Epoch 001 | Train Loss: 0.4945 | Val Loss: 0.4798 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4798)\r\n",
      "Epoch 002 | Train Loss: 0.4787 | Val Loss: 0.4550 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4550)\r\n",
      "Epoch 003 | Train Loss: 0.4653 | Val Loss: 0.4545 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4545)\r\n",
      "Epoch 004 | Train Loss: 0.4684 | Val Loss: 0.4727 | Time: 2.43s\r\n",
      "Epoch 005 | Train Loss: 0.4679 | Val Loss: 0.4622 | Time: 2.39s\r\n",
      "Epoch 006 | Train Loss: 0.4733 | Val Loss: 0.4626 | Time: 2.39s\r\n",
      "Epoch 007 | Train Loss: 0.4623 | Val Loss: 0.4875 | Time: 2.41s\r\n",
      "Epoch 008 | Train Loss: 0.4686 | Val Loss: 0.4600 | Time: 2.42s\r\n",
      "Epoch 009 | Train Loss: 0.4648 | Val Loss: 0.4591 | Time: 2.52s\r\n",
      "Epoch 010 | Train Loss: 0.4661 | Val Loss: 0.4639 | Time: 2.53s\r\n",
      "Epoch 011 | Train Loss: 0.4692 | Val Loss: 0.4779 | Time: 2.50s\r\n",
      "Epoch 012 | Train Loss: 0.4688 | Val Loss: 0.4672 | Time: 2.41s\r\n",
      "Epoch 013 | Train Loss: 0.4583 | Val Loss: 0.4620 | Time: 2.44s\r\n",
      "Epoch 014 | Train Loss: 0.4628 | Val Loss: 0.4578 | Time: 2.36s\r\n",
      "Epoch 015 | Train Loss: 0.4683 | Val Loss: 0.4704 | Time: 2.41s\r\n",
      "Epoch 016 | Train Loss: 0.4773 | Val Loss: 0.4684 | Time: 2.42s\r\n",
      "Epoch 017 | Train Loss: 0.4696 | Val Loss: 0.4470 | Time: 2.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4470)\r\n",
      "Epoch 018 | Train Loss: 0.4618 | Val Loss: 0.4668 | Time: 2.41s\r\n",
      "Epoch 019 | Train Loss: 0.4610 | Val Loss: 0.4504 | Time: 2.40s\r\n",
      "Epoch 020 | Train Loss: 0.4593 | Val Loss: 0.5457 | Time: 2.41s\r\n",
      "Epoch 021 | Train Loss: 0.4798 | Val Loss: 0.4924 | Time: 2.42s\r\n",
      "Epoch 022 | Train Loss: 0.4720 | Val Loss: 0.4529 | Time: 2.40s\r\n",
      "Epoch 023 | Train Loss: 0.4731 | Val Loss: 0.4661 | Time: 2.42s\r\n",
      "Epoch 024 | Train Loss: 0.4795 | Val Loss: 0.4605 | Time: 2.51s\r\n",
      "Epoch 025 | Train Loss: 0.4787 | Val Loss: 0.4531 | Time: 2.52s\r\n",
      "Epoch 026 | Train Loss: 0.4709 | Val Loss: 0.4461 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4461)\r\n",
      "Epoch 027 | Train Loss: 0.4648 | Val Loss: 0.4646 | Time: 2.38s\r\n",
      "Epoch 028 | Train Loss: 0.4553 | Val Loss: 0.4572 | Time: 2.37s\r\n",
      "Epoch 029 | Train Loss: 0.4736 | Val Loss: 0.4582 | Time: 2.42s\r\n",
      "Epoch 030 | Train Loss: 0.4618 | Val Loss: 0.4472 | Time: 2.36s\r\n",
      "Epoch 031 | Train Loss: 0.4609 | Val Loss: 0.4424 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4424)\r\n",
      "Epoch 032 | Train Loss: 0.4592 | Val Loss: 0.4527 | Time: 2.40s\r\n",
      "Epoch 033 | Train Loss: 0.4688 | Val Loss: 0.4552 | Time: 2.45s\r\n",
      "Epoch 034 | Train Loss: 0.4703 | Val Loss: 0.4568 | Time: 2.37s\r\n",
      "Epoch 035 | Train Loss: 0.4696 | Val Loss: 0.4535 | Time: 2.39s\r\n",
      "Epoch 036 | Train Loss: 0.4685 | Val Loss: 0.4476 | Time: 2.35s\r\n",
      "Epoch 037 | Train Loss: 0.4745 | Val Loss: 0.5032 | Time: 2.63s\r\n",
      "Epoch 038 | Train Loss: 0.4747 | Val Loss: 0.4711 | Time: 2.37s\r\n",
      "Epoch 039 | Train Loss: 0.4747 | Val Loss: 0.4797 | Time: 2.37s\r\n",
      "Epoch 040 | Train Loss: 0.4662 | Val Loss: 0.4639 | Time: 2.38s\r\n",
      "Epoch 041 | Train Loss: 0.4647 | Val Loss: 0.4670 | Time: 2.40s\r\n",
      "Epoch 042 | Train Loss: 0.4617 | Val Loss: 0.4514 | Time: 2.41s\r\n",
      "Epoch 043 | Train Loss: 0.4667 | Val Loss: 0.4544 | Time: 2.38s\r\n",
      "Epoch 044 | Train Loss: 0.4657 | Val Loss: 0.4558 | Time: 2.39s\r\n",
      "Epoch 045 | Train Loss: 0.4633 | Val Loss: 0.4602 | Time: 2.39s\r\n",
      "Epoch 046 | Train Loss: 0.4720 | Val Loss: 0.4530 | Time: 2.42s\r\n",
      "Epoch 047 | Train Loss: 0.4688 | Val Loss: 0.4625 | Time: 2.37s\r\n",
      "Epoch 048 | Train Loss: 0.4666 | Val Loss: 0.4686 | Time: 2.36s\r\n",
      "Epoch 049 | Train Loss: 0.4658 | Val Loss: 0.4609 | Time: 2.36s\r\n",
      "✅ Meilleur modèle Random chargé (époque 31, val_loss: 0.4424)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.3325\r\n",
      "   2. MonthlyCharges       (NUM): 0.1930\r\n",
      "   3. OnlineSecurity       (CAT): 0.1473\r\n",
      "   4. PaperlessBilling     (CAT): 0.0431\r\n",
      "   5. DeviceProtection     (CAT): 0.0421\r\n",
      "   6. MultipleLines        (CAT): 0.0418\r\n",
      "   7. InternetService      (CAT): 0.0396\r\n",
      "   8. TechSupport          (CAT): 0.0391\r\n",
      "   9. Partner              (CAT): 0.0347\r\n",
      "  10. TotalCharges         (NUM): 0.0347\r\n",
      "  11. Contract             (CAT): 0.0305\r\n",
      "  12. SeniorCitizen        (CAT): 0.0218\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.3325\r\n",
      "   2. MonthlyCharges      : 0.1930\r\n",
      "   3. OnlineSecurity      : 0.1473\r\n",
      "   4. PaperlessBilling    : 0.0431\r\n",
      "   5. DeviceProtection    : 0.0421\r\n",
      "   6. MultipleLines       : 0.0418\r\n",
      "   7. InternetService     : 0.0396\r\n",
      "   8. TechSupport         : 0.0391\r\n",
      "   9. Partner             : 0.0347\r\n",
      "  10. TotalCharges        : 0.0347\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 158.0s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 24,001 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5204 | Val Loss: 0.4592 | Time: 0.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4592)\r\n",
      "Epoch 001 | Train Loss: 0.4449 | Val Loss: 0.4349 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4349)\r\n",
      "Epoch 002 | Train Loss: 0.4292 | Val Loss: 0.4278 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4278)\r\n",
      "Epoch 003 | Train Loss: 0.4220 | Val Loss: 0.4234 | Time: 0.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4234)\r\n",
      "Epoch 004 | Train Loss: 0.4174 | Val Loss: 0.4207 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4207)\r\n",
      "Epoch 005 | Train Loss: 0.4119 | Val Loss: 0.4216 | Time: 0.87s\r\n",
      "Epoch 006 | Train Loss: 0.4103 | Val Loss: 0.4244 | Time: 0.88s\r\n",
      "Epoch 007 | Train Loss: 0.4117 | Val Loss: 0.4206 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4206)\r\n",
      "Epoch 008 | Train Loss: 0.4115 | Val Loss: 0.4200 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 009 | Train Loss: 0.4079 | Val Loss: 0.4213 | Time: 0.91s\r\n",
      "Epoch 010 | Train Loss: 0.4084 | Val Loss: 0.4186 | Time: 0.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 011 | Train Loss: 0.4088 | Val Loss: 0.4182 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4182)\r\n",
      "Epoch 012 | Train Loss: 0.4075 | Val Loss: 0.4200 | Time: 0.88s\r\n",
      "Epoch 013 | Train Loss: 0.4046 | Val Loss: 0.4191 | Time: 0.89s\r\n",
      "Epoch 014 | Train Loss: 0.4055 | Val Loss: 0.4196 | Time: 0.89s\r\n",
      "Epoch 015 | Train Loss: 0.4078 | Val Loss: 0.4163 | Time: 0.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4163)\r\n",
      "Epoch 016 | Train Loss: 0.4023 | Val Loss: 0.4172 | Time: 0.89s\r\n",
      "Epoch 017 | Train Loss: 0.4053 | Val Loss: 0.4185 | Time: 0.89s\r\n",
      "Epoch 018 | Train Loss: 0.4043 | Val Loss: 0.4171 | Time: 0.89s\r\n",
      "Epoch 019 | Train Loss: 0.4023 | Val Loss: 0.4159 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 020 | Train Loss: 0.4008 | Val Loss: 0.4169 | Time: 0.92s\r\n",
      "Epoch 021 | Train Loss: 0.4014 | Val Loss: 0.4201 | Time: 0.89s\r\n",
      "Epoch 022 | Train Loss: 0.3992 | Val Loss: 0.4178 | Time: 0.89s\r\n",
      "Epoch 023 | Train Loss: 0.3996 | Val Loss: 0.4186 | Time: 0.89s\r\n",
      "Epoch 024 | Train Loss: 0.3994 | Val Loss: 0.4181 | Time: 0.90s\r\n",
      "Epoch 025 | Train Loss: 0.3996 | Val Loss: 0.4186 | Time: 0.89s\r\n",
      "Epoch 026 | Train Loss: 0.3987 | Val Loss: 0.4240 | Time: 0.89s\r\n",
      "Epoch 027 | Train Loss: 0.3977 | Val Loss: 0.4196 | Time: 0.89s\r\n",
      "Epoch 028 | Train Loss: 0.3979 | Val Loss: 0.4205 | Time: 0.89s\r\n",
      "Epoch 029 | Train Loss: 0.3958 | Val Loss: 0.4224 | Time: 0.89s\r\n",
      "Epoch 030 | Train Loss: 0.3935 | Val Loss: 0.4221 | Time: 0.89s\r\n",
      "Epoch 031 | Train Loss: 0.3967 | Val Loss: 0.4207 | Time: 0.92s\r\n",
      "Epoch 032 | Train Loss: 0.3988 | Val Loss: 0.4191 | Time: 0.88s\r\n",
      "Epoch 033 | Train Loss: 0.3986 | Val Loss: 0.4226 | Time: 0.87s\r\n",
      "Epoch 034 | Train Loss: 0.3925 | Val Loss: 0.4233 | Time: 0.98s\r\n",
      "Epoch 035 | Train Loss: 0.3928 | Val Loss: 0.4241 | Time: 0.99s\r\n",
      "Epoch 036 | Train Loss: 0.3907 | Val Loss: 0.4239 | Time: 0.90s\r\n",
      "Epoch 037 | Train Loss: 0.3940 | Val Loss: 0.4212 | Time: 0.90s\r\n",
      "Epoch 038 | Train Loss: 0.3951 | Val Loss: 0.4176 | Time: 0.89s\r\n",
      "Epoch 039 | Train Loss: 0.3932 | Val Loss: 0.4198 | Time: 0.89s\r\n",
      "Epoch 040 | Train Loss: 0.3950 | Val Loss: 0.4215 | Time: 0.89s\r\n",
      "Epoch 041 | Train Loss: 0.3896 | Val Loss: 0.4279 | Time: 0.89s\r\n",
      "Epoch 042 | Train Loss: 0.3936 | Val Loss: 0.4252 | Time: 0.93s\r\n",
      "Epoch 043 | Train Loss: 0.3906 | Val Loss: 0.4227 | Time: 0.90s\r\n",
      "Epoch 044 | Train Loss: 0.3891 | Val Loss: 0.4243 | Time: 0.88s\r\n",
      "\r\n",
      "Early stopping à l'époque 44 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 19, val_loss: 0.4159)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0757\r\n",
      "   2. StreamingTV         : 0.0670\r\n",
      "   3. InternetService     : 0.0649\r\n",
      "   4. gender              : 0.0647\r\n",
      "   5. SeniorCitizen       : 0.0644\r\n",
      "   6. OnlineBackup        : 0.0615\r\n",
      "   7. MonthlyCharges      : 0.0575\r\n",
      "   8. MultipleLines       : 0.0545\r\n",
      "   9. Contract            : 0.0535\r\n",
      "  10. StreamingMovies     : 0.0507\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0757\r\n",
      "   2. StreamingTV          (CAT): 0.0670\r\n",
      "   3. InternetService      (CAT): 0.0649\r\n",
      "   4. gender               (CAT): 0.0647\r\n",
      "   5. SeniorCitizen        (CAT): 0.0644\r\n",
      "   6. OnlineBackup         (CAT): 0.0615\r\n",
      "   7. MonthlyCharges       (NUM): 0.0575\r\n",
      "   8. MultipleLines        (CAT): 0.0545\r\n",
      "   9. Contract             (CAT): 0.0535\r\n",
      "  10. StreamingMovies      (CAT): 0.0507\r\n",
      "  11. Partner              (CAT): 0.0483\r\n",
      "  12. PaperlessBilling     (CAT): 0.0459\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'StreamingTV', 'InternetService', 'gender', 'SeniorCitizen', 'OnlineBackup', 'MultipleLines', 'Contract', 'StreamingMovies', 'Partner', 'PaperlessBilling'] → indices [15, 11, 6, 0, 1, 8, 5, 13, 12, 2, 14]\r\n",
      "📊 Features sélectionnées: 1 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 46,161 paramètres\r\n",
      "🔗 Sparsité d'attention: 78.70%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5504 | Val Loss: 0.5337 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5337)\r\n",
      "Epoch 001 | Train Loss: 0.5181 | Val Loss: 0.5231 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5231)\r\n",
      "Epoch 002 | Train Loss: 0.5052 | Val Loss: 0.4810 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4810)\r\n",
      "Epoch 003 | Train Loss: 0.4874 | Val Loss: 0.4863 | Time: 2.40s\r\n",
      "Epoch 004 | Train Loss: 0.4847 | Val Loss: 0.4842 | Time: 2.38s\r\n",
      "Epoch 005 | Train Loss: 0.4897 | Val Loss: 0.4769 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4769)\r\n",
      "Epoch 006 | Train Loss: 0.4887 | Val Loss: 0.4764 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4764)\r\n",
      "Epoch 007 | Train Loss: 0.4832 | Val Loss: 0.4759 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4759)\r\n",
      "Epoch 008 | Train Loss: 0.4859 | Val Loss: 0.4791 | Time: 2.47s\r\n",
      "Epoch 009 | Train Loss: 0.4879 | Val Loss: 0.4791 | Time: 2.43s\r\n",
      "Epoch 010 | Train Loss: 0.4904 | Val Loss: 0.4751 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4751)\r\n",
      "Epoch 011 | Train Loss: 0.4801 | Val Loss: 0.4718 | Time: 2.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4718)\r\n",
      "Epoch 012 | Train Loss: 0.4904 | Val Loss: 0.4892 | Time: 2.40s\r\n",
      "Epoch 013 | Train Loss: 0.4978 | Val Loss: 0.4784 | Time: 2.38s\r\n",
      "Epoch 014 | Train Loss: 0.5047 | Val Loss: 0.4870 | Time: 2.39s\r\n",
      "Epoch 015 | Train Loss: 0.4932 | Val Loss: 0.4765 | Time: 2.42s\r\n",
      "Epoch 016 | Train Loss: 0.4906 | Val Loss: 0.4769 | Time: 2.40s\r\n",
      "Epoch 017 | Train Loss: 0.4809 | Val Loss: 0.4671 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4671)\r\n",
      "Epoch 018 | Train Loss: 0.4708 | Val Loss: 0.4663 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4663)\r\n",
      "Epoch 019 | Train Loss: 0.4668 | Val Loss: 0.4718 | Time: 2.43s\r\n",
      "Epoch 020 | Train Loss: 0.4673 | Val Loss: 0.5279 | Time: 2.41s\r\n",
      "Epoch 021 | Train Loss: 0.4843 | Val Loss: 0.4733 | Time: 2.43s\r\n",
      "Epoch 022 | Train Loss: 0.4927 | Val Loss: 0.4640 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4640)\r\n",
      "Epoch 023 | Train Loss: 0.5004 | Val Loss: 0.4830 | Time: 2.40s\r\n",
      "Epoch 024 | Train Loss: 0.4847 | Val Loss: 0.4805 | Time: 2.41s\r\n",
      "Epoch 025 | Train Loss: 0.4841 | Val Loss: 0.4711 | Time: 2.40s\r\n",
      "Epoch 026 | Train Loss: 0.4733 | Val Loss: 0.4667 | Time: 2.41s\r\n",
      "Epoch 027 | Train Loss: 0.4880 | Val Loss: 0.4684 | Time: 2.43s\r\n",
      "Epoch 028 | Train Loss: 0.4773 | Val Loss: 0.4644 | Time: 2.38s\r\n",
      "Epoch 029 | Train Loss: 0.4710 | Val Loss: 0.4624 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4624)\r\n",
      "Epoch 030 | Train Loss: 0.4740 | Val Loss: 0.4648 | Time: 2.39s\r\n",
      "Epoch 031 | Train Loss: 0.4717 | Val Loss: 0.4581 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4581)\r\n",
      "Epoch 032 | Train Loss: 0.4765 | Val Loss: 0.4590 | Time: 2.44s\r\n",
      "Epoch 033 | Train Loss: 0.4679 | Val Loss: 0.4553 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4553)\r\n",
      "Epoch 034 | Train Loss: 0.4704 | Val Loss: 0.4600 | Time: 2.41s\r\n",
      "Epoch 035 | Train Loss: 0.4674 | Val Loss: 0.4626 | Time: 2.47s\r\n",
      "Epoch 036 | Train Loss: 0.4682 | Val Loss: 0.4572 | Time: 2.39s\r\n",
      "Epoch 037 | Train Loss: 0.4714 | Val Loss: 0.4591 | Time: 2.37s\r\n",
      "Epoch 038 | Train Loss: 0.4705 | Val Loss: 0.4566 | Time: 2.40s\r\n",
      "Epoch 039 | Train Loss: 0.4706 | Val Loss: 0.4587 | Time: 2.41s\r\n",
      "Epoch 040 | Train Loss: 0.4689 | Val Loss: 0.4591 | Time: 2.44s\r\n",
      "Epoch 041 | Train Loss: 0.4664 | Val Loss: 0.4601 | Time: 2.39s\r\n",
      "Epoch 042 | Train Loss: 0.4654 | Val Loss: 0.4579 | Time: 2.39s\r\n",
      "Epoch 043 | Train Loss: 0.4718 | Val Loss: 0.4614 | Time: 2.39s\r\n",
      "Epoch 044 | Train Loss: 0.4635 | Val Loss: 0.4612 | Time: 2.43s\r\n",
      "Epoch 045 | Train Loss: 0.4646 | Val Loss: 0.4591 | Time: 2.40s\r\n",
      "Epoch 046 | Train Loss: 0.4645 | Val Loss: 0.4542 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4542)\r\n",
      "Epoch 047 | Train Loss: 0.4594 | Val Loss: 0.4549 | Time: 2.40s\r\n",
      "Epoch 048 | Train Loss: 0.4696 | Val Loss: 0.4549 | Time: 2.52s\r\n",
      "Epoch 049 | Train Loss: 0.4679 | Val Loss: 0.4548 | Time: 2.39s\r\n",
      "✅ Meilleur modèle Random chargé (époque 46, val_loss: 0.4542)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineBackup         (CAT): 0.5819\r\n",
      "   2. StreamingMovies      (CAT): 0.0789\r\n",
      "   3. SeniorCitizen        (CAT): 0.0502\r\n",
      "   4. gender               (CAT): 0.0490\r\n",
      "   5. InternetService      (CAT): 0.0474\r\n",
      "   6. Partner              (CAT): 0.0464\r\n",
      "   7. PaymentMethod        (CAT): 0.0319\r\n",
      "   8. MonthlyCharges       (NUM): 0.0307\r\n",
      "   9. Contract             (CAT): 0.0286\r\n",
      "  10. StreamingTV          (CAT): 0.0243\r\n",
      "  11. MultipleLines        (CAT): 0.0179\r\n",
      "  12. PaperlessBilling     (CAT): 0.0130\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.5819\r\n",
      "   2. StreamingMovies     : 0.0789\r\n",
      "   3. SeniorCitizen       : 0.0502\r\n",
      "   4. gender              : 0.0490\r\n",
      "   5. InternetService     : 0.0474\r\n",
      "   6. Partner             : 0.0464\r\n",
      "   7. PaymentMethod       : 0.0319\r\n",
      "   8. MonthlyCharges      : 0.0307\r\n",
      "   9. Contract            : 0.0286\r\n",
      "  10. StreamingTV         : 0.0243\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 164.1s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 24,001 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5032 | Val Loss: 0.4402 | Time: 0.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4402)\r\n",
      "Epoch 001 | Train Loss: 0.4478 | Val Loss: 0.4219 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4219)\r\n",
      "Epoch 002 | Train Loss: 0.4370 | Val Loss: 0.4209 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 003 | Train Loss: 0.4280 | Val Loss: 0.4155 | Time: 0.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4155)\r\n",
      "Epoch 004 | Train Loss: 0.4259 | Val Loss: 0.4198 | Time: 0.92s\r\n",
      "Epoch 005 | Train Loss: 0.4242 | Val Loss: 0.4150 | Time: 0.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4150)\r\n",
      "Epoch 006 | Train Loss: 0.4233 | Val Loss: 0.4142 | Time: 0.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4142)\r\n",
      "Epoch 007 | Train Loss: 0.4220 | Val Loss: 0.4147 | Time: 0.91s\r\n",
      "Epoch 008 | Train Loss: 0.4206 | Val Loss: 0.4125 | Time: 0.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4125)\r\n",
      "Epoch 009 | Train Loss: 0.4198 | Val Loss: 0.4089 | Time: 0.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4089)\r\n",
      "Epoch 010 | Train Loss: 0.4167 | Val Loss: 0.4128 | Time: 0.89s\r\n",
      "Epoch 011 | Train Loss: 0.4212 | Val Loss: 0.4119 | Time: 0.93s\r\n",
      "Epoch 012 | Train Loss: 0.4158 | Val Loss: 0.4138 | Time: 0.90s\r\n",
      "Epoch 013 | Train Loss: 0.4174 | Val Loss: 0.4122 | Time: 0.90s\r\n",
      "Epoch 014 | Train Loss: 0.4161 | Val Loss: 0.4122 | Time: 0.89s\r\n",
      "Epoch 015 | Train Loss: 0.4114 | Val Loss: 0.4146 | Time: 0.92s\r\n",
      "Epoch 016 | Train Loss: 0.4139 | Val Loss: 0.4160 | Time: 0.89s\r\n",
      "Epoch 017 | Train Loss: 0.4160 | Val Loss: 0.4123 | Time: 0.92s\r\n",
      "Epoch 018 | Train Loss: 0.4136 | Val Loss: 0.4141 | Time: 0.96s\r\n",
      "Epoch 019 | Train Loss: 0.4112 | Val Loss: 0.4144 | Time: 0.89s\r\n",
      "Epoch 020 | Train Loss: 0.4128 | Val Loss: 0.4150 | Time: 0.91s\r\n",
      "Epoch 021 | Train Loss: 0.4085 | Val Loss: 0.4157 | Time: 0.89s\r\n",
      "Epoch 022 | Train Loss: 0.4100 | Val Loss: 0.4197 | Time: 0.89s\r\n",
      "Epoch 023 | Train Loss: 0.4097 | Val Loss: 0.4127 | Time: 0.89s\r\n",
      "Epoch 024 | Train Loss: 0.4064 | Val Loss: 0.4135 | Time: 0.89s\r\n",
      "Epoch 025 | Train Loss: 0.4106 | Val Loss: 0.4120 | Time: 0.89s\r\n",
      "Epoch 026 | Train Loss: 0.4067 | Val Loss: 0.4136 | Time: 0.93s\r\n",
      "Epoch 027 | Train Loss: 0.4064 | Val Loss: 0.4179 | Time: 0.98s\r\n",
      "Epoch 028 | Train Loss: 0.4063 | Val Loss: 0.4152 | Time: 0.99s\r\n",
      "Epoch 029 | Train Loss: 0.4081 | Val Loss: 0.4167 | Time: 0.89s\r\n",
      "Epoch 030 | Train Loss: 0.4051 | Val Loss: 0.4234 | Time: 0.89s\r\n",
      "Epoch 031 | Train Loss: 0.4037 | Val Loss: 0.4223 | Time: 0.89s\r\n",
      "Epoch 032 | Train Loss: 0.4069 | Val Loss: 0.4248 | Time: 0.88s\r\n",
      "Epoch 033 | Train Loss: 0.4053 | Val Loss: 0.4210 | Time: 0.87s\r\n",
      "Epoch 034 | Train Loss: 0.4035 | Val Loss: 0.4247 | Time: 0.88s\r\n",
      "\r\n",
      "Early stopping à l'époque 34 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 9, val_loss: 0.4089)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.0834\r\n",
      "   2. PhoneService        : 0.0603\r\n",
      "   3. OnlineSecurity      : 0.0594\r\n",
      "   4. TotalCharges        : 0.0579\r\n",
      "   5. StreamingTV         : 0.0574\r\n",
      "   6. InternetService     : 0.0569\r\n",
      "   7. DeviceProtection    : 0.0554\r\n",
      "   8. Contract            : 0.0522\r\n",
      "   9. PaperlessBilling    : 0.0512\r\n",
      "  10. MultipleLines       : 0.0505\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. OnlineBackup         (CAT): 0.0834\r\n",
      "   2. PhoneService         (CAT): 0.0603\r\n",
      "   3. OnlineSecurity       (CAT): 0.0594\r\n",
      "   4. TotalCharges         (NUM): 0.0579\r\n",
      "   5. StreamingTV          (CAT): 0.0574\r\n",
      "   6. InternetService      (CAT): 0.0569\r\n",
      "   7. DeviceProtection     (CAT): 0.0554\r\n",
      "   8. Contract             (CAT): 0.0522\r\n",
      "   9. PaperlessBilling     (CAT): 0.0512\r\n",
      "  10. MultipleLines        (CAT): 0.0505\r\n",
      "  11. SeniorCitizen        (CAT): 0.0504\r\n",
      "  12. tenure               (NUM): 0.0501\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure'] → indices [2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['OnlineBackup', 'PhoneService', 'OnlineSecurity', 'StreamingTV', 'InternetService', 'DeviceProtection', 'Contract', 'PaperlessBilling', 'MultipleLines', 'SeniorCitizen'] → indices [8, 4, 7, 11, 6, 9, 13, 14, 5, 1]\r\n",
      "📊 Features sélectionnées: 2 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 46,129 paramètres\r\n",
      "🔗 Sparsité d'attention: 78.70%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5097 | Val Loss: 0.4740 | Time: 2.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4740)\r\n",
      "Epoch 001 | Train Loss: 0.4842 | Val Loss: 0.4426 | Time: 2.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4426)\r\n",
      "Epoch 002 | Train Loss: 0.4747 | Val Loss: 0.4531 | Time: 2.39s\r\n",
      "Epoch 003 | Train Loss: 0.4776 | Val Loss: 0.4363 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4363)\r\n",
      "Epoch 004 | Train Loss: 0.4859 | Val Loss: 0.4433 | Time: 2.44s\r\n",
      "Epoch 005 | Train Loss: 0.4756 | Val Loss: 0.4304 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4304)\r\n",
      "Epoch 006 | Train Loss: 0.4719 | Val Loss: 0.4308 | Time: 2.39s\r\n",
      "Epoch 007 | Train Loss: 0.4685 | Val Loss: 0.4289 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 008 | Train Loss: 0.4642 | Val Loss: 0.4350 | Time: 2.40s\r\n",
      "Epoch 009 | Train Loss: 0.4591 | Val Loss: 0.4265 | Time: 2.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 010 | Train Loss: 0.4541 | Val Loss: 0.4261 | Time: 2.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4261)\r\n",
      "Epoch 011 | Train Loss: 0.4612 | Val Loss: 0.4239 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "Epoch 012 | Train Loss: 0.4594 | Val Loss: 0.4284 | Time: 2.38s\r\n",
      "Epoch 013 | Train Loss: 0.4538 | Val Loss: 0.4272 | Time: 2.41s\r\n",
      "Epoch 014 | Train Loss: 0.4486 | Val Loss: 0.4268 | Time: 2.36s\r\n",
      "Epoch 015 | Train Loss: 0.4586 | Val Loss: 0.4305 | Time: 2.42s\r\n",
      "Epoch 016 | Train Loss: 0.4530 | Val Loss: 0.4286 | Time: 2.40s\r\n",
      "Epoch 017 | Train Loss: 0.4610 | Val Loss: 0.4363 | Time: 2.46s\r\n",
      "Epoch 018 | Train Loss: 0.4692 | Val Loss: 0.4367 | Time: 2.41s\r\n",
      "Epoch 019 | Train Loss: 0.4671 | Val Loss: 0.4534 | Time: 2.43s\r\n",
      "Epoch 020 | Train Loss: 0.4718 | Val Loss: 0.4387 | Time: 2.42s\r\n",
      "Epoch 021 | Train Loss: 0.4778 | Val Loss: 0.4412 | Time: 2.48s\r\n",
      "Epoch 022 | Train Loss: 0.4699 | Val Loss: 0.4323 | Time: 2.46s\r\n",
      "Epoch 023 | Train Loss: 0.4668 | Val Loss: 0.4430 | Time: 2.52s\r\n",
      "Epoch 024 | Train Loss: 0.4640 | Val Loss: 0.4252 | Time: 2.41s\r\n",
      "Epoch 025 | Train Loss: 0.4575 | Val Loss: 0.4297 | Time: 2.43s\r\n",
      "Epoch 026 | Train Loss: 0.4619 | Val Loss: 0.4232 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4232)\r\n",
      "Epoch 027 | Train Loss: 0.4579 | Val Loss: 0.4264 | Time: 2.41s\r\n",
      "Epoch 028 | Train Loss: 0.4632 | Val Loss: 0.4406 | Time: 2.42s\r\n",
      "Epoch 029 | Train Loss: 0.4670 | Val Loss: 0.4284 | Time: 2.47s\r\n",
      "Epoch 030 | Train Loss: 0.4576 | Val Loss: 0.4299 | Time: 2.41s\r\n",
      "Epoch 031 | Train Loss: 0.4572 | Val Loss: 0.4245 | Time: 2.41s\r\n",
      "Epoch 032 | Train Loss: 0.4547 | Val Loss: 0.4198 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4198)\r\n",
      "Epoch 033 | Train Loss: 0.4534 | Val Loss: 0.4218 | Time: 2.42s\r\n",
      "Epoch 034 | Train Loss: 0.4525 | Val Loss: 0.4151 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4151)\r\n",
      "Epoch 035 | Train Loss: 0.4528 | Val Loss: 0.4282 | Time: 2.40s\r\n",
      "Epoch 036 | Train Loss: 0.4471 | Val Loss: 0.4213 | Time: 2.50s\r\n",
      "Epoch 037 | Train Loss: 0.4606 | Val Loss: 0.4338 | Time: 2.41s\r\n",
      "Epoch 038 | Train Loss: 0.4610 | Val Loss: 0.4232 | Time: 2.39s\r\n",
      "Epoch 039 | Train Loss: 0.4609 | Val Loss: 0.4280 | Time: 2.41s\r\n",
      "Epoch 040 | Train Loss: 0.4596 | Val Loss: 0.4326 | Time: 2.40s\r\n",
      "Epoch 041 | Train Loss: 0.4734 | Val Loss: 0.4406 | Time: 2.43s\r\n",
      "Epoch 042 | Train Loss: 0.4711 | Val Loss: 0.4595 | Time: 2.42s\r\n",
      "Epoch 043 | Train Loss: 0.4720 | Val Loss: 0.4329 | Time: 2.39s\r\n",
      "Epoch 044 | Train Loss: 0.4725 | Val Loss: 0.4398 | Time: 2.37s\r\n",
      "Epoch 045 | Train Loss: 0.4662 | Val Loss: 0.4398 | Time: 2.40s\r\n",
      "Epoch 046 | Train Loss: 0.4676 | Val Loss: 0.4289 | Time: 2.44s\r\n",
      "Epoch 047 | Train Loss: 0.4731 | Val Loss: 0.4364 | Time: 2.40s\r\n",
      "Epoch 048 | Train Loss: 0.4679 | Val Loss: 0.4363 | Time: 2.43s\r\n",
      "Epoch 049 | Train Loss: 0.4713 | Val Loss: 0.4405 | Time: 2.47s\r\n",
      "✅ Meilleur modèle Random chargé (époque 34, val_loss: 0.4151)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Contract             (CAT): 0.0837\r\n",
      "   2. OnlineSecurity       (CAT): 0.0836\r\n",
      "   3. tenure               (NUM): 0.0835\r\n",
      "   4. StreamingTV          (CAT): 0.0834\r\n",
      "   5. PaperlessBilling     (CAT): 0.0834\r\n",
      "   6. OnlineBackup         (CAT): 0.0833\r\n",
      "   7. DeviceProtection     (CAT): 0.0833\r\n",
      "   8. InternetService      (CAT): 0.0833\r\n",
      "   9. PhoneService         (CAT): 0.0832\r\n",
      "  10. MultipleLines        (CAT): 0.0832\r\n",
      "  11. SeniorCitizen        (CAT): 0.0831\r\n",
      "  12. TotalCharges         (NUM): 0.0830\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.0837\r\n",
      "   2. OnlineSecurity      : 0.0836\r\n",
      "   3. tenure              : 0.0835\r\n",
      "   4. StreamingTV         : 0.0834\r\n",
      "   5. PaperlessBilling    : 0.0834\r\n",
      "   6. OnlineBackup        : 0.0833\r\n",
      "   7. DeviceProtection    : 0.0833\r\n",
      "   8. InternetService     : 0.0833\r\n",
      "   9. PhoneService        : 0.0832\r\n",
      "  10. MultipleLines       : 0.0832\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_5/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 156.0s ===\r\n",
      "\u001b[32m[I 2025-07-19 20:13:37,604]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 2, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 64, 'attention_dropout_stage1': 0.27022733430337137, 'ffn_dropout_stage1': 0.16338440103125554, 'residual_dropout_stage1': 0.11694927466860926, 'lr_stage1': 0.0016873495104568939, 'weight_decay_stage1': 0.04794837252202714, 'd_token_stage2': 16, 'n_blocks_stage2': 6, 'n_heads_stage2': 8, 'ffn_hidden_stage2': 128, 'attention_dropout_stage2': 0.15871836885289867, 'ffn_dropout_stage2': 0.2618722310957027, 'residual_dropout_stage2': 0.1810113394679181, 'lr_stage2': 0.02939607009739149, 'weight_decay_stage2': 0.0368300885295475, 'batch_size': 128, 'patience': 25, 'embedding_type': 'Q', 'M': 12, 'k': 6}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  24%|█▉      | 6/25 [2:00:40<5:26:45, 1031.85s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 105,121 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5052 | Val Loss: 0.4598 | Time: 4.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4598)\r\n",
      "Epoch 001 | Train Loss: 0.4415 | Val Loss: 0.4240 | Time: 4.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4240)\r\n",
      "Epoch 002 | Train Loss: 0.4320 | Val Loss: 0.4147 | Time: 4.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "Epoch 003 | Train Loss: 0.4328 | Val Loss: 0.4155 | Time: 4.37s\r\n",
      "Epoch 004 | Train Loss: 0.4257 | Val Loss: 0.4126 | Time: 4.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4126)\r\n",
      "Epoch 005 | Train Loss: 0.4266 | Val Loss: 0.4137 | Time: 4.42s\r\n",
      "Epoch 006 | Train Loss: 0.4234 | Val Loss: 0.4116 | Time: 4.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4116)\r\n",
      "Epoch 007 | Train Loss: 0.4217 | Val Loss: 0.4150 | Time: 4.39s\r\n",
      "Epoch 008 | Train Loss: 0.4214 | Val Loss: 0.4115 | Time: 4.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4115)\r\n",
      "Epoch 009 | Train Loss: 0.4183 | Val Loss: 0.4200 | Time: 4.39s\r\n",
      "Epoch 010 | Train Loss: 0.4192 | Val Loss: 0.4142 | Time: 4.40s\r\n",
      "Epoch 011 | Train Loss: 0.4177 | Val Loss: 0.4101 | Time: 4.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4101)\r\n",
      "Epoch 012 | Train Loss: 0.4189 | Val Loss: 0.4136 | Time: 4.38s\r\n",
      "Epoch 013 | Train Loss: 0.4178 | Val Loss: 0.4127 | Time: 4.51s\r\n",
      "Epoch 014 | Train Loss: 0.4171 | Val Loss: 0.4135 | Time: 4.35s\r\n",
      "Epoch 015 | Train Loss: 0.4158 | Val Loss: 0.4122 | Time: 4.39s\r\n",
      "Epoch 016 | Train Loss: 0.4130 | Val Loss: 0.4153 | Time: 4.35s\r\n",
      "Epoch 017 | Train Loss: 0.4140 | Val Loss: 0.4180 | Time: 4.39s\r\n",
      "Epoch 018 | Train Loss: 0.4131 | Val Loss: 0.4169 | Time: 4.36s\r\n",
      "Epoch 019 | Train Loss: 0.4122 | Val Loss: 0.4110 | Time: 4.37s\r\n",
      "Epoch 020 | Train Loss: 0.4153 | Val Loss: 0.4134 | Time: 4.49s\r\n",
      "Epoch 021 | Train Loss: 0.4112 | Val Loss: 0.4174 | Time: 4.34s\r\n",
      "Epoch 022 | Train Loss: 0.4098 | Val Loss: 0.4226 | Time: 4.40s\r\n",
      "Epoch 023 | Train Loss: 0.4106 | Val Loss: 0.4237 | Time: 4.39s\r\n",
      "Epoch 024 | Train Loss: 0.4076 | Val Loss: 0.4332 | Time: 4.37s\r\n",
      "Epoch 025 | Train Loss: 0.4105 | Val Loss: 0.4360 | Time: 4.39s\r\n",
      "Epoch 026 | Train Loss: 0.4147 | Val Loss: 0.4135 | Time: 4.40s\r\n",
      "Epoch 027 | Train Loss: 0.4107 | Val Loss: 0.4184 | Time: 4.45s\r\n",
      "Epoch 028 | Train Loss: 0.4071 | Val Loss: 0.4185 | Time: 4.35s\r\n",
      "Epoch 029 | Train Loss: 0.4066 | Val Loss: 0.4268 | Time: 4.38s\r\n",
      "Epoch 030 | Train Loss: 0.4102 | Val Loss: 0.4316 | Time: 4.28s\r\n",
      "Epoch 031 | Train Loss: 0.4083 | Val Loss: 0.4295 | Time: 4.33s\r\n",
      "Epoch 032 | Train Loss: 0.4080 | Val Loss: 0.4278 | Time: 4.34s\r\n",
      "Epoch 033 | Train Loss: 0.4104 | Val Loss: 0.4197 | Time: 4.39s\r\n",
      "Epoch 034 | Train Loss: 0.4046 | Val Loss: 0.4205 | Time: 4.31s\r\n",
      "Epoch 035 | Train Loss: 0.4034 | Val Loss: 0.4258 | Time: 4.44s\r\n",
      "Epoch 036 | Train Loss: 0.4030 | Val Loss: 0.4285 | Time: 4.35s\r\n",
      "Epoch 037 | Train Loss: 0.4045 | Val Loss: 0.4274 | Time: 4.36s\r\n",
      "\r\n",
      "Early stopping à l'époque 37 (patience: 26)\r\n",
      "✅ Meilleur modèle chargé (époque 11, val_loss: 0.4101)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Dependents          : 0.1254\r\n",
      "   2. gender              : 0.0859\r\n",
      "   3. tenure              : 0.0770\r\n",
      "   4. StreamingMovies     : 0.0758\r\n",
      "   5. PaymentMethod       : 0.0739\r\n",
      "   6. PhoneService        : 0.0637\r\n",
      "   7. PaperlessBilling    : 0.0502\r\n",
      "   8. Partner             : 0.0490\r\n",
      "   9. OnlineSecurity      : 0.0434\r\n",
      "  10. TotalCharges        : 0.0431\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 5 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Dependents           (CAT): 0.1254\r\n",
      "   2. gender               (CAT): 0.0859\r\n",
      "   3. tenure               (NUM): 0.0770\r\n",
      "   4. StreamingMovies      (CAT): 0.0758\r\n",
      "   5. PaymentMethod        (CAT): 0.0739\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure'] → indices [0]\r\n",
      "   - Catégorielles sélectionnées: ['Dependents', 'gender', 'StreamingMovies', 'PaymentMethod'] → indices [3, 0, 12, 15]\r\n",
      "📊 Features sélectionnées: 1 numériques, 4 catégorielles\r\n",
      "🎲 Interactions aléatoires: 2 paires\r\n",
      "Modèle Random créé avec 13,441 paramètres\r\n",
      "🔗 Sparsité d'attention: 61.11%\r\n",
      "   - Connexions feature-feature: 4\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6124 | Val Loss: 0.5348 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5348)\r\n",
      "Epoch 001 | Train Loss: 0.5204 | Val Loss: 0.4834 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4834)\r\n",
      "Epoch 002 | Train Loss: 0.4910 | Val Loss: 0.4853 | Time: 1.23s\r\n",
      "Epoch 003 | Train Loss: 0.4859 | Val Loss: 0.4738 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4738)\r\n",
      "Epoch 004 | Train Loss: 0.4857 | Val Loss: 0.4681 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4681)\r\n",
      "Epoch 005 | Train Loss: 0.4812 | Val Loss: 0.4654 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4654)\r\n",
      "Epoch 006 | Train Loss: 0.4800 | Val Loss: 0.4625 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4625)\r\n",
      "Epoch 007 | Train Loss: 0.4771 | Val Loss: 0.4636 | Time: 1.25s\r\n",
      "Epoch 008 | Train Loss: 0.4737 | Val Loss: 0.4606 | Time: 1.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4606)\r\n",
      "Epoch 009 | Train Loss: 0.4774 | Val Loss: 0.4582 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4582)\r\n",
      "Epoch 010 | Train Loss: 0.4746 | Val Loss: 0.4605 | Time: 1.23s\r\n",
      "Epoch 011 | Train Loss: 0.4749 | Val Loss: 0.4524 | Time: 1.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4524)\r\n",
      "Epoch 012 | Train Loss: 0.4729 | Val Loss: 0.4593 | Time: 1.20s\r\n",
      "Epoch 013 | Train Loss: 0.4728 | Val Loss: 0.4536 | Time: 1.26s\r\n",
      "Epoch 014 | Train Loss: 0.4709 | Val Loss: 0.4509 | Time: 1.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4509)\r\n",
      "Epoch 015 | Train Loss: 0.4687 | Val Loss: 0.4542 | Time: 1.24s\r\n",
      "Epoch 016 | Train Loss: 0.4673 | Val Loss: 0.4519 | Time: 1.28s\r\n",
      "Epoch 017 | Train Loss: 0.4688 | Val Loss: 0.4551 | Time: 1.24s\r\n",
      "Epoch 018 | Train Loss: 0.4709 | Val Loss: 0.4489 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4489)\r\n",
      "Epoch 019 | Train Loss: 0.4662 | Val Loss: 0.4560 | Time: 1.22s\r\n",
      "Epoch 020 | Train Loss: 0.4658 | Val Loss: 0.4589 | Time: 1.22s\r\n",
      "Epoch 021 | Train Loss: 0.4683 | Val Loss: 0.4587 | Time: 1.22s\r\n",
      "Epoch 022 | Train Loss: 0.4665 | Val Loss: 0.4603 | Time: 1.23s\r\n",
      "Epoch 023 | Train Loss: 0.4651 | Val Loss: 0.4555 | Time: 1.21s\r\n",
      "Epoch 024 | Train Loss: 0.4666 | Val Loss: 0.4589 | Time: 1.24s\r\n",
      "Epoch 025 | Train Loss: 0.4675 | Val Loss: 0.4493 | Time: 1.21s\r\n",
      "Epoch 026 | Train Loss: 0.4628 | Val Loss: 0.4526 | Time: 1.22s\r\n",
      "Epoch 027 | Train Loss: 0.4637 | Val Loss: 0.4472 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4472)\r\n",
      "Epoch 028 | Train Loss: 0.4661 | Val Loss: 0.4465 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4465)\r\n",
      "Epoch 029 | Train Loss: 0.4642 | Val Loss: 0.4547 | Time: 1.23s\r\n",
      "Epoch 030 | Train Loss: 0.4592 | Val Loss: 0.4521 | Time: 1.24s\r\n",
      "Epoch 031 | Train Loss: 0.4619 | Val Loss: 0.4479 | Time: 1.23s\r\n",
      "Epoch 032 | Train Loss: 0.4639 | Val Loss: 0.4530 | Time: 1.25s\r\n",
      "Epoch 033 | Train Loss: 0.4643 | Val Loss: 0.4449 | Time: 1.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4449)\r\n",
      "Epoch 034 | Train Loss: 0.4647 | Val Loss: 0.4485 | Time: 1.22s\r\n",
      "Epoch 035 | Train Loss: 0.4604 | Val Loss: 0.4487 | Time: 1.22s\r\n",
      "Epoch 036 | Train Loss: 0.4631 | Val Loss: 0.4440 | Time: 1.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4440)\r\n",
      "Epoch 037 | Train Loss: 0.4605 | Val Loss: 0.4470 | Time: 1.23s\r\n",
      "Epoch 038 | Train Loss: 0.4594 | Val Loss: 0.4449 | Time: 1.24s\r\n",
      "Epoch 039 | Train Loss: 0.4644 | Val Loss: 0.4421 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4421)\r\n",
      "Epoch 040 | Train Loss: 0.4628 | Val Loss: 0.4418 | Time: 1.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4418)\r\n",
      "Epoch 041 | Train Loss: 0.4628 | Val Loss: 0.4437 | Time: 1.24s\r\n",
      "Epoch 042 | Train Loss: 0.4618 | Val Loss: 0.4467 | Time: 1.24s\r\n",
      "Epoch 043 | Train Loss: 0.4624 | Val Loss: 0.4465 | Time: 1.23s\r\n",
      "Epoch 044 | Train Loss: 0.4617 | Val Loss: 0.4502 | Time: 1.23s\r\n",
      "Epoch 045 | Train Loss: 0.4616 | Val Loss: 0.4498 | Time: 1.23s\r\n",
      "Epoch 046 | Train Loss: 0.4617 | Val Loss: 0.4457 | Time: 1.23s\r\n",
      "Epoch 047 | Train Loss: 0.4585 | Val Loss: 0.4471 | Time: 1.23s\r\n",
      "Epoch 048 | Train Loss: 0.4622 | Val Loss: 0.4449 | Time: 1.27s\r\n",
      "Epoch 049 | Train Loss: 0.4606 | Val Loss: 0.4414 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4414)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4414)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingMovies      (CAT): 0.2499\r\n",
      "   2. PaymentMethod        (CAT): 0.2000\r\n",
      "   3. tenure               (NUM): 0.1981\r\n",
      "   4. gender               (CAT): 0.1852\r\n",
      "   5. Dependents           (CAT): 0.1668\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingMovies     : 0.2499\r\n",
      "   2. PaymentMethod       : 0.2000\r\n",
      "   3. tenure              : 0.1981\r\n",
      "   4. gender              : 0.1852\r\n",
      "   5. Dependents          : 0.1668\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 231.4s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 105,121 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5001 | Val Loss: 0.4524 | Time: 4.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4524)\r\n",
      "Epoch 001 | Train Loss: 0.4372 | Val Loss: 0.4396 | Time: 4.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4396)\r\n",
      "Epoch 002 | Train Loss: 0.4280 | Val Loss: 0.4275 | Time: 4.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4275)\r\n",
      "Epoch 003 | Train Loss: 0.4194 | Val Loss: 0.4264 | Time: 4.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4264)\r\n",
      "Epoch 004 | Train Loss: 0.4198 | Val Loss: 0.4277 | Time: 4.48s\r\n",
      "Epoch 005 | Train Loss: 0.4128 | Val Loss: 0.4272 | Time: 4.42s\r\n",
      "Epoch 006 | Train Loss: 0.4099 | Val Loss: 0.4191 | Time: 4.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 007 | Train Loss: 0.4078 | Val Loss: 0.4173 | Time: 4.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4173)\r\n",
      "Epoch 008 | Train Loss: 0.4066 | Val Loss: 0.4154 | Time: 4.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4154)\r\n",
      "Epoch 009 | Train Loss: 0.4080 | Val Loss: 0.4159 | Time: 4.40s\r\n",
      "Epoch 010 | Train Loss: 0.4070 | Val Loss: 0.4207 | Time: 4.43s\r\n",
      "Epoch 011 | Train Loss: 0.4047 | Val Loss: 0.4148 | Time: 4.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4148)\r\n",
      "Epoch 012 | Train Loss: 0.4025 | Val Loss: 0.4152 | Time: 4.40s\r\n",
      "Epoch 013 | Train Loss: 0.3986 | Val Loss: 0.4158 | Time: 4.34s\r\n",
      "Epoch 014 | Train Loss: 0.4018 | Val Loss: 0.4182 | Time: 4.36s\r\n",
      "Epoch 015 | Train Loss: 0.3982 | Val Loss: 0.4181 | Time: 4.35s\r\n",
      "Epoch 016 | Train Loss: 0.4000 | Val Loss: 0.4214 | Time: 4.38s\r\n",
      "Epoch 017 | Train Loss: 0.3970 | Val Loss: 0.4216 | Time: 4.44s\r\n",
      "Epoch 018 | Train Loss: 0.3976 | Val Loss: 0.4179 | Time: 4.49s\r\n",
      "Epoch 019 | Train Loss: 0.3961 | Val Loss: 0.4268 | Time: 4.41s\r\n",
      "Epoch 020 | Train Loss: 0.3975 | Val Loss: 0.4223 | Time: 4.38s\r\n",
      "Epoch 021 | Train Loss: 0.3934 | Val Loss: 0.4227 | Time: 4.39s\r\n",
      "Epoch 022 | Train Loss: 0.3952 | Val Loss: 0.4319 | Time: 4.39s\r\n",
      "Epoch 023 | Train Loss: 0.3919 | Val Loss: 0.4273 | Time: 4.39s\r\n",
      "Epoch 024 | Train Loss: 0.3953 | Val Loss: 0.4223 | Time: 4.44s\r\n",
      "Epoch 025 | Train Loss: 0.3917 | Val Loss: 0.4235 | Time: 4.59s\r\n",
      "Epoch 026 | Train Loss: 0.3948 | Val Loss: 0.4253 | Time: 4.42s\r\n",
      "Epoch 027 | Train Loss: 0.3934 | Val Loss: 0.4268 | Time: 4.40s\r\n",
      "Epoch 028 | Train Loss: 0.3930 | Val Loss: 0.4293 | Time: 4.44s\r\n",
      "Epoch 029 | Train Loss: 0.3889 | Val Loss: 0.4228 | Time: 4.41s\r\n",
      "Epoch 030 | Train Loss: 0.3882 | Val Loss: 0.4274 | Time: 4.44s\r\n",
      "Epoch 031 | Train Loss: 0.3861 | Val Loss: 0.4276 | Time: 4.37s\r\n",
      "Epoch 032 | Train Loss: 0.3870 | Val Loss: 0.4245 | Time: 4.61s\r\n",
      "Epoch 033 | Train Loss: 0.3921 | Val Loss: 0.4278 | Time: 4.44s\r\n",
      "Epoch 034 | Train Loss: 0.3896 | Val Loss: 0.4276 | Time: 4.38s\r\n",
      "Epoch 035 | Train Loss: 0.3905 | Val Loss: 0.4295 | Time: 4.40s\r\n",
      "Epoch 036 | Train Loss: 0.3860 | Val Loss: 0.4273 | Time: 4.39s\r\n",
      "Epoch 037 | Train Loss: 0.3830 | Val Loss: 0.4352 | Time: 4.43s\r\n",
      "\r\n",
      "Early stopping à l'époque 37 (patience: 26)\r\n",
      "✅ Meilleur modèle chargé (époque 11, val_loss: 0.4148)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.0543\r\n",
      "   2. Contract            : 0.0539\r\n",
      "   3. gender              : 0.0538\r\n",
      "   4. TechSupport         : 0.0538\r\n",
      "   5. Dependents          : 0.0537\r\n",
      "   6. tenure              : 0.0533\r\n",
      "   7. Partner             : 0.0530\r\n",
      "   8. StreamingMovies     : 0.0526\r\n",
      "   9. OnlineSecurity      : 0.0526\r\n",
      "  10. TotalCharges        : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 5 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. InternetService      (CAT): 0.0543\r\n",
      "   2. Contract             (CAT): 0.0539\r\n",
      "   3. gender               (CAT): 0.0538\r\n",
      "   4. TechSupport          (CAT): 0.0538\r\n",
      "   5. Dependents           (CAT): 0.0537\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['InternetService', 'Contract', 'gender', 'TechSupport', 'Dependents'] → indices [6, 13, 0, 10, 3]\r\n",
      "📊 Features sélectionnées: 0 numériques, 5 catégorielles\r\n",
      "🎲 Interactions aléatoires: 2 paires\r\n",
      "Modèle Random créé avec 13,457 paramètres\r\n",
      "🔗 Sparsité d'attention: 61.11%\r\n",
      "   - Connexions feature-feature: 4\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5193 | Val Loss: 0.4764 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4764)\r\n",
      "Epoch 001 | Train Loss: 0.4737 | Val Loss: 0.4674 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4674)\r\n",
      "Epoch 002 | Train Loss: 0.4668 | Val Loss: 0.4633 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4633)\r\n",
      "Epoch 003 | Train Loss: 0.4649 | Val Loss: 0.4632 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4632)\r\n",
      "Epoch 004 | Train Loss: 0.4563 | Val Loss: 0.4589 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4589)\r\n",
      "Epoch 005 | Train Loss: 0.4571 | Val Loss: 0.4585 | Time: 1.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4585)\r\n",
      "Epoch 006 | Train Loss: 0.4546 | Val Loss: 0.4557 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4557)\r\n",
      "Epoch 007 | Train Loss: 0.4553 | Val Loss: 0.4538 | Time: 1.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4538)\r\n",
      "Epoch 008 | Train Loss: 0.4518 | Val Loss: 0.4596 | Time: 1.20s\r\n",
      "Epoch 009 | Train Loss: 0.4526 | Val Loss: 0.4540 | Time: 1.21s\r\n",
      "Epoch 010 | Train Loss: 0.4484 | Val Loss: 0.4532 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4532)\r\n",
      "Epoch 011 | Train Loss: 0.4523 | Val Loss: 0.4544 | Time: 1.23s\r\n",
      "Epoch 012 | Train Loss: 0.4511 | Val Loss: 0.4545 | Time: 1.22s\r\n",
      "Epoch 013 | Train Loss: 0.4491 | Val Loss: 0.4536 | Time: 1.25s\r\n",
      "Epoch 014 | Train Loss: 0.4484 | Val Loss: 0.4531 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4531)\r\n",
      "Epoch 015 | Train Loss: 0.4456 | Val Loss: 0.4550 | Time: 1.23s\r\n",
      "Epoch 016 | Train Loss: 0.4492 | Val Loss: 0.4560 | Time: 1.23s\r\n",
      "Epoch 017 | Train Loss: 0.4469 | Val Loss: 0.4544 | Time: 1.23s\r\n",
      "Epoch 018 | Train Loss: 0.4475 | Val Loss: 0.4520 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4520)\r\n",
      "Epoch 019 | Train Loss: 0.4456 | Val Loss: 0.4560 | Time: 1.24s\r\n",
      "Epoch 020 | Train Loss: 0.4463 | Val Loss: 0.4538 | Time: 1.23s\r\n",
      "Epoch 021 | Train Loss: 0.4451 | Val Loss: 0.4527 | Time: 1.33s\r\n",
      "Epoch 022 | Train Loss: 0.4455 | Val Loss: 0.4542 | Time: 1.23s\r\n",
      "Epoch 023 | Train Loss: 0.4469 | Val Loss: 0.4546 | Time: 1.25s\r\n",
      "Epoch 024 | Train Loss: 0.4483 | Val Loss: 0.4534 | Time: 1.25s\r\n",
      "Epoch 025 | Train Loss: 0.4451 | Val Loss: 0.4547 | Time: 1.23s\r\n",
      "Epoch 026 | Train Loss: 0.4461 | Val Loss: 0.4555 | Time: 1.23s\r\n",
      "Epoch 027 | Train Loss: 0.4453 | Val Loss: 0.4542 | Time: 1.21s\r\n",
      "Epoch 028 | Train Loss: 0.4455 | Val Loss: 0.4511 | Time: 1.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4511)\r\n",
      "Epoch 029 | Train Loss: 0.4465 | Val Loss: 0.4534 | Time: 1.21s\r\n",
      "Epoch 030 | Train Loss: 0.4454 | Val Loss: 0.4540 | Time: 1.26s\r\n",
      "Epoch 031 | Train Loss: 0.4468 | Val Loss: 0.4531 | Time: 1.34s\r\n",
      "Epoch 032 | Train Loss: 0.4456 | Val Loss: 0.4534 | Time: 1.30s\r\n",
      "Epoch 033 | Train Loss: 0.4432 | Val Loss: 0.4535 | Time: 1.20s\r\n",
      "Epoch 034 | Train Loss: 0.4442 | Val Loss: 0.4525 | Time: 1.20s\r\n",
      "Epoch 035 | Train Loss: 0.4443 | Val Loss: 0.4536 | Time: 1.21s\r\n",
      "Epoch 036 | Train Loss: 0.4461 | Val Loss: 0.4534 | Time: 1.21s\r\n",
      "Epoch 037 | Train Loss: 0.4446 | Val Loss: 0.4527 | Time: 1.19s\r\n",
      "Epoch 038 | Train Loss: 0.4459 | Val Loss: 0.4533 | Time: 1.25s\r\n",
      "Epoch 039 | Train Loss: 0.4428 | Val Loss: 0.4528 | Time: 1.21s\r\n",
      "Epoch 040 | Train Loss: 0.4437 | Val Loss: 0.4524 | Time: 1.20s\r\n",
      "Epoch 041 | Train Loss: 0.4452 | Val Loss: 0.4523 | Time: 1.20s\r\n",
      "Epoch 042 | Train Loss: 0.4439 | Val Loss: 0.4529 | Time: 1.20s\r\n",
      "Epoch 043 | Train Loss: 0.4436 | Val Loss: 0.4528 | Time: 1.22s\r\n",
      "Epoch 044 | Train Loss: 0.4445 | Val Loss: 0.4528 | Time: 1.22s\r\n",
      "Epoch 045 | Train Loss: 0.4448 | Val Loss: 0.4524 | Time: 1.22s\r\n",
      "Epoch 046 | Train Loss: 0.4461 | Val Loss: 0.4537 | Time: 1.26s\r\n",
      "Epoch 047 | Train Loss: 0.4465 | Val Loss: 0.4533 | Time: 1.25s\r\n",
      "Epoch 048 | Train Loss: 0.4467 | Val Loss: 0.4535 | Time: 1.22s\r\n",
      "Epoch 049 | Train Loss: 0.4450 | Val Loss: 0.4527 | Time: 1.25s\r\n",
      "✅ Meilleur modèle Random chargé (époque 28, val_loss: 0.4511)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. InternetService      (CAT): 0.2261\r\n",
      "   2. TechSupport          (CAT): 0.2130\r\n",
      "   3. Contract             (CAT): 0.2069\r\n",
      "   4. gender               (CAT): 0.2034\r\n",
      "   5. Dependents           (CAT): 0.1507\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.2261\r\n",
      "   2. TechSupport         : 0.2130\r\n",
      "   3. Contract            : 0.2069\r\n",
      "   4. gender              : 0.2034\r\n",
      "   5. Dependents          : 0.1507\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 232.5s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 105,121 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5022 | Val Loss: 0.4274 | Time: 4.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4274)\r\n",
      "Epoch 001 | Train Loss: 0.4392 | Val Loss: 0.4176 | Time: 4.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4176)\r\n",
      "Epoch 002 | Train Loss: 0.4305 | Val Loss: 0.4156 | Time: 4.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4156)\r\n",
      "Epoch 003 | Train Loss: 0.4244 | Val Loss: 0.4098 | Time: 4.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4098)\r\n",
      "Epoch 004 | Train Loss: 0.4195 | Val Loss: 0.4067 | Time: 4.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4067)\r\n",
      "Epoch 005 | Train Loss: 0.4136 | Val Loss: 0.4111 | Time: 4.41s\r\n",
      "Epoch 006 | Train Loss: 0.4148 | Val Loss: 0.4083 | Time: 4.36s\r\n",
      "Epoch 007 | Train Loss: 0.4137 | Val Loss: 0.4095 | Time: 4.38s\r\n",
      "Epoch 008 | Train Loss: 0.4140 | Val Loss: 0.4119 | Time: 4.45s\r\n",
      "Epoch 009 | Train Loss: 0.4144 | Val Loss: 0.4105 | Time: 4.41s\r\n",
      "Epoch 010 | Train Loss: 0.4137 | Val Loss: 0.4177 | Time: 4.37s\r\n",
      "Epoch 011 | Train Loss: 0.4095 | Val Loss: 0.4141 | Time: 4.41s\r\n",
      "Epoch 012 | Train Loss: 0.4104 | Val Loss: 0.4111 | Time: 4.40s\r\n",
      "Epoch 013 | Train Loss: 0.4054 | Val Loss: 0.4138 | Time: 4.39s\r\n",
      "Epoch 014 | Train Loss: 0.4017 | Val Loss: 0.4202 | Time: 4.40s\r\n",
      "Epoch 015 | Train Loss: 0.4051 | Val Loss: 0.4138 | Time: 4.36s\r\n",
      "Epoch 016 | Train Loss: 0.4055 | Val Loss: 0.4170 | Time: 4.53s\r\n",
      "Epoch 017 | Train Loss: 0.4034 | Val Loss: 0.4166 | Time: 4.41s\r\n",
      "Epoch 018 | Train Loss: 0.4004 | Val Loss: 0.4123 | Time: 4.44s\r\n",
      "Epoch 019 | Train Loss: 0.4000 | Val Loss: 0.4188 | Time: 4.45s\r\n",
      "Epoch 020 | Train Loss: 0.4011 | Val Loss: 0.4189 | Time: 4.39s\r\n",
      "Epoch 021 | Train Loss: 0.3975 | Val Loss: 0.4177 | Time: 4.39s\r\n",
      "Epoch 022 | Train Loss: 0.3966 | Val Loss: 0.4213 | Time: 4.31s\r\n",
      "Epoch 023 | Train Loss: 0.3994 | Val Loss: 0.4147 | Time: 4.49s\r\n",
      "Epoch 024 | Train Loss: 0.3979 | Val Loss: 0.4194 | Time: 4.38s\r\n",
      "Epoch 025 | Train Loss: 0.3982 | Val Loss: 0.4165 | Time: 4.38s\r\n",
      "Epoch 026 | Train Loss: 0.3975 | Val Loss: 0.4246 | Time: 4.34s\r\n",
      "Epoch 027 | Train Loss: 0.3978 | Val Loss: 0.4199 | Time: 4.36s\r\n",
      "Epoch 028 | Train Loss: 0.3948 | Val Loss: 0.4292 | Time: 4.47s\r\n",
      "Epoch 029 | Train Loss: 0.3966 | Val Loss: 0.4273 | Time: 4.35s\r\n",
      "Epoch 030 | Train Loss: 0.3944 | Val Loss: 0.4174 | Time: 4.57s\r\n",
      "\r\n",
      "Early stopping à l'époque 30 (patience: 26)\r\n",
      "✅ Meilleur modèle chargé (époque 4, val_loss: 0.4067)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.0586\r\n",
      "   2. PaperlessBilling    : 0.0578\r\n",
      "   3. TotalCharges        : 0.0569\r\n",
      "   4. PhoneService        : 0.0554\r\n",
      "   5. gender              : 0.0546\r\n",
      "   6. OnlineBackup        : 0.0534\r\n",
      "   7. tenure              : 0.0533\r\n",
      "   8. MultipleLines       : 0.0528\r\n",
      "   9. OnlineSecurity      : 0.0528\r\n",
      "  10. TechSupport         : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 5 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. InternetService      (CAT): 0.0586\r\n",
      "   2. PaperlessBilling     (CAT): 0.0578\r\n",
      "   3. TotalCharges         (NUM): 0.0569\r\n",
      "   4. PhoneService         (CAT): 0.0554\r\n",
      "   5. gender               (CAT): 0.0546\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['InternetService', 'PaperlessBilling', 'PhoneService', 'gender'] → indices [6, 14, 4, 0]\r\n",
      "📊 Features sélectionnées: 1 numériques, 4 catégorielles\r\n",
      "🎲 Interactions aléatoires: 2 paires\r\n",
      "Modèle Random créé avec 13,409 paramètres\r\n",
      "🔗 Sparsité d'attention: 61.11%\r\n",
      "   - Connexions feature-feature: 4\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5917 | Val Loss: 0.5206 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5206)\r\n",
      "Epoch 001 | Train Loss: 0.5164 | Val Loss: 0.4566 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4566)\r\n",
      "Epoch 002 | Train Loss: 0.4864 | Val Loss: 0.4525 | Time: 1.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4525)\r\n",
      "Epoch 003 | Train Loss: 0.4779 | Val Loss: 0.4462 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4462)\r\n",
      "Epoch 004 | Train Loss: 0.4653 | Val Loss: 0.4425 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 005 | Train Loss: 0.4675 | Val Loss: 0.4466 | Time: 1.25s\r\n",
      "Epoch 006 | Train Loss: 0.4696 | Val Loss: 0.4343 | Time: 1.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4343)\r\n",
      "Epoch 007 | Train Loss: 0.4640 | Val Loss: 0.4329 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4329)\r\n",
      "Epoch 008 | Train Loss: 0.4625 | Val Loss: 0.4356 | Time: 1.23s\r\n",
      "Epoch 009 | Train Loss: 0.4579 | Val Loss: 0.4367 | Time: 1.24s\r\n",
      "Epoch 010 | Train Loss: 0.4584 | Val Loss: 0.4346 | Time: 1.25s\r\n",
      "Epoch 011 | Train Loss: 0.4581 | Val Loss: 0.4339 | Time: 1.22s\r\n",
      "Epoch 012 | Train Loss: 0.4563 | Val Loss: 0.4323 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4323)\r\n",
      "Epoch 013 | Train Loss: 0.4514 | Val Loss: 0.4291 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4291)\r\n",
      "Epoch 014 | Train Loss: 0.4560 | Val Loss: 0.4291 | Time: 1.25s\r\n",
      "Epoch 015 | Train Loss: 0.4525 | Val Loss: 0.4330 | Time: 1.24s\r\n",
      "Epoch 016 | Train Loss: 0.4508 | Val Loss: 0.4335 | Time: 1.23s\r\n",
      "Epoch 017 | Train Loss: 0.4532 | Val Loss: 0.4272 | Time: 1.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4272)\r\n",
      "Epoch 018 | Train Loss: 0.4546 | Val Loss: 0.4309 | Time: 1.22s\r\n",
      "Epoch 019 | Train Loss: 0.4525 | Val Loss: 0.4301 | Time: 1.23s\r\n",
      "Epoch 020 | Train Loss: 0.4495 | Val Loss: 0.4305 | Time: 1.27s\r\n",
      "Epoch 021 | Train Loss: 0.4524 | Val Loss: 0.4347 | Time: 1.23s\r\n",
      "Epoch 022 | Train Loss: 0.4528 | Val Loss: 0.4291 | Time: 1.33s\r\n",
      "Epoch 023 | Train Loss: 0.4509 | Val Loss: 0.4314 | Time: 1.23s\r\n",
      "Epoch 024 | Train Loss: 0.4477 | Val Loss: 0.4284 | Time: 1.22s\r\n",
      "Epoch 025 | Train Loss: 0.4514 | Val Loss: 0.4275 | Time: 1.23s\r\n",
      "Epoch 026 | Train Loss: 0.4482 | Val Loss: 0.4276 | Time: 1.25s\r\n",
      "Epoch 027 | Train Loss: 0.4513 | Val Loss: 0.4258 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 028 | Train Loss: 0.4464 | Val Loss: 0.4274 | Time: 1.27s\r\n",
      "Epoch 029 | Train Loss: 0.4461 | Val Loss: 0.4279 | Time: 1.22s\r\n",
      "Epoch 030 | Train Loss: 0.4506 | Val Loss: 0.4316 | Time: 1.23s\r\n",
      "Epoch 031 | Train Loss: 0.4464 | Val Loss: 0.4302 | Time: 1.21s\r\n",
      "Epoch 032 | Train Loss: 0.4474 | Val Loss: 0.4281 | Time: 1.24s\r\n",
      "Epoch 033 | Train Loss: 0.4485 | Val Loss: 0.4276 | Time: 1.23s\r\n",
      "Epoch 034 | Train Loss: 0.4480 | Val Loss: 0.4293 | Time: 1.23s\r\n",
      "Epoch 035 | Train Loss: 0.4482 | Val Loss: 0.4282 | Time: 1.22s\r\n",
      "Epoch 036 | Train Loss: 0.4495 | Val Loss: 0.4245 | Time: 1.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4245)\r\n",
      "Epoch 037 | Train Loss: 0.4495 | Val Loss: 0.4272 | Time: 1.23s\r\n",
      "Epoch 038 | Train Loss: 0.4474 | Val Loss: 0.4259 | Time: 1.25s\r\n",
      "Epoch 039 | Train Loss: 0.4489 | Val Loss: 0.4257 | Time: 1.23s\r\n",
      "Epoch 040 | Train Loss: 0.4470 | Val Loss: 0.4297 | Time: 1.22s\r\n",
      "Epoch 041 | Train Loss: 0.4489 | Val Loss: 0.4275 | Time: 1.24s\r\n",
      "Epoch 042 | Train Loss: 0.4451 | Val Loss: 0.4266 | Time: 1.24s\r\n",
      "Epoch 043 | Train Loss: 0.4440 | Val Loss: 0.4272 | Time: 1.23s\r\n",
      "Epoch 044 | Train Loss: 0.4472 | Val Loss: 0.4244 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4244)\r\n",
      "Epoch 045 | Train Loss: 0.4439 | Val Loss: 0.4271 | Time: 1.23s\r\n",
      "Epoch 046 | Train Loss: 0.4458 | Val Loss: 0.4236 | Time: 1.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4236)\r\n",
      "Epoch 047 | Train Loss: 0.4462 | Val Loss: 0.4256 | Time: 1.32s\r\n",
      "Epoch 048 | Train Loss: 0.4456 | Val Loss: 0.4269 | Time: 1.37s\r\n",
      "Epoch 049 | Train Loss: 0.4468 | Val Loss: 0.4252 | Time: 1.22s\r\n",
      "✅ Meilleur modèle Random chargé (époque 46, val_loss: 0.4236)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. TotalCharges         (NUM): 0.2432\r\n",
      "   2. PhoneService         (CAT): 0.2024\r\n",
      "   3. PaperlessBilling     (CAT): 0.1913\r\n",
      "   4. gender               (CAT): 0.1869\r\n",
      "   5. InternetService      (CAT): 0.1763\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TotalCharges        : 0.2432\r\n",
      "   2. PhoneService        : 0.2024\r\n",
      "   3. PaperlessBilling    : 0.1913\r\n",
      "   4. gender              : 0.1869\r\n",
      "   5. InternetService     : 0.1763\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_6/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 201.8s ===\r\n",
      "\u001b[32m[I 2025-07-19 20:24:43,879]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 6, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 128, 'attention_dropout_stage1': 0.11033634423372154, 'ffn_dropout_stage1': 0.2062709263136296, 'residual_dropout_stage1': 0.15406351216101066, 'lr_stage1': 0.0035458438462279804, 'weight_decay_stage1': 0.004270283109049075, 'd_token_stage2': 16, 'n_blocks_stage2': 3, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.13465886401416916, 'ffn_dropout_stage2': 0.13128740853421722, 'residual_dropout_stage2': 0.12502428981645955, 'lr_stage2': 0.0015736446109772138, 'weight_decay_stage2': 0.003740930273158259, 'batch_size': 128, 'patience': 26, 'embedding_type': 'T-LR', 'M': 5, 'k': 2}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  28%|██▌      | 7/25 [2:11:46<4:33:41, 912.33s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 31,617 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5475 | Val Loss: 0.5216 | Time: 2.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5216)\r\n",
      "Epoch 001 | Train Loss: 0.5199 | Val Loss: 0.4916 | Time: 3.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4916)\r\n",
      "Epoch 002 | Train Loss: 0.5219 | Val Loss: 0.5478 | Time: 2.95s\r\n",
      "Epoch 003 | Train Loss: 0.5222 | Val Loss: 0.5111 | Time: 3.00s\r\n",
      "Epoch 004 | Train Loss: 0.5192 | Val Loss: 0.5055 | Time: 2.94s\r\n",
      "Epoch 005 | Train Loss: 0.5232 | Val Loss: 0.6799 | Time: 2.93s\r\n",
      "Epoch 006 | Train Loss: 0.5643 | Val Loss: 0.5434 | Time: 2.94s\r\n",
      "Epoch 007 | Train Loss: 0.5645 | Val Loss: 0.5571 | Time: 2.93s\r\n",
      "Epoch 008 | Train Loss: 0.5581 | Val Loss: 0.5277 | Time: 2.96s\r\n",
      "Epoch 009 | Train Loss: 0.5502 | Val Loss: 0.5567 | Time: 3.02s\r\n",
      "Epoch 010 | Train Loss: 0.5440 | Val Loss: 0.5341 | Time: 2.96s\r\n",
      "Epoch 011 | Train Loss: 0.5512 | Val Loss: 0.5279 | Time: 2.97s\r\n",
      "Epoch 012 | Train Loss: 0.5614 | Val Loss: 0.6030 | Time: 2.93s\r\n",
      "Epoch 013 | Train Loss: 0.5772 | Val Loss: 0.5893 | Time: 2.96s\r\n",
      "Epoch 014 | Train Loss: 0.5742 | Val Loss: 0.6053 | Time: 2.97s\r\n",
      "Epoch 015 | Train Loss: 0.5745 | Val Loss: 0.5870 | Time: 2.97s\r\n",
      "Epoch 016 | Train Loss: 0.5761 | Val Loss: 0.5764 | Time: 3.00s\r\n",
      "Epoch 017 | Train Loss: 0.5733 | Val Loss: 0.5707 | Time: 2.96s\r\n",
      "Epoch 018 | Train Loss: 0.5598 | Val Loss: 0.5469 | Time: 2.90s\r\n",
      "Epoch 019 | Train Loss: 0.5630 | Val Loss: 0.5432 | Time: 3.01s\r\n",
      "Epoch 020 | Train Loss: 0.5603 | Val Loss: 0.5221 | Time: 3.01s\r\n",
      "Epoch 021 | Train Loss: 0.5617 | Val Loss: 0.5429 | Time: 2.92s\r\n",
      "Epoch 022 | Train Loss: 0.5545 | Val Loss: 0.5271 | Time: 2.95s\r\n",
      "Epoch 023 | Train Loss: 0.5453 | Val Loss: 0.5336 | Time: 2.92s\r\n",
      "Epoch 024 | Train Loss: 0.5476 | Val Loss: 0.5426 | Time: 2.98s\r\n",
      "Epoch 025 | Train Loss: 0.5540 | Val Loss: 0.5497 | Time: 2.94s\r\n",
      "Epoch 026 | Train Loss: 0.5512 | Val Loss: 0.5437 | Time: 2.92s\r\n",
      "\r\n",
      "Early stopping à l'époque 26 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 1, val_loss: 0.4916)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TechSupport         : 0.1460\r\n",
      "   2. tenure              : 0.1439\r\n",
      "   3. SeniorCitizen       : 0.1299\r\n",
      "   4. OnlineBackup        : 0.1223\r\n",
      "   5. DeviceProtection    : 0.0827\r\n",
      "   6. MonthlyCharges      : 0.0729\r\n",
      "   7. MultipleLines       : 0.0521\r\n",
      "   8. Partner             : 0.0394\r\n",
      "   9. Dependents          : 0.0380\r\n",
      "  10. StreamingMovies     : 0.0362\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 15 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. TechSupport          (CAT): 0.1460\r\n",
      "   2. tenure               (NUM): 0.1439\r\n",
      "   3. SeniorCitizen        (CAT): 0.1299\r\n",
      "   4. OnlineBackup         (CAT): 0.1223\r\n",
      "   5. DeviceProtection     (CAT): 0.0827\r\n",
      "   6. MonthlyCharges       (NUM): 0.0729\r\n",
      "   7. MultipleLines        (CAT): 0.0521\r\n",
      "   8. Partner              (CAT): 0.0394\r\n",
      "   9. Dependents           (CAT): 0.0380\r\n",
      "  10. StreamingMovies      (CAT): 0.0362\r\n",
      "  11. PaymentMethod        (CAT): 0.0280\r\n",
      "  12. gender               (CAT): 0.0259\r\n",
      "  13. PaperlessBilling     (CAT): 0.0223\r\n",
      "  14. TotalCharges         (NUM): 0.0162\r\n",
      "  15. OnlineSecurity       (CAT): 0.0161\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'MonthlyCharges', 'TotalCharges'] → indices [0, 1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['TechSupport', 'SeniorCitizen', 'OnlineBackup', 'DeviceProtection', 'MultipleLines', 'Partner', 'Dependents', 'StreamingMovies', 'PaymentMethod', 'gender', 'PaperlessBilling', 'OnlineSecurity'] → indices [10, 1, 8, 9, 5, 2, 3, 12, 15, 0, 14, 7]\r\n",
      "📊 Features sélectionnées: 3 numériques, 12 catégorielles\r\n",
      "🎲 Interactions aléatoires: 9 paires\r\n",
      "Modèle Random créé avec 269,953 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.25%\r\n",
      "   - Connexions feature-feature: 18\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5473 | Val Loss: 0.4744 | Time: 6.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4744)\r\n",
      "Epoch 001 | Train Loss: 0.4923 | Val Loss: 0.4681 | Time: 6.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4681)\r\n",
      "Epoch 002 | Train Loss: 0.4774 | Val Loss: 0.4521 | Time: 6.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4521)\r\n",
      "Epoch 003 | Train Loss: 0.4671 | Val Loss: 0.4422 | Time: 6.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4422)\r\n",
      "Epoch 004 | Train Loss: 0.4615 | Val Loss: 0.4368 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 005 | Train Loss: 0.4516 | Val Loss: 0.4334 | Time: 6.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4334)\r\n",
      "Epoch 006 | Train Loss: 0.4511 | Val Loss: 0.4295 | Time: 6.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4295)\r\n",
      "Epoch 007 | Train Loss: 0.4487 | Val Loss: 0.4270 | Time: 6.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4270)\r\n",
      "Epoch 008 | Train Loss: 0.4460 | Val Loss: 0.4305 | Time: 6.24s\r\n",
      "Epoch 009 | Train Loss: 0.4437 | Val Loss: 0.4307 | Time: 6.34s\r\n",
      "Epoch 010 | Train Loss: 0.4459 | Val Loss: 0.4281 | Time: 6.26s\r\n",
      "Epoch 011 | Train Loss: 0.4410 | Val Loss: 0.4283 | Time: 6.41s\r\n",
      "Epoch 012 | Train Loss: 0.4391 | Val Loss: 0.4265 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 013 | Train Loss: 0.4435 | Val Loss: 0.4240 | Time: 6.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4240)\r\n",
      "Epoch 014 | Train Loss: 0.4370 | Val Loss: 0.4222 | Time: 6.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 015 | Train Loss: 0.4377 | Val Loss: 0.4231 | Time: 6.36s\r\n",
      "Epoch 016 | Train Loss: 0.4361 | Val Loss: 0.4200 | Time: 6.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 017 | Train Loss: 0.4332 | Val Loss: 0.4196 | Time: 6.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4196)\r\n",
      "Epoch 018 | Train Loss: 0.4333 | Val Loss: 0.4195 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4195)\r\n",
      "Epoch 019 | Train Loss: 0.4356 | Val Loss: 0.4183 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 020 | Train Loss: 0.4336 | Val Loss: 0.4153 | Time: 6.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4153)\r\n",
      "Epoch 021 | Train Loss: 0.4337 | Val Loss: 0.4144 | Time: 6.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4144)\r\n",
      "Epoch 022 | Train Loss: 0.4346 | Val Loss: 0.4146 | Time: 6.21s\r\n",
      "Epoch 023 | Train Loss: 0.4365 | Val Loss: 0.4145 | Time: 6.24s\r\n",
      "Epoch 024 | Train Loss: 0.4369 | Val Loss: 0.4139 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4139)\r\n",
      "Epoch 025 | Train Loss: 0.4325 | Val Loss: 0.4144 | Time: 6.25s\r\n",
      "Epoch 026 | Train Loss: 0.4338 | Val Loss: 0.4156 | Time: 6.57s\r\n",
      "Epoch 027 | Train Loss: 0.4311 | Val Loss: 0.4140 | Time: 6.12s\r\n",
      "Epoch 028 | Train Loss: 0.4304 | Val Loss: 0.4138 | Time: 6.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4138)\r\n",
      "Epoch 029 | Train Loss: 0.4312 | Val Loss: 0.4110 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4110)\r\n",
      "Epoch 030 | Train Loss: 0.4343 | Val Loss: 0.4124 | Time: 6.24s\r\n",
      "Epoch 031 | Train Loss: 0.4291 | Val Loss: 0.4117 | Time: 6.40s\r\n",
      "Epoch 032 | Train Loss: 0.4313 | Val Loss: 0.4109 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4109)\r\n",
      "Epoch 033 | Train Loss: 0.4316 | Val Loss: 0.4102 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4102)\r\n",
      "Epoch 034 | Train Loss: 0.4375 | Val Loss: 0.4122 | Time: 6.29s\r\n",
      "Epoch 035 | Train Loss: 0.4326 | Val Loss: 0.4126 | Time: 6.24s\r\n",
      "Epoch 036 | Train Loss: 0.4283 | Val Loss: 0.4114 | Time: 6.51s\r\n",
      "Epoch 037 | Train Loss: 0.4281 | Val Loss: 0.4117 | Time: 6.26s\r\n",
      "Epoch 038 | Train Loss: 0.4286 | Val Loss: 0.4096 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4096)\r\n",
      "Epoch 039 | Train Loss: 0.4301 | Val Loss: 0.4112 | Time: 6.32s\r\n",
      "Epoch 040 | Train Loss: 0.4246 | Val Loss: 0.4104 | Time: 6.34s\r\n",
      "Epoch 041 | Train Loss: 0.4248 | Val Loss: 0.4115 | Time: 6.38s\r\n",
      "Epoch 042 | Train Loss: 0.4298 | Val Loss: 0.4112 | Time: 6.28s\r\n",
      "Epoch 043 | Train Loss: 0.4292 | Val Loss: 0.4087 | Time: 6.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4087)\r\n",
      "Epoch 044 | Train Loss: 0.4229 | Val Loss: 0.4119 | Time: 6.19s\r\n",
      "Epoch 045 | Train Loss: 0.4247 | Val Loss: 0.4087 | Time: 6.37s\r\n",
      "Epoch 046 | Train Loss: 0.4275 | Val Loss: 0.4114 | Time: 6.37s\r\n",
      "Epoch 047 | Train Loss: 0.4243 | Val Loss: 0.4095 | Time: 6.25s\r\n",
      "Epoch 048 | Train Loss: 0.4250 | Val Loss: 0.4109 | Time: 6.50s\r\n",
      "Epoch 049 | Train Loss: 0.4257 | Val Loss: 0.4096 | Time: 6.75s\r\n",
      "✅ Meilleur modèle Random chargé (époque 43, val_loss: 0.4087)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. TotalCharges         (NUM): 0.0939\r\n",
      "   2. Partner              (CAT): 0.0861\r\n",
      "   3. gender               (CAT): 0.0818\r\n",
      "   4. DeviceProtection     (CAT): 0.0809\r\n",
      "   5. PaymentMethod        (CAT): 0.0731\r\n",
      "   6. PaperlessBilling     (CAT): 0.0713\r\n",
      "   7. StreamingMovies      (CAT): 0.0647\r\n",
      "   8. TechSupport          (CAT): 0.0646\r\n",
      "   9. Dependents           (CAT): 0.0598\r\n",
      "  10. MonthlyCharges       (NUM): 0.0586\r\n",
      "  11. tenure               (NUM): 0.0565\r\n",
      "  12. OnlineBackup         (CAT): 0.0565\r\n",
      "  13. MultipleLines        (CAT): 0.0514\r\n",
      "  14. OnlineSecurity       (CAT): 0.0508\r\n",
      "  15. SeniorCitizen        (CAT): 0.0500\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TotalCharges        : 0.0939\r\n",
      "   2. Partner             : 0.0861\r\n",
      "   3. gender              : 0.0818\r\n",
      "   4. DeviceProtection    : 0.0809\r\n",
      "   5. PaymentMethod       : 0.0731\r\n",
      "   6. PaperlessBilling    : 0.0713\r\n",
      "   7. StreamingMovies     : 0.0647\r\n",
      "   8. TechSupport         : 0.0646\r\n",
      "   9. Dependents          : 0.0598\r\n",
      "  10. MonthlyCharges      : 0.0586\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 400.3s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 31,617 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5342 | Val Loss: 0.4952 | Time: 3.12s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4952)\r\n",
      "Epoch 001 | Train Loss: 0.5216 | Val Loss: 0.4872 | Time: 2.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4872)\r\n",
      "Epoch 002 | Train Loss: 0.4967 | Val Loss: 0.4876 | Time: 3.05s\r\n",
      "Epoch 003 | Train Loss: 0.5235 | Val Loss: 0.5853 | Time: 2.96s\r\n",
      "Epoch 004 | Train Loss: 0.5781 | Val Loss: 0.5541 | Time: 2.93s\r\n",
      "Epoch 005 | Train Loss: 0.5587 | Val Loss: 0.5499 | Time: 2.95s\r\n",
      "Epoch 006 | Train Loss: 0.5713 | Val Loss: 0.5704 | Time: 2.94s\r\n",
      "Epoch 007 | Train Loss: 0.5744 | Val Loss: 0.5592 | Time: 3.00s\r\n",
      "Epoch 008 | Train Loss: 0.5757 | Val Loss: 0.5747 | Time: 2.94s\r\n",
      "Epoch 009 | Train Loss: 0.5626 | Val Loss: 0.5516 | Time: 2.97s\r\n",
      "Epoch 010 | Train Loss: 0.5592 | Val Loss: 0.5579 | Time: 2.99s\r\n",
      "Epoch 011 | Train Loss: 0.5567 | Val Loss: 0.5479 | Time: 2.95s\r\n",
      "Epoch 012 | Train Loss: 0.5534 | Val Loss: 0.5429 | Time: 2.93s\r\n",
      "Epoch 013 | Train Loss: 0.5543 | Val Loss: 0.5431 | Time: 3.12s\r\n",
      "Epoch 014 | Train Loss: 0.5516 | Val Loss: 0.5310 | Time: 2.91s\r\n",
      "Epoch 015 | Train Loss: 0.5631 | Val Loss: 0.5712 | Time: 2.93s\r\n",
      "Epoch 016 | Train Loss: 0.5624 | Val Loss: 0.5197 | Time: 2.94s\r\n",
      "Epoch 017 | Train Loss: 0.5535 | Val Loss: 0.5398 | Time: 2.97s\r\n",
      "Epoch 018 | Train Loss: 0.5411 | Val Loss: 0.5185 | Time: 2.93s\r\n",
      "Epoch 019 | Train Loss: 0.5426 | Val Loss: 0.5201 | Time: 2.87s\r\n",
      "Epoch 020 | Train Loss: 0.5224 | Val Loss: 0.5062 | Time: 2.90s\r\n",
      "Epoch 021 | Train Loss: 0.5320 | Val Loss: 0.5104 | Time: 2.92s\r\n",
      "Epoch 022 | Train Loss: 0.5318 | Val Loss: 0.5164 | Time: 2.94s\r\n",
      "Epoch 023 | Train Loss: 0.5301 | Val Loss: 0.5283 | Time: 3.02s\r\n",
      "Epoch 024 | Train Loss: 0.5354 | Val Loss: 0.5200 | Time: 2.97s\r\n",
      "Epoch 025 | Train Loss: 0.5410 | Val Loss: 0.5802 | Time: 2.94s\r\n",
      "Epoch 026 | Train Loss: 0.5663 | Val Loss: 0.5676 | Time: 2.93s\r\n",
      "\r\n",
      "Early stopping à l'époque 26 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 1, val_loss: 0.4872)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.2377\r\n",
      "   2. OnlineSecurity      : 0.1829\r\n",
      "   3. tenure              : 0.1044\r\n",
      "   4. InternetService     : 0.0917\r\n",
      "   5. DeviceProtection    : 0.0912\r\n",
      "   6. StreamingTV         : 0.0496\r\n",
      "   7. PaperlessBilling    : 0.0448\r\n",
      "   8. Dependents          : 0.0436\r\n",
      "   9. PhoneService        : 0.0415\r\n",
      "  10. Contract            : 0.0352\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 15 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. OnlineBackup         (CAT): 0.2377\r\n",
      "   2. OnlineSecurity       (CAT): 0.1829\r\n",
      "   3. tenure               (NUM): 0.1044\r\n",
      "   4. InternetService      (CAT): 0.0917\r\n",
      "   5. DeviceProtection     (CAT): 0.0912\r\n",
      "   6. StreamingTV          (CAT): 0.0496\r\n",
      "   7. PaperlessBilling     (CAT): 0.0448\r\n",
      "   8. Dependents           (CAT): 0.0436\r\n",
      "   9. PhoneService         (CAT): 0.0415\r\n",
      "  10. Contract             (CAT): 0.0352\r\n",
      "  11. MultipleLines        (CAT): 0.0294\r\n",
      "  12. TotalCharges         (NUM): 0.0242\r\n",
      "  13. TechSupport          (CAT): 0.0120\r\n",
      "  14. StreamingMovies      (CAT): 0.0066\r\n",
      "  15. gender               (CAT): 0.0019\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges'] → indices [0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['OnlineBackup', 'OnlineSecurity', 'InternetService', 'DeviceProtection', 'StreamingTV', 'PaperlessBilling', 'Dependents', 'PhoneService', 'Contract', 'MultipleLines', 'TechSupport', 'StreamingMovies', 'gender'] → indices [8, 7, 6, 9, 11, 14, 3, 4, 13, 5, 10, 12, 0]\r\n",
      "📊 Features sélectionnées: 2 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 9 paires\r\n",
      "Modèle Random créé avec 270,081 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.25%\r\n",
      "   - Connexions feature-feature: 18\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5367 | Val Loss: 0.4938 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4938)\r\n",
      "Epoch 001 | Train Loss: 0.4854 | Val Loss: 0.4658 | Time: 6.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4658)\r\n",
      "Epoch 002 | Train Loss: 0.4605 | Val Loss: 0.4540 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4540)\r\n",
      "Epoch 003 | Train Loss: 0.4499 | Val Loss: 0.4548 | Time: 6.43s\r\n",
      "Epoch 004 | Train Loss: 0.4419 | Val Loss: 0.4385 | Time: 6.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 005 | Train Loss: 0.4409 | Val Loss: 0.4393 | Time: 6.26s\r\n",
      "Epoch 006 | Train Loss: 0.4333 | Val Loss: 0.4355 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4355)\r\n",
      "Epoch 007 | Train Loss: 0.4329 | Val Loss: 0.4336 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4336)\r\n",
      "Epoch 008 | Train Loss: 0.4309 | Val Loss: 0.4315 | Time: 6.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4315)\r\n",
      "Epoch 009 | Train Loss: 0.4294 | Val Loss: 0.4307 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 010 | Train Loss: 0.4304 | Val Loss: 0.4276 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4276)\r\n",
      "Epoch 011 | Train Loss: 0.4265 | Val Loss: 0.4250 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4250)\r\n",
      "Epoch 012 | Train Loss: 0.4291 | Val Loss: 0.4241 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 013 | Train Loss: 0.4255 | Val Loss: 0.4227 | Time: 6.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 014 | Train Loss: 0.4241 | Val Loss: 0.4214 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 015 | Train Loss: 0.4247 | Val Loss: 0.4209 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 016 | Train Loss: 0.4241 | Val Loss: 0.4216 | Time: 6.21s\r\n",
      "Epoch 017 | Train Loss: 0.4222 | Val Loss: 0.4237 | Time: 6.23s\r\n",
      "Epoch 018 | Train Loss: 0.4205 | Val Loss: 0.4224 | Time: 6.32s\r\n",
      "Epoch 019 | Train Loss: 0.4197 | Val Loss: 0.4236 | Time: 6.26s\r\n",
      "Epoch 020 | Train Loss: 0.4208 | Val Loss: 0.4216 | Time: 6.21s\r\n",
      "Epoch 021 | Train Loss: 0.4206 | Val Loss: 0.4207 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4207)\r\n",
      "Epoch 022 | Train Loss: 0.4201 | Val Loss: 0.4215 | Time: 6.22s\r\n",
      "Epoch 023 | Train Loss: 0.4204 | Val Loss: 0.4219 | Time: 6.37s\r\n",
      "Epoch 024 | Train Loss: 0.4194 | Val Loss: 0.4183 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 025 | Train Loss: 0.4241 | Val Loss: 0.4187 | Time: 6.23s\r\n",
      "Epoch 026 | Train Loss: 0.4174 | Val Loss: 0.4208 | Time: 6.21s\r\n",
      "Epoch 027 | Train Loss: 0.4192 | Val Loss: 0.4201 | Time: 6.27s\r\n",
      "Epoch 028 | Train Loss: 0.4150 | Val Loss: 0.4180 | Time: 6.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 029 | Train Loss: 0.4165 | Val Loss: 0.4165 | Time: 6.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4165)\r\n",
      "Epoch 030 | Train Loss: 0.4158 | Val Loss: 0.4176 | Time: 6.20s\r\n",
      "Epoch 031 | Train Loss: 0.4112 | Val Loss: 0.4183 | Time: 6.14s\r\n",
      "Epoch 032 | Train Loss: 0.4153 | Val Loss: 0.4173 | Time: 6.22s\r\n",
      "Epoch 033 | Train Loss: 0.4137 | Val Loss: 0.4174 | Time: 6.33s\r\n",
      "Epoch 034 | Train Loss: 0.4106 | Val Loss: 0.4172 | Time: 6.23s\r\n",
      "Epoch 035 | Train Loss: 0.4120 | Val Loss: 0.4160 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4160)\r\n",
      "Epoch 036 | Train Loss: 0.4158 | Val Loss: 0.4153 | Time: 6.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4153)\r\n",
      "Epoch 037 | Train Loss: 0.4135 | Val Loss: 0.4139 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4139)\r\n",
      "Epoch 038 | Train Loss: 0.4138 | Val Loss: 0.4140 | Time: 6.28s\r\n",
      "Epoch 039 | Train Loss: 0.4139 | Val Loss: 0.4144 | Time: 6.44s\r\n",
      "Epoch 040 | Train Loss: 0.4112 | Val Loss: 0.4155 | Time: 6.27s\r\n",
      "Epoch 041 | Train Loss: 0.4114 | Val Loss: 0.4151 | Time: 6.25s\r\n",
      "Epoch 042 | Train Loss: 0.4091 | Val Loss: 0.4146 | Time: 6.20s\r\n",
      "Epoch 043 | Train Loss: 0.4095 | Val Loss: 0.4143 | Time: 6.31s\r\n",
      "Epoch 044 | Train Loss: 0.4140 | Val Loss: 0.4156 | Time: 6.37s\r\n",
      "Epoch 045 | Train Loss: 0.4081 | Val Loss: 0.4152 | Time: 6.21s\r\n",
      "Epoch 046 | Train Loss: 0.4076 | Val Loss: 0.4166 | Time: 6.25s\r\n",
      "Epoch 047 | Train Loss: 0.4120 | Val Loss: 0.4136 | Time: 6.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4136)\r\n",
      "Epoch 048 | Train Loss: 0.4085 | Val Loss: 0.4137 | Time: 6.22s\r\n",
      "Epoch 049 | Train Loss: 0.4065 | Val Loss: 0.4135 | Time: 6.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4135)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4135)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineSecurity       (CAT): 0.0909\r\n",
      "   2. tenure               (NUM): 0.0902\r\n",
      "   3. InternetService      (CAT): 0.0818\r\n",
      "   4. MultipleLines        (CAT): 0.0811\r\n",
      "   5. StreamingMovies      (CAT): 0.0722\r\n",
      "   6. TechSupport          (CAT): 0.0690\r\n",
      "   7. Dependents           (CAT): 0.0686\r\n",
      "   8. gender               (CAT): 0.0645\r\n",
      "   9. DeviceProtection     (CAT): 0.0622\r\n",
      "  10. Contract             (CAT): 0.0615\r\n",
      "  11. StreamingTV          (CAT): 0.0593\r\n",
      "  12. PaperlessBilling     (CAT): 0.0533\r\n",
      "  13. OnlineBackup         (CAT): 0.0507\r\n",
      "  14. TotalCharges         (NUM): 0.0483\r\n",
      "  15. PhoneService         (CAT): 0.0464\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.0909\r\n",
      "   2. tenure              : 0.0902\r\n",
      "   3. InternetService     : 0.0818\r\n",
      "   4. MultipleLines       : 0.0811\r\n",
      "   5. StreamingMovies     : 0.0722\r\n",
      "   6. TechSupport         : 0.0690\r\n",
      "   7. Dependents          : 0.0686\r\n",
      "   8. gender              : 0.0645\r\n",
      "   9. DeviceProtection    : 0.0622\r\n",
      "  10. Contract            : 0.0615\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 396.7s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 31,617 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5613 | Val Loss: 0.5902 | Time: 2.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5902)\r\n",
      "Epoch 001 | Train Loss: 0.5768 | Val Loss: 0.5900 | Time: 2.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5900)\r\n",
      "Epoch 002 | Train Loss: 0.5515 | Val Loss: 0.5532 | Time: 2.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5532)\r\n",
      "Epoch 003 | Train Loss: 0.5395 | Val Loss: 0.5272 | Time: 2.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5272)\r\n",
      "Epoch 004 | Train Loss: 0.5498 | Val Loss: 0.5648 | Time: 2.99s\r\n",
      "Epoch 005 | Train Loss: 0.5484 | Val Loss: 0.5324 | Time: 2.97s\r\n",
      "Epoch 006 | Train Loss: 0.5432 | Val Loss: 0.5076 | Time: 2.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5076)\r\n",
      "Epoch 007 | Train Loss: 0.5329 | Val Loss: 0.5284 | Time: 3.08s\r\n",
      "Epoch 008 | Train Loss: 0.5285 | Val Loss: 0.4976 | Time: 3.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4976)\r\n",
      "Epoch 009 | Train Loss: 0.5249 | Val Loss: 0.5114 | Time: 2.93s\r\n",
      "Epoch 010 | Train Loss: 0.5227 | Val Loss: 0.5114 | Time: 2.93s\r\n",
      "Epoch 011 | Train Loss: 0.5165 | Val Loss: 0.4909 | Time: 2.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4909)\r\n",
      "Epoch 012 | Train Loss: 0.5187 | Val Loss: 0.5102 | Time: 2.92s\r\n",
      "Epoch 013 | Train Loss: 0.5108 | Val Loss: 0.5003 | Time: 2.93s\r\n",
      "Epoch 014 | Train Loss: 0.5043 | Val Loss: 0.4876 | Time: 2.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4876)\r\n",
      "Epoch 015 | Train Loss: 0.5033 | Val Loss: 0.4715 | Time: 2.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4715)\r\n",
      "Epoch 016 | Train Loss: 0.5001 | Val Loss: 0.4765 | Time: 2.90s\r\n",
      "Epoch 017 | Train Loss: 0.4910 | Val Loss: 0.4861 | Time: 2.91s\r\n",
      "Epoch 018 | Train Loss: 0.4931 | Val Loss: 0.4926 | Time: 3.05s\r\n",
      "Epoch 019 | Train Loss: 0.4995 | Val Loss: 0.5057 | Time: 2.92s\r\n",
      "Epoch 020 | Train Loss: 0.5090 | Val Loss: 0.5080 | Time: 2.89s\r\n",
      "Epoch 021 | Train Loss: 0.5088 | Val Loss: 0.4858 | Time: 2.93s\r\n",
      "Epoch 022 | Train Loss: 0.5102 | Val Loss: 0.5141 | Time: 2.89s\r\n",
      "Epoch 023 | Train Loss: 0.4970 | Val Loss: 0.5016 | Time: 2.94s\r\n",
      "Epoch 024 | Train Loss: 0.5125 | Val Loss: 0.5021 | Time: 2.98s\r\n",
      "Epoch 025 | Train Loss: 0.5229 | Val Loss: 0.5378 | Time: 2.98s\r\n",
      "Epoch 026 | Train Loss: 0.5011 | Val Loss: 0.5154 | Time: 2.94s\r\n",
      "Epoch 027 | Train Loss: 0.4944 | Val Loss: 0.4954 | Time: 2.91s\r\n",
      "Epoch 028 | Train Loss: 0.4918 | Val Loss: 0.4898 | Time: 2.96s\r\n",
      "Epoch 029 | Train Loss: 0.4954 | Val Loss: 0.4885 | Time: 3.07s\r\n",
      "Epoch 030 | Train Loss: 0.4879 | Val Loss: 0.4945 | Time: 2.91s\r\n",
      "Epoch 031 | Train Loss: 0.4960 | Val Loss: 0.4863 | Time: 2.95s\r\n",
      "Epoch 032 | Train Loss: 0.4812 | Val Loss: 0.4818 | Time: 2.94s\r\n",
      "Epoch 033 | Train Loss: 0.4928 | Val Loss: 0.4946 | Time: 3.00s\r\n",
      "Epoch 034 | Train Loss: 0.4892 | Val Loss: 0.4781 | Time: 2.94s\r\n",
      "Epoch 035 | Train Loss: 0.4894 | Val Loss: 0.4788 | Time: 2.94s\r\n",
      "Epoch 036 | Train Loss: 0.4869 | Val Loss: 0.4718 | Time: 2.90s\r\n",
      "Epoch 037 | Train Loss: 0.4902 | Val Loss: 0.4859 | Time: 2.88s\r\n",
      "Epoch 038 | Train Loss: 0.4890 | Val Loss: 0.4745 | Time: 2.97s\r\n",
      "Epoch 039 | Train Loss: 0.4942 | Val Loss: 0.4763 | Time: 2.95s\r\n",
      "Epoch 040 | Train Loss: 0.4805 | Val Loss: 0.4861 | Time: 3.13s\r\n",
      "\r\n",
      "Early stopping à l'époque 40 (patience: 25)\r\n",
      "✅ Meilleur modèle chargé (époque 15, val_loss: 0.4715)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MultipleLines       : 0.3969\r\n",
      "   2. PaperlessBilling    : 0.2059\r\n",
      "   3. tenure              : 0.1170\r\n",
      "   4. StreamingMovies     : 0.1165\r\n",
      "   5. PhoneService        : 0.0702\r\n",
      "   6. Partner             : 0.0292\r\n",
      "   7. DeviceProtection    : 0.0123\r\n",
      "   8. Dependents          : 0.0118\r\n",
      "   9. OnlineBackup        : 0.0109\r\n",
      "  10. OnlineSecurity      : 0.0074\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 15 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MultipleLines        (CAT): 0.3969\r\n",
      "   2. PaperlessBilling     (CAT): 0.2059\r\n",
      "   3. tenure               (NUM): 0.1170\r\n",
      "   4. StreamingMovies      (CAT): 0.1165\r\n",
      "   5. PhoneService         (CAT): 0.0702\r\n",
      "   6. Partner              (CAT): 0.0292\r\n",
      "   7. DeviceProtection     (CAT): 0.0123\r\n",
      "   8. Dependents           (CAT): 0.0118\r\n",
      "   9. OnlineBackup         (CAT): 0.0109\r\n",
      "  10. OnlineSecurity       (CAT): 0.0074\r\n",
      "  11. TechSupport          (CAT): 0.0070\r\n",
      "  12. InternetService      (CAT): 0.0059\r\n",
      "  13. Contract             (CAT): 0.0048\r\n",
      "  14. TotalCharges         (NUM): 0.0017\r\n",
      "  15. gender               (CAT): 0.0011\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges'] → indices [0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['MultipleLines', 'PaperlessBilling', 'StreamingMovies', 'PhoneService', 'Partner', 'DeviceProtection', 'Dependents', 'OnlineBackup', 'OnlineSecurity', 'TechSupport', 'InternetService', 'Contract', 'gender'] → indices [5, 14, 12, 4, 2, 9, 3, 8, 7, 10, 6, 13, 0]\r\n",
      "📊 Features sélectionnées: 2 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 9 paires\r\n",
      "Modèle Random créé avec 270,017 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.25%\r\n",
      "   - Connexions feature-feature: 18\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5884 | Val Loss: 0.4735 | Time: 6.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4735)\r\n",
      "Epoch 001 | Train Loss: 0.4981 | Val Loss: 0.4313 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4313)\r\n",
      "Epoch 002 | Train Loss: 0.4729 | Val Loss: 0.4287 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4287)\r\n",
      "Epoch 003 | Train Loss: 0.4642 | Val Loss: 0.4237 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4237)\r\n",
      "Epoch 004 | Train Loss: 0.4550 | Val Loss: 0.4232 | Time: 6.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4232)\r\n",
      "Epoch 005 | Train Loss: 0.4550 | Val Loss: 0.4198 | Time: 6.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4198)\r\n",
      "Epoch 006 | Train Loss: 0.4503 | Val Loss: 0.4192 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 007 | Train Loss: 0.4445 | Val Loss: 0.4180 | Time: 6.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 008 | Train Loss: 0.4484 | Val Loss: 0.4161 | Time: 6.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4161)\r\n",
      "Epoch 009 | Train Loss: 0.4456 | Val Loss: 0.4154 | Time: 6.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4154)\r\n",
      "Epoch 010 | Train Loss: 0.4459 | Val Loss: 0.4137 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 011 | Train Loss: 0.4408 | Val Loss: 0.4138 | Time: 6.23s\r\n",
      "Epoch 012 | Train Loss: 0.4380 | Val Loss: 0.4137 | Time: 6.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 013 | Train Loss: 0.4400 | Val Loss: 0.4136 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4136)\r\n",
      "Epoch 014 | Train Loss: 0.4400 | Val Loss: 0.4127 | Time: 6.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4127)\r\n",
      "Epoch 015 | Train Loss: 0.4412 | Val Loss: 0.4116 | Time: 6.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4116)\r\n",
      "Epoch 016 | Train Loss: 0.4418 | Val Loss: 0.4091 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4091)\r\n",
      "Epoch 017 | Train Loss: 0.4387 | Val Loss: 0.4084 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4084)\r\n",
      "Epoch 018 | Train Loss: 0.4350 | Val Loss: 0.4106 | Time: 6.25s\r\n",
      "Epoch 019 | Train Loss: 0.4361 | Val Loss: 0.4113 | Time: 6.32s\r\n",
      "Epoch 020 | Train Loss: 0.4348 | Val Loss: 0.4101 | Time: 6.16s\r\n",
      "Epoch 021 | Train Loss: 0.4346 | Val Loss: 0.4092 | Time: 6.19s\r\n",
      "Epoch 022 | Train Loss: 0.4335 | Val Loss: 0.4084 | Time: 6.21s\r\n",
      "Epoch 023 | Train Loss: 0.4343 | Val Loss: 0.4091 | Time: 6.23s\r\n",
      "Epoch 024 | Train Loss: 0.4265 | Val Loss: 0.4102 | Time: 6.34s\r\n",
      "Epoch 025 | Train Loss: 0.4378 | Val Loss: 0.4078 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4078)\r\n",
      "Epoch 026 | Train Loss: 0.4290 | Val Loss: 0.4099 | Time: 6.21s\r\n",
      "Epoch 027 | Train Loss: 0.4314 | Val Loss: 0.4118 | Time: 6.23s\r\n",
      "Epoch 028 | Train Loss: 0.4299 | Val Loss: 0.4103 | Time: 6.20s\r\n",
      "Epoch 029 | Train Loss: 0.4290 | Val Loss: 0.4107 | Time: 6.36s\r\n",
      "Epoch 030 | Train Loss: 0.4258 | Val Loss: 0.4103 | Time: 6.19s\r\n",
      "Epoch 031 | Train Loss: 0.4276 | Val Loss: 0.4122 | Time: 6.22s\r\n",
      "Epoch 032 | Train Loss: 0.4271 | Val Loss: 0.4113 | Time: 6.28s\r\n",
      "Epoch 033 | Train Loss: 0.4319 | Val Loss: 0.4118 | Time: 6.31s\r\n",
      "Epoch 034 | Train Loss: 0.4320 | Val Loss: 0.4121 | Time: 6.38s\r\n",
      "Epoch 035 | Train Loss: 0.4282 | Val Loss: 0.4139 | Time: 6.26s\r\n",
      "Epoch 036 | Train Loss: 0.4284 | Val Loss: 0.4114 | Time: 6.23s\r\n",
      "Epoch 037 | Train Loss: 0.4267 | Val Loss: 0.4114 | Time: 6.22s\r\n",
      "Epoch 038 | Train Loss: 0.4256 | Val Loss: 0.4119 | Time: 6.27s\r\n",
      "Epoch 039 | Train Loss: 0.4311 | Val Loss: 0.4130 | Time: 6.27s\r\n",
      "Epoch 040 | Train Loss: 0.4240 | Val Loss: 0.4127 | Time: 6.37s\r\n",
      "Epoch 041 | Train Loss: 0.4294 | Val Loss: 0.4108 | Time: 6.25s\r\n",
      "Epoch 042 | Train Loss: 0.4261 | Val Loss: 0.4118 | Time: 6.22s\r\n",
      "Epoch 043 | Train Loss: 0.4245 | Val Loss: 0.4130 | Time: 6.22s\r\n",
      "Epoch 044 | Train Loss: 0.4279 | Val Loss: 0.4151 | Time: 6.24s\r\n",
      "Epoch 045 | Train Loss: 0.4236 | Val Loss: 0.4139 | Time: 6.46s\r\n",
      "Epoch 046 | Train Loss: 0.4252 | Val Loss: 0.4163 | Time: 6.25s\r\n",
      "Epoch 047 | Train Loss: 0.4225 | Val Loss: 0.4166 | Time: 6.22s\r\n",
      "Epoch 048 | Train Loss: 0.4230 | Val Loss: 0.4160 | Time: 6.23s\r\n",
      "Epoch 049 | Train Loss: 0.4239 | Val Loss: 0.4157 | Time: 6.27s\r\n",
      "✅ Meilleur modèle Random chargé (époque 25, val_loss: 0.4078)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineSecurity       (CAT): 0.0881\r\n",
      "   2. gender               (CAT): 0.0845\r\n",
      "   3. StreamingMovies      (CAT): 0.0762\r\n",
      "   4. DeviceProtection     (CAT): 0.0728\r\n",
      "   5. PaperlessBilling     (CAT): 0.0706\r\n",
      "   6. TotalCharges         (NUM): 0.0678\r\n",
      "   7. PhoneService         (CAT): 0.0631\r\n",
      "   8. MultipleLines        (CAT): 0.0628\r\n",
      "   9. TechSupport          (CAT): 0.0614\r\n",
      "  10. tenure               (NUM): 0.0608\r\n",
      "  11. InternetService      (CAT): 0.0595\r\n",
      "  12. Partner              (CAT): 0.0585\r\n",
      "  13. OnlineBackup         (CAT): 0.0585\r\n",
      "  14. Dependents           (CAT): 0.0578\r\n",
      "  15. Contract             (CAT): 0.0577\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.0881\r\n",
      "   2. gender              : 0.0845\r\n",
      "   3. StreamingMovies     : 0.0762\r\n",
      "   4. DeviceProtection    : 0.0728\r\n",
      "   5. PaperlessBilling    : 0.0706\r\n",
      "   6. TotalCharges        : 0.0678\r\n",
      "   7. PhoneService        : 0.0631\r\n",
      "   8. MultipleLines       : 0.0628\r\n",
      "   9. TechSupport         : 0.0614\r\n",
      "  10. tenure              : 0.0608\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_7/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 438.3s ===\r\n",
      "\u001b[32m[I 2025-07-19 20:45:19,824]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 4, 'n_heads_stage1': 2, 'ffn_hidden_stage1': 128, 'attention_dropout_stage1': 0.17492252292529425, 'ffn_dropout_stage1': 0.22517198314284728, 'residual_dropout_stage1': 0.15031362585800878, 'lr_stage1': 0.026666091485947507, 'weight_decay_stage1': 0.001965477869022202, 'd_token_stage2': 64, 'n_blocks_stage2': 4, 'n_heads_stage2': 2, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.17722052756015486, 'ffn_dropout_stage2': 0.29223811276478284, 'residual_dropout_stage2': 0.1905350641956064, 'lr_stage2': 6.069662445477729e-05, 'weight_decay_stage2': 2.2223195374893766e-06, 'batch_size': 32, 'patience': 25, 'embedding_type': 'Q', 'M': 15, 'k': 9}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  32%|██▌     | 8/25 [2:32:22<4:47:41, 1015.35s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 150,913 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5036 | Val Loss: 0.4652 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4652)\r\n",
      "Epoch 001 | Train Loss: 0.4544 | Val Loss: 0.4581 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4581)\r\n",
      "Epoch 002 | Train Loss: 0.4437 | Val Loss: 0.4348 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4348)\r\n",
      "Epoch 003 | Train Loss: 0.4371 | Val Loss: 0.4205 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4205)\r\n",
      "Epoch 004 | Train Loss: 0.4410 | Val Loss: 0.4386 | Time: 1.30s\r\n",
      "Epoch 005 | Train Loss: 0.4421 | Val Loss: 0.4256 | Time: 1.28s\r\n",
      "Epoch 006 | Train Loss: 0.4328 | Val Loss: 0.4350 | Time: 1.28s\r\n",
      "Epoch 007 | Train Loss: 0.4391 | Val Loss: 0.4202 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4202)\r\n",
      "Epoch 008 | Train Loss: 0.4344 | Val Loss: 0.4213 | Time: 1.27s\r\n",
      "Epoch 009 | Train Loss: 0.4372 | Val Loss: 0.4117 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4117)\r\n",
      "Epoch 010 | Train Loss: 0.4284 | Val Loss: 0.4145 | Time: 1.29s\r\n",
      "Epoch 011 | Train Loss: 0.4297 | Val Loss: 0.4170 | Time: 1.27s\r\n",
      "Epoch 012 | Train Loss: 0.4280 | Val Loss: 0.4161 | Time: 1.32s\r\n",
      "Epoch 013 | Train Loss: 0.4326 | Val Loss: 0.4133 | Time: 1.28s\r\n",
      "Epoch 014 | Train Loss: 0.4297 | Val Loss: 0.4067 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4067)\r\n",
      "Epoch 015 | Train Loss: 0.4249 | Val Loss: 0.4114 | Time: 1.27s\r\n",
      "Epoch 016 | Train Loss: 0.4273 | Val Loss: 0.4078 | Time: 1.26s\r\n",
      "Epoch 017 | Train Loss: 0.4258 | Val Loss: 0.4365 | Time: 1.28s\r\n",
      "Epoch 018 | Train Loss: 0.4332 | Val Loss: 0.4125 | Time: 1.28s\r\n",
      "Epoch 019 | Train Loss: 0.4318 | Val Loss: 0.4254 | Time: 1.29s\r\n",
      "Epoch 020 | Train Loss: 0.4451 | Val Loss: 0.4265 | Time: 1.30s\r\n",
      "Epoch 021 | Train Loss: 0.4493 | Val Loss: 0.4214 | Time: 1.27s\r\n",
      "Epoch 022 | Train Loss: 0.4412 | Val Loss: 0.4296 | Time: 1.37s\r\n",
      "Epoch 023 | Train Loss: 0.4344 | Val Loss: 0.4105 | Time: 1.29s\r\n",
      "Epoch 024 | Train Loss: 0.4373 | Val Loss: 0.4085 | Time: 1.27s\r\n",
      "Epoch 025 | Train Loss: 0.4370 | Val Loss: 0.4117 | Time: 1.27s\r\n",
      "Epoch 026 | Train Loss: 0.4327 | Val Loss: 0.4106 | Time: 1.28s\r\n",
      "Epoch 027 | Train Loss: 0.4320 | Val Loss: 0.4103 | Time: 1.31s\r\n",
      "Epoch 028 | Train Loss: 0.4332 | Val Loss: 0.4121 | Time: 1.27s\r\n",
      "Epoch 029 | Train Loss: 0.4334 | Val Loss: 0.4174 | Time: 1.27s\r\n",
      "Epoch 030 | Train Loss: 0.4321 | Val Loss: 0.4190 | Time: 1.29s\r\n",
      "Epoch 031 | Train Loss: 0.4367 | Val Loss: 0.4128 | Time: 1.28s\r\n",
      "Epoch 032 | Train Loss: 0.4280 | Val Loss: 0.4143 | Time: 1.27s\r\n",
      "Epoch 033 | Train Loss: 0.4321 | Val Loss: 0.4384 | Time: 1.28s\r\n",
      "Epoch 034 | Train Loss: 0.4433 | Val Loss: 0.4299 | Time: 1.28s\r\n",
      "Epoch 035 | Train Loss: 0.4338 | Val Loss: 0.4069 | Time: 1.31s\r\n",
      "Epoch 036 | Train Loss: 0.4320 | Val Loss: 0.4079 | Time: 1.26s\r\n",
      "Epoch 037 | Train Loss: 0.4289 | Val Loss: 0.4098 | Time: 1.26s\r\n",
      "Epoch 038 | Train Loss: 0.4315 | Val Loss: 0.4041 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4041)\r\n",
      "Epoch 039 | Train Loss: 0.4359 | Val Loss: 0.4083 | Time: 1.26s\r\n",
      "Epoch 040 | Train Loss: 0.4333 | Val Loss: 0.4116 | Time: 1.28s\r\n",
      "Epoch 041 | Train Loss: 0.4300 | Val Loss: 0.4089 | Time: 1.28s\r\n",
      "Epoch 042 | Train Loss: 0.4281 | Val Loss: 0.4125 | Time: 1.27s\r\n",
      "Epoch 043 | Train Loss: 0.4308 | Val Loss: 0.4106 | Time: 1.32s\r\n",
      "Epoch 044 | Train Loss: 0.4253 | Val Loss: 0.4096 | Time: 1.29s\r\n",
      "Epoch 045 | Train Loss: 0.4228 | Val Loss: 0.4110 | Time: 1.28s\r\n",
      "Epoch 046 | Train Loss: 0.4262 | Val Loss: 0.4096 | Time: 1.28s\r\n",
      "Epoch 047 | Train Loss: 0.4285 | Val Loss: 0.4141 | Time: 1.42s\r\n",
      "Epoch 048 | Train Loss: 0.4260 | Val Loss: 0.4067 | Time: 1.27s\r\n",
      "Epoch 049 | Train Loss: 0.4236 | Val Loss: 0.4039 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4039)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4039)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.1794\r\n",
      "   2. Partner             : 0.1473\r\n",
      "   3. PaperlessBilling    : 0.0730\r\n",
      "   4. Dependents          : 0.0711\r\n",
      "   5. TechSupport         : 0.0665\r\n",
      "   6. Contract            : 0.0636\r\n",
      "   7. tenure              : 0.0534\r\n",
      "   8. StreamingMovies     : 0.0438\r\n",
      "   9. MultipleLines       : 0.0405\r\n",
      "  10. StreamingTV         : 0.0343\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 7 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MonthlyCharges       (NUM): 0.1794\r\n",
      "   2. Partner              (CAT): 0.1473\r\n",
      "   3. PaperlessBilling     (CAT): 0.0730\r\n",
      "   4. Dependents           (CAT): 0.0711\r\n",
      "   5. TechSupport          (CAT): 0.0665\r\n",
      "   6. Contract             (CAT): 0.0636\r\n",
      "   7. tenure               (NUM): 0.0534\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure'] → indices [1, 0]\r\n",
      "   - Catégorielles sélectionnées: ['Partner', 'PaperlessBilling', 'Dependents', 'TechSupport', 'Contract'] → indices [2, 14, 3, 10, 13]\r\n",
      "📊 Features sélectionnées: 2 numériques, 5 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 84,209 paramètres\r\n",
      "🔗 Sparsité d'attention: 53.12%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6359 | Val Loss: 0.5767 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5767)\r\n",
      "Epoch 001 | Train Loss: 0.5733 | Val Loss: 0.5621 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5621)\r\n",
      "Epoch 002 | Train Loss: 0.5637 | Val Loss: 0.5459 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5459)\r\n",
      "Epoch 003 | Train Loss: 0.5428 | Val Loss: 0.5108 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5108)\r\n",
      "Epoch 004 | Train Loss: 0.5248 | Val Loss: 0.4937 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4937)\r\n",
      "Epoch 005 | Train Loss: 0.5095 | Val Loss: 0.4894 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4894)\r\n",
      "Epoch 006 | Train Loss: 0.5036 | Val Loss: 0.4867 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4867)\r\n",
      "Epoch 007 | Train Loss: 0.4944 | Val Loss: 0.4813 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4813)\r\n",
      "Epoch 008 | Train Loss: 0.4882 | Val Loss: 0.4803 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4803)\r\n",
      "Epoch 009 | Train Loss: 0.4844 | Val Loss: 0.4786 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4786)\r\n",
      "Epoch 010 | Train Loss: 0.4829 | Val Loss: 0.4750 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4750)\r\n",
      "Epoch 011 | Train Loss: 0.4779 | Val Loss: 0.4739 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4739)\r\n",
      "Epoch 012 | Train Loss: 0.4751 | Val Loss: 0.4702 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4702)\r\n",
      "Epoch 013 | Train Loss: 0.4730 | Val Loss: 0.4703 | Time: 2.36s\r\n",
      "Epoch 014 | Train Loss: 0.4739 | Val Loss: 0.4675 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4675)\r\n",
      "Epoch 015 | Train Loss: 0.4739 | Val Loss: 0.4683 | Time: 2.37s\r\n",
      "Epoch 016 | Train Loss: 0.4701 | Val Loss: 0.4644 | Time: 2.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4644)\r\n",
      "Epoch 017 | Train Loss: 0.4678 | Val Loss: 0.4647 | Time: 2.39s\r\n",
      "Epoch 018 | Train Loss: 0.4617 | Val Loss: 0.4638 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4638)\r\n",
      "Epoch 019 | Train Loss: 0.4672 | Val Loss: 0.4637 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4637)\r\n",
      "Epoch 020 | Train Loss: 0.4680 | Val Loss: 0.4621 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4621)\r\n",
      "Epoch 021 | Train Loss: 0.4657 | Val Loss: 0.4615 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4615)\r\n",
      "Epoch 022 | Train Loss: 0.4660 | Val Loss: 0.4606 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4606)\r\n",
      "Epoch 023 | Train Loss: 0.4639 | Val Loss: 0.4600 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4600)\r\n",
      "Epoch 024 | Train Loss: 0.4625 | Val Loss: 0.4582 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4582)\r\n",
      "Epoch 025 | Train Loss: 0.4608 | Val Loss: 0.4614 | Time: 2.41s\r\n",
      "Epoch 026 | Train Loss: 0.4641 | Val Loss: 0.4594 | Time: 2.38s\r\n",
      "Epoch 027 | Train Loss: 0.4572 | Val Loss: 0.4589 | Time: 2.36s\r\n",
      "Epoch 028 | Train Loss: 0.4582 | Val Loss: 0.4574 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4574)\r\n",
      "Epoch 029 | Train Loss: 0.4560 | Val Loss: 0.4565 | Time: 2.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4565)\r\n",
      "Epoch 030 | Train Loss: 0.4556 | Val Loss: 0.4566 | Time: 2.38s\r\n",
      "Epoch 031 | Train Loss: 0.4590 | Val Loss: 0.4569 | Time: 2.37s\r\n",
      "Epoch 032 | Train Loss: 0.4591 | Val Loss: 0.4547 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4547)\r\n",
      "Epoch 033 | Train Loss: 0.4548 | Val Loss: 0.4541 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4541)\r\n",
      "Epoch 034 | Train Loss: 0.4532 | Val Loss: 0.4557 | Time: 2.38s\r\n",
      "Epoch 035 | Train Loss: 0.4544 | Val Loss: 0.4552 | Time: 2.38s\r\n",
      "Epoch 036 | Train Loss: 0.4531 | Val Loss: 0.4516 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4516)\r\n",
      "Epoch 037 | Train Loss: 0.4542 | Val Loss: 0.4513 | Time: 2.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4513)\r\n",
      "Epoch 038 | Train Loss: 0.4551 | Val Loss: 0.4517 | Time: 2.38s\r\n",
      "Epoch 039 | Train Loss: 0.4497 | Val Loss: 0.4497 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4497)\r\n",
      "Epoch 040 | Train Loss: 0.4540 | Val Loss: 0.4510 | Time: 2.40s\r\n",
      "Epoch 041 | Train Loss: 0.4508 | Val Loss: 0.4486 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4486)\r\n",
      "Epoch 042 | Train Loss: 0.4509 | Val Loss: 0.4482 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4482)\r\n",
      "Epoch 043 | Train Loss: 0.4511 | Val Loss: 0.4473 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4473)\r\n",
      "Epoch 044 | Train Loss: 0.4501 | Val Loss: 0.4446 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4446)\r\n",
      "Epoch 045 | Train Loss: 0.4518 | Val Loss: 0.4459 | Time: 2.42s\r\n",
      "Epoch 046 | Train Loss: 0.4472 | Val Loss: 0.4459 | Time: 2.39s\r\n",
      "Epoch 047 | Train Loss: 0.4532 | Val Loss: 0.4463 | Time: 2.37s\r\n",
      "Epoch 048 | Train Loss: 0.4480 | Val Loss: 0.4446 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4446)\r\n",
      "Epoch 049 | Train Loss: 0.4470 | Val Loss: 0.4419 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4419)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. tenure               (NUM): 0.1508\r\n",
      "   2. Contract             (CAT): 0.1468\r\n",
      "   3. Dependents           (CAT): 0.1451\r\n",
      "   4. Partner              (CAT): 0.1436\r\n",
      "   5. TechSupport          (CAT): 0.1413\r\n",
      "   6. PaperlessBilling     (CAT): 0.1408\r\n",
      "   7. MonthlyCharges       (NUM): 0.1316\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.1508\r\n",
      "   2. Contract            : 0.1468\r\n",
      "   3. Dependents          : 0.1451\r\n",
      "   4. Partner             : 0.1436\r\n",
      "   5. TechSupport         : 0.1413\r\n",
      "   6. PaperlessBilling    : 0.1408\r\n",
      "   7. MonthlyCharges      : 0.1316\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 187.4s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 150,913 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5131 | Val Loss: 0.4800 | Time: 1.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4800)\r\n",
      "Epoch 001 | Train Loss: 0.4701 | Val Loss: 0.4545 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4545)\r\n",
      "Epoch 002 | Train Loss: 0.4455 | Val Loss: 0.4705 | Time: 1.27s\r\n",
      "Epoch 003 | Train Loss: 0.4430 | Val Loss: 0.4348 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4348)\r\n",
      "Epoch 004 | Train Loss: 0.4431 | Val Loss: 0.4392 | Time: 1.28s\r\n",
      "Epoch 005 | Train Loss: 0.4388 | Val Loss: 0.4322 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4322)\r\n",
      "Epoch 006 | Train Loss: 0.4331 | Val Loss: 0.4245 | Time: 1.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4245)\r\n",
      "Epoch 007 | Train Loss: 0.4289 | Val Loss: 0.4350 | Time: 1.28s\r\n",
      "Epoch 008 | Train Loss: 0.4339 | Val Loss: 0.4336 | Time: 1.28s\r\n",
      "Epoch 009 | Train Loss: 0.4292 | Val Loss: 0.4271 | Time: 1.32s\r\n",
      "Epoch 010 | Train Loss: 0.4383 | Val Loss: 0.4227 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 011 | Train Loss: 0.4343 | Val Loss: 0.4440 | Time: 1.29s\r\n",
      "Epoch 012 | Train Loss: 0.4441 | Val Loss: 0.4277 | Time: 1.29s\r\n",
      "Epoch 013 | Train Loss: 0.4440 | Val Loss: 0.4613 | Time: 1.32s\r\n",
      "Epoch 014 | Train Loss: 0.4452 | Val Loss: 0.4621 | Time: 1.29s\r\n",
      "Epoch 015 | Train Loss: 0.4387 | Val Loss: 0.4305 | Time: 1.29s\r\n",
      "Epoch 016 | Train Loss: 0.4352 | Val Loss: 0.4333 | Time: 1.28s\r\n",
      "Epoch 017 | Train Loss: 0.4331 | Val Loss: 0.4256 | Time: 1.28s\r\n",
      "Epoch 018 | Train Loss: 0.4362 | Val Loss: 0.4285 | Time: 1.28s\r\n",
      "Epoch 019 | Train Loss: 0.4363 | Val Loss: 0.4237 | Time: 1.30s\r\n",
      "Epoch 020 | Train Loss: 0.4433 | Val Loss: 0.4365 | Time: 1.29s\r\n",
      "Epoch 021 | Train Loss: 0.4473 | Val Loss: 0.4386 | Time: 1.32s\r\n",
      "Epoch 022 | Train Loss: 0.4435 | Val Loss: 0.4528 | Time: 1.28s\r\n",
      "Epoch 023 | Train Loss: 0.4454 | Val Loss: 0.4328 | Time: 1.28s\r\n",
      "Epoch 024 | Train Loss: 0.4384 | Val Loss: 0.4287 | Time: 1.39s\r\n",
      "Epoch 025 | Train Loss: 0.4300 | Val Loss: 0.4296 | Time: 1.27s\r\n",
      "Epoch 026 | Train Loss: 0.4283 | Val Loss: 0.4299 | Time: 1.28s\r\n",
      "Epoch 027 | Train Loss: 0.4272 | Val Loss: 0.4824 | Time: 1.29s\r\n",
      "Epoch 028 | Train Loss: 0.4478 | Val Loss: 0.4357 | Time: 1.27s\r\n",
      "Epoch 029 | Train Loss: 0.4320 | Val Loss: 0.4358 | Time: 1.32s\r\n",
      "Epoch 030 | Train Loss: 0.4313 | Val Loss: 0.4347 | Time: 1.34s\r\n",
      "Epoch 031 | Train Loss: 0.4316 | Val Loss: 0.4304 | Time: 1.29s\r\n",
      "Epoch 032 | Train Loss: 0.4289 | Val Loss: 0.4252 | Time: 1.29s\r\n",
      "Epoch 033 | Train Loss: 0.4247 | Val Loss: 0.4288 | Time: 1.28s\r\n",
      "Epoch 034 | Train Loss: 0.4236 | Val Loss: 0.4293 | Time: 1.28s\r\n",
      "Epoch 035 | Train Loss: 0.4222 | Val Loss: 0.4199 | Time: 1.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4199)\r\n",
      "Epoch 036 | Train Loss: 0.4227 | Val Loss: 0.4321 | Time: 1.27s\r\n",
      "Epoch 037 | Train Loss: 0.4294 | Val Loss: 0.4257 | Time: 1.32s\r\n",
      "Epoch 038 | Train Loss: 0.4319 | Val Loss: 0.4298 | Time: 1.29s\r\n",
      "Epoch 039 | Train Loss: 0.4239 | Val Loss: 0.4325 | Time: 1.28s\r\n",
      "Epoch 040 | Train Loss: 0.4295 | Val Loss: 0.4254 | Time: 1.28s\r\n",
      "Epoch 041 | Train Loss: 0.4281 | Val Loss: 0.4240 | Time: 1.27s\r\n",
      "Epoch 042 | Train Loss: 0.4280 | Val Loss: 0.4250 | Time: 1.28s\r\n",
      "Epoch 043 | Train Loss: 0.4336 | Val Loss: 0.4243 | Time: 1.29s\r\n",
      "Epoch 044 | Train Loss: 0.4257 | Val Loss: 0.4325 | Time: 1.32s\r\n",
      "Epoch 045 | Train Loss: 0.4304 | Val Loss: 0.4240 | Time: 1.27s\r\n",
      "Epoch 046 | Train Loss: 0.4255 | Val Loss: 0.4245 | Time: 1.29s\r\n",
      "Epoch 047 | Train Loss: 0.4264 | Val Loss: 0.4235 | Time: 1.28s\r\n",
      "Epoch 048 | Train Loss: 0.4231 | Val Loss: 0.4299 | Time: 1.33s\r\n",
      "Epoch 049 | Train Loss: 0.4246 | Val Loss: 0.4238 | Time: 1.31s\r\n",
      "✅ Meilleur modèle chargé (époque 35, val_loss: 0.4199)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.2376\r\n",
      "   2. StreamingMovies     : 0.2037\r\n",
      "   3. SeniorCitizen       : 0.1053\r\n",
      "   4. OnlineBackup        : 0.0940\r\n",
      "   5. PaperlessBilling    : 0.0643\r\n",
      "   6. TechSupport         : 0.0592\r\n",
      "   7. Partner             : 0.0412\r\n",
      "   8. DeviceProtection    : 0.0406\r\n",
      "   9. PhoneService        : 0.0294\r\n",
      "  10. gender              : 0.0281\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 7 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Contract             (CAT): 0.2376\r\n",
      "   2. StreamingMovies      (CAT): 0.2037\r\n",
      "   3. SeniorCitizen        (CAT): 0.1053\r\n",
      "   4. OnlineBackup         (CAT): 0.0940\r\n",
      "   5. PaperlessBilling     (CAT): 0.0643\r\n",
      "   6. TechSupport          (CAT): 0.0592\r\n",
      "   7. Partner              (CAT): 0.0412\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['Contract', 'StreamingMovies', 'SeniorCitizen', 'OnlineBackup', 'PaperlessBilling', 'TechSupport', 'Partner'] → indices [13, 12, 1, 8, 14, 10, 2]\r\n",
      "📊 Features sélectionnées: 0 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 84,273 paramètres\r\n",
      "🔗 Sparsité d'attention: 53.12%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5800 | Val Loss: 0.5540 | Time: 2.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5540)\r\n",
      "Epoch 001 | Train Loss: 0.5574 | Val Loss: 0.5365 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5365)\r\n",
      "Epoch 002 | Train Loss: 0.5454 | Val Loss: 0.5180 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5180)\r\n",
      "Epoch 003 | Train Loss: 0.5309 | Val Loss: 0.5103 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5103)\r\n",
      "Epoch 004 | Train Loss: 0.5138 | Val Loss: 0.5085 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5085)\r\n",
      "Epoch 005 | Train Loss: 0.5108 | Val Loss: 0.5046 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5046)\r\n",
      "Epoch 006 | Train Loss: 0.5038 | Val Loss: 0.5048 | Time: 2.34s\r\n",
      "Epoch 007 | Train Loss: 0.4962 | Val Loss: 0.4980 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4980)\r\n",
      "Epoch 008 | Train Loss: 0.4905 | Val Loss: 0.4989 | Time: 2.37s\r\n",
      "Epoch 009 | Train Loss: 0.4882 | Val Loss: 0.4973 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4973)\r\n",
      "Epoch 010 | Train Loss: 0.4885 | Val Loss: 0.4940 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4940)\r\n",
      "Epoch 011 | Train Loss: 0.4839 | Val Loss: 0.4942 | Time: 2.36s\r\n",
      "Epoch 012 | Train Loss: 0.4812 | Val Loss: 0.4926 | Time: 2.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4926)\r\n",
      "Epoch 013 | Train Loss: 0.4820 | Val Loss: 0.4922 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4922)\r\n",
      "Epoch 014 | Train Loss: 0.4834 | Val Loss: 0.4904 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4904)\r\n",
      "Epoch 015 | Train Loss: 0.4804 | Val Loss: 0.4912 | Time: 2.38s\r\n",
      "Epoch 016 | Train Loss: 0.4776 | Val Loss: 0.4899 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4899)\r\n",
      "Epoch 017 | Train Loss: 0.4756 | Val Loss: 0.4893 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4893)\r\n",
      "Epoch 018 | Train Loss: 0.4750 | Val Loss: 0.4889 | Time: 2.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4889)\r\n",
      "Epoch 019 | Train Loss: 0.4753 | Val Loss: 0.4893 | Time: 2.35s\r\n",
      "Epoch 020 | Train Loss: 0.4735 | Val Loss: 0.4881 | Time: 2.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4881)\r\n",
      "Epoch 021 | Train Loss: 0.4734 | Val Loss: 0.4875 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4875)\r\n",
      "Epoch 022 | Train Loss: 0.4727 | Val Loss: 0.4876 | Time: 2.35s\r\n",
      "Epoch 023 | Train Loss: 0.4714 | Val Loss: 0.4864 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4864)\r\n",
      "Epoch 024 | Train Loss: 0.4735 | Val Loss: 0.4857 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4857)\r\n",
      "Epoch 025 | Train Loss: 0.4714 | Val Loss: 0.4854 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4854)\r\n",
      "Epoch 026 | Train Loss: 0.4722 | Val Loss: 0.4850 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4850)\r\n",
      "Epoch 027 | Train Loss: 0.4682 | Val Loss: 0.4846 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4846)\r\n",
      "Epoch 028 | Train Loss: 0.4700 | Val Loss: 0.4847 | Time: 2.37s\r\n",
      "Epoch 029 | Train Loss: 0.4695 | Val Loss: 0.4847 | Time: 2.36s\r\n",
      "Epoch 030 | Train Loss: 0.4696 | Val Loss: 0.4826 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4826)\r\n",
      "Epoch 031 | Train Loss: 0.4674 | Val Loss: 0.4817 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4817)\r\n",
      "Epoch 032 | Train Loss: 0.4666 | Val Loss: 0.4810 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4810)\r\n",
      "Epoch 033 | Train Loss: 0.4656 | Val Loss: 0.4818 | Time: 2.39s\r\n",
      "Epoch 034 | Train Loss: 0.4653 | Val Loss: 0.4808 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4808)\r\n",
      "Epoch 035 | Train Loss: 0.4666 | Val Loss: 0.4801 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4801)\r\n",
      "Epoch 036 | Train Loss: 0.4642 | Val Loss: 0.4793 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4793)\r\n",
      "Epoch 037 | Train Loss: 0.4643 | Val Loss: 0.4782 | Time: 2.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4782)\r\n",
      "Epoch 038 | Train Loss: 0.4667 | Val Loss: 0.4770 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4770)\r\n",
      "Epoch 039 | Train Loss: 0.4670 | Val Loss: 0.4761 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4761)\r\n",
      "Epoch 040 | Train Loss: 0.4634 | Val Loss: 0.4766 | Time: 2.37s\r\n",
      "Epoch 041 | Train Loss: 0.4636 | Val Loss: 0.4746 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4746)\r\n",
      "Epoch 042 | Train Loss: 0.4639 | Val Loss: 0.4741 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4741)\r\n",
      "Epoch 043 | Train Loss: 0.4579 | Val Loss: 0.4740 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4740)\r\n",
      "Epoch 044 | Train Loss: 0.4590 | Val Loss: 0.4724 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4724)\r\n",
      "Epoch 045 | Train Loss: 0.4600 | Val Loss: 0.4732 | Time: 2.37s\r\n",
      "Epoch 046 | Train Loss: 0.4603 | Val Loss: 0.4721 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4721)\r\n",
      "Epoch 047 | Train Loss: 0.4560 | Val Loss: 0.4705 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4705)\r\n",
      "Epoch 048 | Train Loss: 0.4585 | Val Loss: 0.4698 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4698)\r\n",
      "Epoch 049 | Train Loss: 0.4591 | Val Loss: 0.4692 | Time: 2.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4692)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4692)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.1632\r\n",
      "   2. PaperlessBilling     (CAT): 0.1474\r\n",
      "   3. OnlineBackup         (CAT): 0.1425\r\n",
      "   4. StreamingMovies      (CAT): 0.1421\r\n",
      "   5. TechSupport          (CAT): 0.1384\r\n",
      "   6. Partner              (CAT): 0.1349\r\n",
      "   7. Contract             (CAT): 0.1315\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.1632\r\n",
      "   2. PaperlessBilling    : 0.1474\r\n",
      "   3. OnlineBackup        : 0.1425\r\n",
      "   4. StreamingMovies     : 0.1421\r\n",
      "   5. TechSupport         : 0.1384\r\n",
      "   6. Partner             : 0.1349\r\n",
      "   7. Contract            : 0.1315\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 187.1s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 150,913 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5059 | Val Loss: 0.4706 | Time: 1.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4706)\r\n",
      "Epoch 001 | Train Loss: 0.4775 | Val Loss: 0.4925 | Time: 1.27s\r\n",
      "Epoch 002 | Train Loss: 0.4734 | Val Loss: 0.4385 | Time: 1.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 003 | Train Loss: 0.4586 | Val Loss: 0.4386 | Time: 1.29s\r\n",
      "Epoch 004 | Train Loss: 0.4505 | Val Loss: 0.4298 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4298)\r\n",
      "Epoch 005 | Train Loss: 0.4572 | Val Loss: 0.4489 | Time: 1.29s\r\n",
      "Epoch 006 | Train Loss: 0.4532 | Val Loss: 0.4381 | Time: 1.29s\r\n",
      "Epoch 007 | Train Loss: 0.4479 | Val Loss: 0.4386 | Time: 1.29s\r\n",
      "Epoch 008 | Train Loss: 0.4423 | Val Loss: 0.4272 | Time: 1.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4272)\r\n",
      "Epoch 009 | Train Loss: 0.4386 | Val Loss: 0.4198 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4198)\r\n",
      "Epoch 010 | Train Loss: 0.4387 | Val Loss: 0.4188 | Time: 1.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 011 | Train Loss: 0.4424 | Val Loss: 0.4298 | Time: 1.30s\r\n",
      "Epoch 012 | Train Loss: 0.4578 | Val Loss: 0.4402 | Time: 1.28s\r\n",
      "Epoch 013 | Train Loss: 0.4704 | Val Loss: 0.4309 | Time: 1.27s\r\n",
      "Epoch 014 | Train Loss: 0.4574 | Val Loss: 0.4762 | Time: 1.27s\r\n",
      "Epoch 015 | Train Loss: 0.4626 | Val Loss: 0.4329 | Time: 1.31s\r\n",
      "Epoch 016 | Train Loss: 0.4413 | Val Loss: 0.4275 | Time: 1.29s\r\n",
      "Epoch 017 | Train Loss: 0.4376 | Val Loss: 0.4172 | Time: 1.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4172)\r\n",
      "Epoch 018 | Train Loss: 0.4385 | Val Loss: 0.4235 | Time: 1.29s\r\n",
      "Epoch 019 | Train Loss: 0.4429 | Val Loss: 0.4251 | Time: 1.30s\r\n",
      "Epoch 020 | Train Loss: 0.4379 | Val Loss: 0.4485 | Time: 1.28s\r\n",
      "Epoch 021 | Train Loss: 0.4428 | Val Loss: 0.4265 | Time: 1.28s\r\n",
      "Epoch 022 | Train Loss: 0.4440 | Val Loss: 0.4170 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 023 | Train Loss: 0.4374 | Val Loss: 0.4245 | Time: 1.32s\r\n",
      "Epoch 024 | Train Loss: 0.4396 | Val Loss: 0.4237 | Time: 1.27s\r\n",
      "Epoch 025 | Train Loss: 0.4424 | Val Loss: 0.4252 | Time: 1.28s\r\n",
      "Epoch 026 | Train Loss: 0.4502 | Val Loss: 0.4408 | Time: 1.36s\r\n",
      "Epoch 027 | Train Loss: 0.4463 | Val Loss: 0.4169 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4169)\r\n",
      "Epoch 028 | Train Loss: 0.4462 | Val Loss: 0.4306 | Time: 1.28s\r\n",
      "Epoch 029 | Train Loss: 0.4433 | Val Loss: 0.4280 | Time: 1.27s\r\n",
      "Epoch 030 | Train Loss: 0.4562 | Val Loss: 0.4363 | Time: 1.27s\r\n",
      "Epoch 031 | Train Loss: 0.4595 | Val Loss: 0.4309 | Time: 1.31s\r\n",
      "Epoch 032 | Train Loss: 0.4502 | Val Loss: 0.4360 | Time: 1.28s\r\n",
      "Epoch 033 | Train Loss: 0.4633 | Val Loss: 0.4445 | Time: 1.28s\r\n",
      "Epoch 034 | Train Loss: 0.4525 | Val Loss: 0.4189 | Time: 1.28s\r\n",
      "Epoch 035 | Train Loss: 0.4670 | Val Loss: 0.4264 | Time: 1.34s\r\n",
      "Epoch 036 | Train Loss: 0.4597 | Val Loss: 0.4408 | Time: 1.29s\r\n",
      "Epoch 037 | Train Loss: 0.4638 | Val Loss: 0.4420 | Time: 1.29s\r\n",
      "Epoch 038 | Train Loss: 0.4541 | Val Loss: 0.4354 | Time: 1.29s\r\n",
      "Epoch 039 | Train Loss: 0.4556 | Val Loss: 0.4377 | Time: 1.31s\r\n",
      "Epoch 040 | Train Loss: 0.4523 | Val Loss: 0.4425 | Time: 1.28s\r\n",
      "Epoch 041 | Train Loss: 0.4557 | Val Loss: 0.4362 | Time: 1.29s\r\n",
      "Epoch 042 | Train Loss: 0.4510 | Val Loss: 0.4724 | Time: 1.29s\r\n",
      "Epoch 043 | Train Loss: 0.4582 | Val Loss: 0.4221 | Time: 1.29s\r\n",
      "Epoch 044 | Train Loss: 0.4603 | Val Loss: 0.4307 | Time: 1.29s\r\n",
      "Epoch 045 | Train Loss: 0.4708 | Val Loss: 0.4427 | Time: 1.29s\r\n",
      "Epoch 046 | Train Loss: 0.4773 | Val Loss: 0.5065 | Time: 1.33s\r\n",
      "Epoch 047 | Train Loss: 0.4797 | Val Loss: 0.4501 | Time: 1.28s\r\n",
      "Epoch 048 | Train Loss: 0.4564 | Val Loss: 0.4348 | Time: 1.28s\r\n",
      "Epoch 049 | Train Loss: 0.4602 | Val Loss: 0.4333 | Time: 1.28s\r\n",
      "✅ Meilleur modèle chargé (époque 27, val_loss: 0.4169)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.2670\r\n",
      "   2. Partner             : 0.0846\r\n",
      "   3. OnlineSecurity      : 0.0814\r\n",
      "   4. SeniorCitizen       : 0.0804\r\n",
      "   5. MultipleLines       : 0.0487\r\n",
      "   6. StreamingMovies     : 0.0452\r\n",
      "   7. TechSupport         : 0.0450\r\n",
      "   8. MonthlyCharges      : 0.0431\r\n",
      "   9. Dependents          : 0.0378\r\n",
      "  10. tenure              : 0.0356\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 7 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. gender               (CAT): 0.2670\r\n",
      "   2. Partner              (CAT): 0.0846\r\n",
      "   3. OnlineSecurity       (CAT): 0.0814\r\n",
      "   4. SeniorCitizen        (CAT): 0.0804\r\n",
      "   5. MultipleLines        (CAT): 0.0487\r\n",
      "   6. StreamingMovies      (CAT): 0.0452\r\n",
      "   7. TechSupport          (CAT): 0.0450\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['gender', 'Partner', 'OnlineSecurity', 'SeniorCitizen', 'MultipleLines', 'StreamingMovies', 'TechSupport'] → indices [0, 2, 7, 1, 5, 12, 10]\r\n",
      "📊 Features sélectionnées: 0 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 84,273 paramètres\r\n",
      "🔗 Sparsité d'attention: 53.12%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6198 | Val Loss: 0.5860 | Time: 2.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5860)\r\n",
      "Epoch 001 | Train Loss: 0.5905 | Val Loss: 0.5759 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5759)\r\n",
      "Epoch 002 | Train Loss: 0.5793 | Val Loss: 0.5711 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5711)\r\n",
      "Epoch 003 | Train Loss: 0.5756 | Val Loss: 0.5652 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5652)\r\n",
      "Epoch 004 | Train Loss: 0.5702 | Val Loss: 0.5566 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5566)\r\n",
      "Epoch 005 | Train Loss: 0.5640 | Val Loss: 0.5411 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5411)\r\n",
      "Epoch 006 | Train Loss: 0.5557 | Val Loss: 0.5235 | Time: 2.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5235)\r\n",
      "Epoch 007 | Train Loss: 0.5407 | Val Loss: 0.5070 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5070)\r\n",
      "Epoch 008 | Train Loss: 0.5303 | Val Loss: 0.4999 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4999)\r\n",
      "Epoch 009 | Train Loss: 0.5161 | Val Loss: 0.4960 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4960)\r\n",
      "Epoch 010 | Train Loss: 0.5143 | Val Loss: 0.4954 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4954)\r\n",
      "Epoch 011 | Train Loss: 0.5093 | Val Loss: 0.4973 | Time: 2.37s\r\n",
      "Epoch 012 | Train Loss: 0.5127 | Val Loss: 0.4960 | Time: 2.45s\r\n",
      "Epoch 013 | Train Loss: 0.5111 | Val Loss: 0.4961 | Time: 2.45s\r\n",
      "Epoch 014 | Train Loss: 0.5062 | Val Loss: 0.4958 | Time: 2.42s\r\n",
      "Epoch 015 | Train Loss: 0.5067 | Val Loss: 0.4956 | Time: 2.41s\r\n",
      "Epoch 016 | Train Loss: 0.5043 | Val Loss: 0.4957 | Time: 2.39s\r\n",
      "Epoch 017 | Train Loss: 0.5070 | Val Loss: 0.4950 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4950)\r\n",
      "Epoch 018 | Train Loss: 0.5028 | Val Loss: 0.4948 | Time: 2.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4948)\r\n",
      "Epoch 019 | Train Loss: 0.5013 | Val Loss: 0.4941 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4941)\r\n",
      "Epoch 020 | Train Loss: 0.5020 | Val Loss: 0.4943 | Time: 2.40s\r\n",
      "Epoch 021 | Train Loss: 0.5001 | Val Loss: 0.4938 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4938)\r\n",
      "Epoch 022 | Train Loss: 0.5021 | Val Loss: 0.4939 | Time: 2.41s\r\n",
      "Epoch 023 | Train Loss: 0.5010 | Val Loss: 0.4938 | Time: 2.37s\r\n",
      "Epoch 024 | Train Loss: 0.5013 | Val Loss: 0.4939 | Time: 2.39s\r\n",
      "Epoch 025 | Train Loss: 0.5005 | Val Loss: 0.4937 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4937)\r\n",
      "Epoch 026 | Train Loss: 0.4993 | Val Loss: 0.4940 | Time: 2.63s\r\n",
      "Epoch 027 | Train Loss: 0.5013 | Val Loss: 0.4940 | Time: 2.38s\r\n",
      "Epoch 028 | Train Loss: 0.4979 | Val Loss: 0.4941 | Time: 2.40s\r\n",
      "Epoch 029 | Train Loss: 0.4973 | Val Loss: 0.4938 | Time: 2.40s\r\n",
      "Epoch 030 | Train Loss: 0.4999 | Val Loss: 0.4941 | Time: 2.39s\r\n",
      "Epoch 031 | Train Loss: 0.4994 | Val Loss: 0.4939 | Time: 2.45s\r\n",
      "Epoch 032 | Train Loss: 0.5012 | Val Loss: 0.4943 | Time: 2.37s\r\n",
      "Epoch 033 | Train Loss: 0.4991 | Val Loss: 0.4938 | Time: 2.35s\r\n",
      "Epoch 034 | Train Loss: 0.4998 | Val Loss: 0.4935 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4935)\r\n",
      "Epoch 035 | Train Loss: 0.4984 | Val Loss: 0.4934 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4934)\r\n",
      "Epoch 036 | Train Loss: 0.4981 | Val Loss: 0.4931 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4931)\r\n",
      "Epoch 037 | Train Loss: 0.4984 | Val Loss: 0.4934 | Time: 2.38s\r\n",
      "Epoch 038 | Train Loss: 0.4974 | Val Loss: 0.4937 | Time: 2.38s\r\n",
      "Epoch 039 | Train Loss: 0.4967 | Val Loss: 0.4940 | Time: 2.52s\r\n",
      "Epoch 040 | Train Loss: 0.4981 | Val Loss: 0.4938 | Time: 2.38s\r\n",
      "Epoch 041 | Train Loss: 0.4969 | Val Loss: 0.4941 | Time: 2.38s\r\n",
      "Epoch 042 | Train Loss: 0.4981 | Val Loss: 0.4944 | Time: 2.38s\r\n",
      "Epoch 043 | Train Loss: 0.4975 | Val Loss: 0.4949 | Time: 2.41s\r\n",
      "Epoch 044 | Train Loss: 0.4943 | Val Loss: 0.4954 | Time: 2.38s\r\n",
      "Epoch 045 | Train Loss: 0.4985 | Val Loss: 0.4954 | Time: 2.44s\r\n",
      "Epoch 046 | Train Loss: 0.4987 | Val Loss: 0.4949 | Time: 2.41s\r\n",
      "Epoch 047 | Train Loss: 0.4985 | Val Loss: 0.4947 | Time: 2.43s\r\n",
      "Epoch 048 | Train Loss: 0.4987 | Val Loss: 0.4948 | Time: 2.38s\r\n",
      "Epoch 049 | Train Loss: 0.4983 | Val Loss: 0.4949 | Time: 2.37s\r\n",
      "✅ Meilleur modèle Random chargé (époque 36, val_loss: 0.4931)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. gender               (CAT): 0.1600\r\n",
      "   2. MultipleLines        (CAT): 0.1576\r\n",
      "   3. TechSupport          (CAT): 0.1497\r\n",
      "   4. SeniorCitizen        (CAT): 0.1429\r\n",
      "   5. Partner              (CAT): 0.1325\r\n",
      "   6. OnlineSecurity       (CAT): 0.1303\r\n",
      "   7. StreamingMovies      (CAT): 0.1269\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.1600\r\n",
      "   2. MultipleLines       : 0.1576\r\n",
      "   3. TechSupport         : 0.1497\r\n",
      "   4. SeniorCitizen       : 0.1429\r\n",
      "   5. Partner             : 0.1325\r\n",
      "   6. OnlineSecurity      : 0.1303\r\n",
      "   7. StreamingMovies     : 0.1269\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_8/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 187.9s ===\r\n",
      "\u001b[32m[I 2025-07-19 20:54:42,888]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 5, 'n_heads_stage1': 4, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.27168255036860234, 'ffn_dropout_stage1': 0.18579880547500366, 'residual_dropout_stage1': 0.17508710677914974, 'lr_stage1': 0.01042729106097903, 'weight_decay_stage1': 3.278078460674785e-06, 'd_token_stage2': 16, 'n_blocks_stage2': 6, 'n_heads_stage2': 8, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.21468757762465723, 'ffn_dropout_stage2': 0.22636744243395984, 'residual_dropout_stage2': 0.144844552197832, 'lr_stage2': 0.00014888230597983555, 'weight_decay_stage2': 4.398683628529417e-05, 'batch_size': 128, 'patience': 27, 'embedding_type': 'T-LR', 'M': 7, 'k': 8}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  36%|███▏     | 9/25 [2:41:45<3:53:03, 873.96s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 196,993 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5679 | Val Loss: 0.5861 | Time: 0.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5861)\r\n",
      "Epoch 001 | Train Loss: 0.5444 | Val Loss: 0.6218 | Time: 0.84s\r\n",
      "Epoch 002 | Train Loss: 0.5839 | Val Loss: 0.6043 | Time: 0.86s\r\n",
      "Epoch 003 | Train Loss: 0.5797 | Val Loss: 0.5989 | Time: 0.85s\r\n",
      "Epoch 004 | Train Loss: 0.5713 | Val Loss: 0.6125 | Time: 0.83s\r\n",
      "Epoch 005 | Train Loss: 0.5731 | Val Loss: 0.5660 | Time: 0.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5660)\r\n",
      "Epoch 006 | Train Loss: 0.5702 | Val Loss: 0.5819 | Time: 0.91s\r\n",
      "Epoch 007 | Train Loss: 0.5703 | Val Loss: 0.5661 | Time: 0.85s\r\n",
      "Epoch 008 | Train Loss: 0.5676 | Val Loss: 0.5695 | Time: 0.84s\r\n",
      "Epoch 009 | Train Loss: 0.5699 | Val Loss: 0.5747 | Time: 0.84s\r\n",
      "Epoch 010 | Train Loss: 0.5685 | Val Loss: 0.5648 | Time: 0.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5648)\r\n",
      "Epoch 011 | Train Loss: 0.5684 | Val Loss: 0.5654 | Time: 0.84s\r\n",
      "Epoch 012 | Train Loss: 0.5674 | Val Loss: 0.5606 | Time: 0.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5606)\r\n",
      "Epoch 013 | Train Loss: 0.5700 | Val Loss: 0.5616 | Time: 0.84s\r\n",
      "Epoch 014 | Train Loss: 0.5674 | Val Loss: 0.5608 | Time: 0.87s\r\n",
      "Epoch 015 | Train Loss: 0.5693 | Val Loss: 0.5599 | Time: 0.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5599)\r\n",
      "Epoch 016 | Train Loss: 0.5674 | Val Loss: 0.5593 | Time: 0.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5593)\r\n",
      "Epoch 017 | Train Loss: 0.5704 | Val Loss: 0.5613 | Time: 0.84s\r\n",
      "Epoch 018 | Train Loss: 0.5692 | Val Loss: 0.5614 | Time: 0.84s\r\n",
      "Epoch 019 | Train Loss: 0.5697 | Val Loss: 0.5647 | Time: 0.84s\r\n",
      "Epoch 020 | Train Loss: 0.5683 | Val Loss: 0.5626 | Time: 0.83s\r\n",
      "Epoch 021 | Train Loss: 0.5689 | Val Loss: 0.5617 | Time: 0.83s\r\n",
      "Epoch 022 | Train Loss: 0.5664 | Val Loss: 0.5597 | Time: 0.83s\r\n",
      "Epoch 023 | Train Loss: 0.5663 | Val Loss: 0.5607 | Time: 0.83s\r\n",
      "Epoch 024 | Train Loss: 0.5670 | Val Loss: 0.5613 | Time: 0.82s\r\n",
      "Epoch 025 | Train Loss: 0.5689 | Val Loss: 0.5622 | Time: 0.83s\r\n",
      "Epoch 026 | Train Loss: 0.5743 | Val Loss: 0.5812 | Time: 0.86s\r\n",
      "Epoch 027 | Train Loss: 0.5703 | Val Loss: 0.5621 | Time: 0.85s\r\n",
      "Epoch 028 | Train Loss: 0.5673 | Val Loss: 0.5673 | Time: 0.85s\r\n",
      "Epoch 029 | Train Loss: 0.5688 | Val Loss: 0.5606 | Time: 0.85s\r\n",
      "Epoch 030 | Train Loss: 0.5696 | Val Loss: 0.5662 | Time: 0.84s\r\n",
      "Epoch 031 | Train Loss: 0.5659 | Val Loss: 0.5607 | Time: 0.84s\r\n",
      "Epoch 032 | Train Loss: 0.5659 | Val Loss: 0.5611 | Time: 0.84s\r\n",
      "Epoch 033 | Train Loss: 0.5668 | Val Loss: 0.5615 | Time: 0.84s\r\n",
      "Epoch 034 | Train Loss: 0.5695 | Val Loss: 0.5611 | Time: 0.84s\r\n",
      "Epoch 035 | Train Loss: 0.5660 | Val Loss: 0.5619 | Time: 0.85s\r\n",
      "Epoch 036 | Train Loss: 0.5682 | Val Loss: 0.5635 | Time: 0.84s\r\n",
      "Epoch 037 | Train Loss: 0.5661 | Val Loss: 0.5607 | Time: 0.84s\r\n",
      "Epoch 038 | Train Loss: 0.5666 | Val Loss: 0.5607 | Time: 0.88s\r\n",
      "Epoch 039 | Train Loss: 0.5647 | Val Loss: 0.5611 | Time: 0.85s\r\n",
      "Epoch 040 | Train Loss: 0.5698 | Val Loss: 0.5648 | Time: 0.85s\r\n",
      "Epoch 041 | Train Loss: 0.5676 | Val Loss: 0.5640 | Time: 0.85s\r\n",
      "Epoch 042 | Train Loss: 0.5665 | Val Loss: 0.5628 | Time: 0.84s\r\n",
      "Epoch 043 | Train Loss: 0.5679 | Val Loss: 0.5648 | Time: 0.91s\r\n",
      "Epoch 044 | Train Loss: 0.5686 | Val Loss: 0.5659 | Time: 0.90s\r\n",
      "Epoch 045 | Train Loss: 0.5676 | Val Loss: 0.5627 | Time: 0.83s\r\n",
      "Epoch 046 | Train Loss: 0.5667 | Val Loss: 0.5608 | Time: 0.84s\r\n",
      "\r\n",
      "Early stopping à l'époque 46 (patience: 30)\r\n",
      "✅ Meilleur modèle chargé (époque 16, val_loss: 0.5593)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TechSupport         : 0.1424\r\n",
      "   2. tenure              : 0.1240\r\n",
      "   3. OnlineSecurity      : 0.0868\r\n",
      "   4. MultipleLines       : 0.0848\r\n",
      "   5. TotalCharges        : 0.0797\r\n",
      "   6. StreamingTV         : 0.0783\r\n",
      "   7. InternetService     : 0.0656\r\n",
      "   8. PaymentMethod       : 0.0602\r\n",
      "   9. Dependents          : 0.0540\r\n",
      "  10. DeviceProtection    : 0.0471\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 20 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. TechSupport          (CAT): 0.1424\r\n",
      "   2. tenure               (NUM): 0.1240\r\n",
      "   3. OnlineSecurity       (CAT): 0.0868\r\n",
      "   4. MultipleLines        (CAT): 0.0848\r\n",
      "   5. TotalCharges         (NUM): 0.0797\r\n",
      "   6. StreamingTV          (CAT): 0.0783\r\n",
      "   7. InternetService      (CAT): 0.0656\r\n",
      "   8. PaymentMethod        (CAT): 0.0602\r\n",
      "   9. Dependents           (CAT): 0.0540\r\n",
      "  10. DeviceProtection     (CAT): 0.0471\r\n",
      "  11. PaperlessBilling     (CAT): 0.0382\r\n",
      "  12. gender               (CAT): 0.0262\r\n",
      "  13. Partner              (CAT): 0.0230\r\n",
      "  14. OnlineBackup         (CAT): 0.0209\r\n",
      "  15. SeniorCitizen        (CAT): 0.0185\r\n",
      "  16. StreamingMovies      (CAT): 0.0161\r\n",
      "  17. PhoneService         (CAT): 0.0154\r\n",
      "  18. MonthlyCharges       (NUM): 0.0099\r\n",
      "  19. Contract             (CAT): 0.0089\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['TechSupport', 'OnlineSecurity', 'MultipleLines', 'StreamingTV', 'InternetService', 'PaymentMethod', 'Dependents', 'DeviceProtection', 'PaperlessBilling', 'gender', 'Partner', 'OnlineBackup', 'SeniorCitizen', 'StreamingMovies', 'PhoneService', 'Contract'] → indices [10, 7, 5, 11, 6, 15, 3, 9, 14, 0, 2, 8, 1, 12, 4, 13]\r\n",
      "📊 Features sélectionnées: 3 numériques, 16 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 103,777 paramètres\r\n",
      "🔗 Sparsité d'attention: 87.50%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5588 | Val Loss: 0.5262 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5262)\r\n",
      "Epoch 001 | Train Loss: 0.5587 | Val Loss: 0.5875 | Time: 4.68s\r\n",
      "Epoch 002 | Train Loss: 0.5548 | Val Loss: 0.5452 | Time: 4.72s\r\n",
      "Epoch 003 | Train Loss: 0.5513 | Val Loss: 0.5431 | Time: 4.71s\r\n",
      "Epoch 004 | Train Loss: 0.5383 | Val Loss: 0.5138 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5138)\r\n",
      "Epoch 005 | Train Loss: 0.5351 | Val Loss: 0.5027 | Time: 4.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5027)\r\n",
      "Epoch 006 | Train Loss: 0.5430 | Val Loss: 0.5196 | Time: 4.79s\r\n",
      "Epoch 007 | Train Loss: 0.5376 | Val Loss: 0.5072 | Time: 4.70s\r\n",
      "Epoch 008 | Train Loss: 0.5297 | Val Loss: 0.5020 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5020)\r\n",
      "Epoch 009 | Train Loss: 0.5284 | Val Loss: 0.5491 | Time: 4.72s\r\n",
      "Epoch 010 | Train Loss: 0.5580 | Val Loss: 0.5239 | Time: 4.74s\r\n",
      "Epoch 011 | Train Loss: 0.5397 | Val Loss: 0.5359 | Time: 4.71s\r\n",
      "Epoch 012 | Train Loss: 0.5392 | Val Loss: 0.5354 | Time: 4.80s\r\n",
      "Epoch 013 | Train Loss: 0.5357 | Val Loss: 0.5523 | Time: 4.69s\r\n",
      "Epoch 014 | Train Loss: 0.5392 | Val Loss: 0.5327 | Time: 4.68s\r\n",
      "Epoch 015 | Train Loss: 0.5391 | Val Loss: 0.5444 | Time: 4.74s\r\n",
      "Epoch 016 | Train Loss: 0.5358 | Val Loss: 0.5377 | Time: 4.72s\r\n",
      "Epoch 017 | Train Loss: 0.5354 | Val Loss: 0.5318 | Time: 4.73s\r\n",
      "Epoch 018 | Train Loss: 0.5311 | Val Loss: 0.5314 | Time: 4.69s\r\n",
      "Epoch 019 | Train Loss: 0.5284 | Val Loss: 0.5237 | Time: 4.78s\r\n",
      "Epoch 020 | Train Loss: 0.5232 | Val Loss: 0.5027 | Time: 4.65s\r\n",
      "Epoch 021 | Train Loss: 0.5218 | Val Loss: 0.5064 | Time: 4.67s\r\n",
      "Epoch 022 | Train Loss: 0.5222 | Val Loss: 0.4966 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4966)\r\n",
      "Epoch 023 | Train Loss: 0.5286 | Val Loss: 0.5060 | Time: 4.83s\r\n",
      "Epoch 024 | Train Loss: 0.5305 | Val Loss: 0.4901 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4901)\r\n",
      "Epoch 025 | Train Loss: 0.5167 | Val Loss: 0.4819 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4819)\r\n",
      "Epoch 026 | Train Loss: 0.5190 | Val Loss: 0.4890 | Time: 4.66s\r\n",
      "Epoch 027 | Train Loss: 0.5371 | Val Loss: 0.5196 | Time: 4.71s\r\n",
      "Epoch 028 | Train Loss: 0.5293 | Val Loss: 0.5140 | Time: 4.64s\r\n",
      "Epoch 029 | Train Loss: 0.5235 | Val Loss: 0.5151 | Time: 4.70s\r\n",
      "Epoch 030 | Train Loss: 0.5169 | Val Loss: 0.4969 | Time: 4.72s\r\n",
      "Epoch 031 | Train Loss: 0.5185 | Val Loss: 0.5142 | Time: 4.67s\r\n",
      "Epoch 032 | Train Loss: 0.5253 | Val Loss: 0.5017 | Time: 4.80s\r\n",
      "Epoch 033 | Train Loss: 0.5167 | Val Loss: 0.4870 | Time: 4.68s\r\n",
      "Epoch 034 | Train Loss: 0.5035 | Val Loss: 0.4929 | Time: 4.71s\r\n",
      "Epoch 035 | Train Loss: 0.5200 | Val Loss: 0.4879 | Time: 4.69s\r\n",
      "Epoch 036 | Train Loss: 0.5053 | Val Loss: 0.5035 | Time: 4.75s\r\n",
      "Epoch 037 | Train Loss: 0.5062 | Val Loss: 0.4722 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4722)\r\n",
      "Epoch 038 | Train Loss: 0.4972 | Val Loss: 0.4713 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4713)\r\n",
      "Epoch 039 | Train Loss: 0.5024 | Val Loss: 0.5488 | Time: 4.76s\r\n",
      "Epoch 040 | Train Loss: 0.5338 | Val Loss: 0.5195 | Time: 4.81s\r\n",
      "Epoch 041 | Train Loss: 0.5178 | Val Loss: 0.5042 | Time: 4.70s\r\n",
      "Epoch 042 | Train Loss: 0.5198 | Val Loss: 0.5029 | Time: 4.73s\r\n",
      "Epoch 043 | Train Loss: 0.5188 | Val Loss: 0.4772 | Time: 4.73s\r\n",
      "Epoch 044 | Train Loss: 0.5094 | Val Loss: 0.4629 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4629)\r\n",
      "Epoch 045 | Train Loss: 0.5097 | Val Loss: 0.4777 | Time: 4.69s\r\n",
      "Epoch 046 | Train Loss: 0.5142 | Val Loss: 0.4719 | Time: 4.81s\r\n",
      "Epoch 047 | Train Loss: 0.5066 | Val Loss: 0.4646 | Time: 4.69s\r\n",
      "Epoch 048 | Train Loss: 0.4964 | Val Loss: 0.4976 | Time: 4.73s\r\n",
      "Epoch 049 | Train Loss: 0.5039 | Val Loss: 0.4791 | Time: 4.80s\r\n",
      "✅ Meilleur modèle Random chargé (époque 44, val_loss: 0.4629)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. DeviceProtection     (CAT): 0.2325\r\n",
      "   2. tenure               (NUM): 0.1805\r\n",
      "   3. OnlineBackup         (CAT): 0.1213\r\n",
      "   4. StreamingMovies      (CAT): 0.1082\r\n",
      "   5. PaperlessBilling     (CAT): 0.1066\r\n",
      "   6. SeniorCitizen        (CAT): 0.1028\r\n",
      "   7. Contract             (CAT): 0.0551\r\n",
      "   8. MultipleLines        (CAT): 0.0489\r\n",
      "   9. PaymentMethod        (CAT): 0.0192\r\n",
      "  10. TechSupport          (CAT): 0.0060\r\n",
      "  11. StreamingTV          (CAT): 0.0060\r\n",
      "  12. InternetService      (CAT): 0.0053\r\n",
      "  13. gender               (CAT): 0.0050\r\n",
      "  14. Partner              (CAT): 0.0011\r\n",
      "  15. MonthlyCharges       (NUM): 0.0009\r\n",
      "  16. TotalCharges         (NUM): 0.0004\r\n",
      "  17. Dependents           (CAT): 0.0002\r\n",
      "  18. OnlineSecurity       (CAT): 0.0000\r\n",
      "  19. PhoneService         (CAT): 0.0000\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.2325\r\n",
      "   2. tenure              : 0.1805\r\n",
      "   3. OnlineBackup        : 0.1213\r\n",
      "   4. StreamingMovies     : 0.1082\r\n",
      "   5. PaperlessBilling    : 0.1066\r\n",
      "   6. SeniorCitizen       : 0.1028\r\n",
      "   7. Contract            : 0.0551\r\n",
      "   8. MultipleLines       : 0.0489\r\n",
      "   9. PaymentMethod       : 0.0192\r\n",
      "  10. TechSupport         : 0.0060\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 280.5s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 196,993 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5327 | Val Loss: 0.4922 | Time: 0.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4922)\r\n",
      "Epoch 001 | Train Loss: 0.5047 | Val Loss: 0.5072 | Time: 0.85s\r\n",
      "Epoch 002 | Train Loss: 0.5122 | Val Loss: 0.4871 | Time: 0.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4871)\r\n",
      "Epoch 003 | Train Loss: 0.4963 | Val Loss: 0.4763 | Time: 0.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4763)\r\n",
      "Epoch 004 | Train Loss: 0.4964 | Val Loss: 0.4826 | Time: 0.84s\r\n",
      "Epoch 005 | Train Loss: 0.5025 | Val Loss: 0.5013 | Time: 0.84s\r\n",
      "Epoch 006 | Train Loss: 0.5188 | Val Loss: 0.6067 | Time: 0.85s\r\n",
      "Epoch 007 | Train Loss: 0.5101 | Val Loss: 0.5320 | Time: 0.82s\r\n",
      "Epoch 008 | Train Loss: 0.5282 | Val Loss: 0.4778 | Time: 0.84s\r\n",
      "Epoch 009 | Train Loss: 0.5053 | Val Loss: 0.4995 | Time: 0.84s\r\n",
      "Epoch 010 | Train Loss: 0.5046 | Val Loss: 0.4967 | Time: 0.84s\r\n",
      "Epoch 011 | Train Loss: 0.5090 | Val Loss: 0.4919 | Time: 0.94s\r\n",
      "Epoch 012 | Train Loss: 0.5064 | Val Loss: 0.4936 | Time: 0.83s\r\n",
      "Epoch 013 | Train Loss: 0.5049 | Val Loss: 0.4904 | Time: 0.83s\r\n",
      "Epoch 014 | Train Loss: 0.5028 | Val Loss: 0.4944 | Time: 0.87s\r\n",
      "Epoch 015 | Train Loss: 0.4999 | Val Loss: 0.4886 | Time: 0.85s\r\n",
      "Epoch 016 | Train Loss: 0.4977 | Val Loss: 0.4852 | Time: 0.85s\r\n",
      "Epoch 017 | Train Loss: 0.4899 | Val Loss: 0.4971 | Time: 0.85s\r\n",
      "Epoch 018 | Train Loss: 0.5006 | Val Loss: 0.4950 | Time: 0.85s\r\n",
      "Epoch 019 | Train Loss: 0.5017 | Val Loss: 0.4919 | Time: 0.84s\r\n",
      "Epoch 020 | Train Loss: 0.5044 | Val Loss: 0.5089 | Time: 0.83s\r\n",
      "Epoch 021 | Train Loss: 0.5030 | Val Loss: 0.4766 | Time: 0.83s\r\n",
      "Epoch 022 | Train Loss: 0.5062 | Val Loss: 0.5225 | Time: 0.84s\r\n",
      "Epoch 023 | Train Loss: 0.5371 | Val Loss: 0.5328 | Time: 0.85s\r\n",
      "Epoch 024 | Train Loss: 0.5370 | Val Loss: 0.5214 | Time: 0.85s\r\n",
      "Epoch 025 | Train Loss: 0.5065 | Val Loss: 0.4970 | Time: 0.88s\r\n",
      "Epoch 026 | Train Loss: 0.4955 | Val Loss: 0.4859 | Time: 0.85s\r\n",
      "Epoch 027 | Train Loss: 0.5061 | Val Loss: 0.4847 | Time: 0.85s\r\n",
      "Epoch 028 | Train Loss: 0.5094 | Val Loss: 0.4871 | Time: 0.83s\r\n",
      "Epoch 029 | Train Loss: 0.5237 | Val Loss: 0.4823 | Time: 0.83s\r\n",
      "Epoch 030 | Train Loss: 0.5375 | Val Loss: 0.5778 | Time: 0.85s\r\n",
      "Epoch 031 | Train Loss: 0.5518 | Val Loss: 0.5158 | Time: 0.84s\r\n",
      "Epoch 032 | Train Loss: 0.5192 | Val Loss: 0.4872 | Time: 0.83s\r\n",
      "Epoch 033 | Train Loss: 0.5123 | Val Loss: 0.4876 | Time: 0.84s\r\n",
      "\r\n",
      "Early stopping à l'époque 33 (patience: 30)\r\n",
      "✅ Meilleur modèle chargé (époque 3, val_loss: 0.4763)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.2617\r\n",
      "   2. PaymentMethod       : 0.1728\r\n",
      "   3. MonthlyCharges      : 0.1669\r\n",
      "   4. StreamingMovies     : 0.0809\r\n",
      "   5. StreamingTV         : 0.0784\r\n",
      "   6. Contract            : 0.0452\r\n",
      "   7. TotalCharges        : 0.0344\r\n",
      "   8. InternetService     : 0.0289\r\n",
      "   9. Dependents          : 0.0268\r\n",
      "  10. Partner             : 0.0172\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 20 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. gender               (CAT): 0.2617\r\n",
      "   2. PaymentMethod        (CAT): 0.1728\r\n",
      "   3. MonthlyCharges       (NUM): 0.1669\r\n",
      "   4. StreamingMovies      (CAT): 0.0809\r\n",
      "   5. StreamingTV          (CAT): 0.0784\r\n",
      "   6. Contract             (CAT): 0.0452\r\n",
      "   7. TotalCharges         (NUM): 0.0344\r\n",
      "   8. InternetService      (CAT): 0.0289\r\n",
      "   9. Dependents           (CAT): 0.0268\r\n",
      "  10. Partner              (CAT): 0.0172\r\n",
      "  11. TechSupport          (CAT): 0.0112\r\n",
      "  12. PaperlessBilling     (CAT): 0.0109\r\n",
      "  13. SeniorCitizen        (CAT): 0.0101\r\n",
      "  14. OnlineSecurity       (CAT): 0.0101\r\n",
      "  15. DeviceProtection     (CAT): 0.0095\r\n",
      "  16. PhoneService         (CAT): 0.0092\r\n",
      "  17. OnlineBackup         (CAT): 0.0092\r\n",
      "  18. tenure               (NUM): 0.0086\r\n",
      "  19. MultipleLines        (CAT): 0.0082\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges', 'tenure'] → indices [1, 2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['gender', 'PaymentMethod', 'StreamingMovies', 'StreamingTV', 'Contract', 'InternetService', 'Dependents', 'Partner', 'TechSupport', 'PaperlessBilling', 'SeniorCitizen', 'OnlineSecurity', 'DeviceProtection', 'PhoneService', 'OnlineBackup', 'MultipleLines'] → indices [0, 15, 12, 11, 13, 6, 3, 2, 10, 14, 1, 7, 9, 4, 8, 5]\r\n",
      "📊 Features sélectionnées: 3 numériques, 16 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 103,777 paramètres\r\n",
      "🔗 Sparsité d'attention: 87.50%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5868 | Val Loss: 0.5773 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5773)\r\n",
      "Epoch 001 | Train Loss: 0.5802 | Val Loss: 0.5757 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5757)\r\n",
      "Epoch 002 | Train Loss: 0.5712 | Val Loss: 0.5701 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5701)\r\n",
      "Epoch 003 | Train Loss: 0.5697 | Val Loss: 0.5632 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5632)\r\n",
      "Epoch 004 | Train Loss: 0.5712 | Val Loss: 0.5580 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5580)\r\n",
      "Epoch 005 | Train Loss: 0.5707 | Val Loss: 0.5791 | Time: 4.65s\r\n",
      "Epoch 006 | Train Loss: 0.5702 | Val Loss: 0.5908 | Time: 4.71s\r\n",
      "Epoch 007 | Train Loss: 0.5750 | Val Loss: 0.5718 | Time: 4.65s\r\n",
      "Epoch 008 | Train Loss: 0.5699 | Val Loss: 0.5555 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5555)\r\n",
      "Epoch 009 | Train Loss: 0.5576 | Val Loss: 0.5274 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5274)\r\n",
      "Epoch 010 | Train Loss: 0.5538 | Val Loss: 0.5332 | Time: 4.71s\r\n",
      "Epoch 011 | Train Loss: 0.5576 | Val Loss: 0.5890 | Time: 4.72s\r\n",
      "Epoch 012 | Train Loss: 0.5806 | Val Loss: 0.5791 | Time: 4.69s\r\n",
      "Epoch 013 | Train Loss: 0.5803 | Val Loss: 0.5788 | Time: 4.78s\r\n",
      "Epoch 014 | Train Loss: 0.5801 | Val Loss: 0.5785 | Time: 4.71s\r\n",
      "Epoch 015 | Train Loss: 0.5800 | Val Loss: 0.5784 | Time: 4.82s\r\n",
      "Epoch 016 | Train Loss: 0.5799 | Val Loss: 0.5783 | Time: 4.76s\r\n",
      "Epoch 017 | Train Loss: 0.5799 | Val Loss: 0.5783 | Time: 4.80s\r\n",
      "Epoch 018 | Train Loss: 0.5799 | Val Loss: 0.5783 | Time: 4.72s\r\n",
      "Epoch 019 | Train Loss: 0.5799 | Val Loss: 0.5783 | Time: 4.74s\r\n",
      "Epoch 020 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.69s\r\n",
      "Epoch 021 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.77s\r\n",
      "Epoch 022 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.79s\r\n",
      "Epoch 023 | Train Loss: 0.5800 | Val Loss: 0.5784 | Time: 4.73s\r\n",
      "Epoch 024 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.69s\r\n",
      "Epoch 025 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.71s\r\n",
      "Epoch 026 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.70s\r\n",
      "Epoch 027 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.70s\r\n",
      "Epoch 028 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.69s\r\n",
      "Epoch 029 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.79s\r\n",
      "Epoch 030 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.75s\r\n",
      "Epoch 031 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.70s\r\n",
      "Epoch 032 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.69s\r\n",
      "Epoch 033 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.64s\r\n",
      "Epoch 034 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.81s\r\n",
      "Epoch 035 | Train Loss: 0.5799 | Val Loss: 0.5784 | Time: 4.74s\r\n",
      "Epoch 036 | Train Loss: 0.5800 | Val Loss: 0.5784 | Time: 4.75s\r\n",
      "Epoch 037 | Train Loss: 0.5800 | Val Loss: 0.5784 | Time: 4.67s\r\n",
      "Epoch 038 | Train Loss: 0.5800 | Val Loss: 0.5784 | Time: 4.71s\r\n",
      "Epoch 039 | Train Loss: 0.5800 | Val Loss: 0.5784 | Time: 4.70s\r\n",
      "\r\n",
      "Early stopping à l'époque 39 (patience: 30)\r\n",
      "✅ Meilleur modèle Random chargé (époque 9, val_loss: 0.5274)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingTV          (CAT): 0.0557\r\n",
      "   2. PhoneService         (CAT): 0.0550\r\n",
      "   3. DeviceProtection     (CAT): 0.0545\r\n",
      "   4. StreamingMovies      (CAT): 0.0544\r\n",
      "   5. InternetService      (CAT): 0.0542\r\n",
      "   6. OnlineBackup         (CAT): 0.0534\r\n",
      "   7. PaymentMethod        (CAT): 0.0527\r\n",
      "   8. MultipleLines        (CAT): 0.0517\r\n",
      "   9. SeniorCitizen        (CAT): 0.0517\r\n",
      "  10. Contract             (CAT): 0.0517\r\n",
      "  11. OnlineSecurity       (CAT): 0.0517\r\n",
      "  12. MonthlyCharges       (NUM): 0.0517\r\n",
      "  13. TotalCharges         (NUM): 0.0517\r\n",
      "  14. TechSupport          (CAT): 0.0517\r\n",
      "  15. gender               (CAT): 0.0517\r\n",
      "  16. Partner              (CAT): 0.0517\r\n",
      "  17. PaperlessBilling     (CAT): 0.0517\r\n",
      "  18. tenure               (NUM): 0.0516\r\n",
      "  19. Dependents           (CAT): 0.0516\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingTV         : 0.0557\r\n",
      "   2. PhoneService        : 0.0550\r\n",
      "   3. DeviceProtection    : 0.0545\r\n",
      "   4. StreamingMovies     : 0.0544\r\n",
      "   5. InternetService     : 0.0542\r\n",
      "   6. OnlineBackup        : 0.0534\r\n",
      "   7. PaymentMethod       : 0.0527\r\n",
      "   8. MultipleLines       : 0.0517\r\n",
      "   9. SeniorCitizen       : 0.0517\r\n",
      "  10. Contract            : 0.0517\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 221.9s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 196,993 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5414 | Val Loss: 0.5442 | Time: 0.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5442)\r\n",
      "Epoch 001 | Train Loss: 0.5172 | Val Loss: 0.5378 | Time: 0.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5378)\r\n",
      "Epoch 002 | Train Loss: 0.5521 | Val Loss: 0.5301 | Time: 0.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5301)\r\n",
      "Epoch 003 | Train Loss: 0.5532 | Val Loss: 0.5472 | Time: 0.85s\r\n",
      "Epoch 004 | Train Loss: 0.5486 | Val Loss: 0.5265 | Time: 0.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5265)\r\n",
      "Epoch 005 | Train Loss: 0.5586 | Val Loss: 0.5504 | Time: 0.84s\r\n",
      "Epoch 006 | Train Loss: 0.5513 | Val Loss: 0.5501 | Time: 0.83s\r\n",
      "Epoch 007 | Train Loss: 0.5434 | Val Loss: 0.5233 | Time: 0.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5233)\r\n",
      "Epoch 008 | Train Loss: 0.5379 | Val Loss: 0.5374 | Time: 0.84s\r\n",
      "Epoch 009 | Train Loss: 0.5431 | Val Loss: 0.5327 | Time: 0.83s\r\n",
      "Epoch 010 | Train Loss: 0.5483 | Val Loss: 0.5291 | Time: 0.88s\r\n",
      "Epoch 011 | Train Loss: 0.5496 | Val Loss: 0.5293 | Time: 0.95s\r\n",
      "Epoch 012 | Train Loss: 0.5516 | Val Loss: 0.5847 | Time: 0.84s\r\n",
      "Epoch 013 | Train Loss: 0.5636 | Val Loss: 0.5738 | Time: 0.86s\r\n",
      "Epoch 014 | Train Loss: 0.5527 | Val Loss: 0.5535 | Time: 0.89s\r\n",
      "Epoch 015 | Train Loss: 0.5507 | Val Loss: 0.5295 | Time: 0.83s\r\n",
      "Epoch 016 | Train Loss: 0.5467 | Val Loss: 0.5277 | Time: 0.83s\r\n",
      "Epoch 017 | Train Loss: 0.5409 | Val Loss: 0.5214 | Time: 0.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5214)\r\n",
      "Epoch 018 | Train Loss: 0.5378 | Val Loss: 0.5164 | Time: 0.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5164)\r\n",
      "Epoch 019 | Train Loss: 0.5405 | Val Loss: 0.5183 | Time: 0.84s\r\n",
      "Epoch 020 | Train Loss: 0.5533 | Val Loss: 0.5539 | Time: 0.85s\r\n",
      "Epoch 021 | Train Loss: 0.5600 | Val Loss: 0.5603 | Time: 0.83s\r\n",
      "Epoch 022 | Train Loss: 0.5474 | Val Loss: 0.5318 | Time: 0.82s\r\n",
      "Epoch 023 | Train Loss: 0.5581 | Val Loss: 0.5748 | Time: 0.87s\r\n",
      "Epoch 024 | Train Loss: 0.5628 | Val Loss: 0.5458 | Time: 0.84s\r\n",
      "Epoch 025 | Train Loss: 0.5650 | Val Loss: 0.5607 | Time: 0.84s\r\n",
      "Epoch 026 | Train Loss: 0.5659 | Val Loss: 0.5496 | Time: 0.84s\r\n",
      "Epoch 027 | Train Loss: 0.5593 | Val Loss: 0.5514 | Time: 0.82s\r\n",
      "Epoch 028 | Train Loss: 0.5469 | Val Loss: 0.5329 | Time: 0.83s\r\n",
      "Epoch 029 | Train Loss: 0.5558 | Val Loss: 0.5526 | Time: 0.83s\r\n",
      "Epoch 030 | Train Loss: 0.5746 | Val Loss: 0.5726 | Time: 0.83s\r\n",
      "Epoch 031 | Train Loss: 0.5641 | Val Loss: 0.5480 | Time: 0.83s\r\n",
      "Epoch 032 | Train Loss: 0.5536 | Val Loss: 0.5481 | Time: 0.83s\r\n",
      "Epoch 033 | Train Loss: 0.5467 | Val Loss: 0.5414 | Time: 0.82s\r\n",
      "Epoch 034 | Train Loss: 0.5433 | Val Loss: 0.5357 | Time: 0.83s\r\n",
      "Epoch 035 | Train Loss: 0.5376 | Val Loss: 0.5465 | Time: 0.88s\r\n",
      "Epoch 036 | Train Loss: 0.5350 | Val Loss: 0.5458 | Time: 0.85s\r\n",
      "Epoch 037 | Train Loss: 0.5355 | Val Loss: 0.5267 | Time: 0.83s\r\n",
      "Epoch 038 | Train Loss: 0.5364 | Val Loss: 0.5302 | Time: 0.83s\r\n",
      "Epoch 039 | Train Loss: 0.5312 | Val Loss: 0.5338 | Time: 0.83s\r\n",
      "Epoch 040 | Train Loss: 0.5320 | Val Loss: 0.5282 | Time: 0.84s\r\n",
      "Epoch 041 | Train Loss: 0.5339 | Val Loss: 0.5382 | Time: 0.84s\r\n",
      "Epoch 042 | Train Loss: 0.5331 | Val Loss: 0.5307 | Time: 0.85s\r\n",
      "Epoch 043 | Train Loss: 0.5356 | Val Loss: 0.5298 | Time: 0.85s\r\n",
      "Epoch 044 | Train Loss: 0.5340 | Val Loss: 0.5285 | Time: 0.85s\r\n",
      "Epoch 045 | Train Loss: 0.5364 | Val Loss: 0.5271 | Time: 0.83s\r\n",
      "Epoch 046 | Train Loss: 0.5411 | Val Loss: 0.5264 | Time: 0.82s\r\n",
      "Epoch 047 | Train Loss: 0.5403 | Val Loss: 0.5239 | Time: 0.88s\r\n",
      "Epoch 048 | Train Loss: 0.5378 | Val Loss: 0.5251 | Time: 0.94s\r\n",
      "\r\n",
      "Early stopping à l'époque 48 (patience: 30)\r\n",
      "✅ Meilleur modèle chargé (époque 18, val_loss: 0.5164)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0953\r\n",
      "   2. MultipleLines       : 0.0926\r\n",
      "   3. OnlineBackup        : 0.0842\r\n",
      "   4. Contract            : 0.0806\r\n",
      "   5. TechSupport         : 0.0593\r\n",
      "   6. StreamingTV         : 0.0579\r\n",
      "   7. tenure              : 0.0566\r\n",
      "   8. StreamingMovies     : 0.0495\r\n",
      "   9. PaymentMethod       : 0.0480\r\n",
      "  10. PhoneService        : 0.0477\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 20 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0953\r\n",
      "   2. MultipleLines        (CAT): 0.0926\r\n",
      "   3. OnlineBackup         (CAT): 0.0842\r\n",
      "   4. Contract             (CAT): 0.0806\r\n",
      "   5. TechSupport          (CAT): 0.0593\r\n",
      "   6. StreamingTV          (CAT): 0.0579\r\n",
      "   7. tenure               (NUM): 0.0566\r\n",
      "   8. StreamingMovies      (CAT): 0.0495\r\n",
      "   9. PaymentMethod        (CAT): 0.0480\r\n",
      "  10. PhoneService         (CAT): 0.0477\r\n",
      "  11. SeniorCitizen        (CAT): 0.0466\r\n",
      "  12. OnlineSecurity       (CAT): 0.0459\r\n",
      "  13. TotalCharges         (NUM): 0.0447\r\n",
      "  14. Dependents           (CAT): 0.0409\r\n",
      "  15. Partner              (CAT): 0.0352\r\n",
      "  16. InternetService      (CAT): 0.0344\r\n",
      "  17. DeviceProtection     (CAT): 0.0319\r\n",
      "  18. MonthlyCharges       (NUM): 0.0253\r\n",
      "  19. gender               (CAT): 0.0235\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'MultipleLines', 'OnlineBackup', 'Contract', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaymentMethod', 'PhoneService', 'SeniorCitizen', 'OnlineSecurity', 'Dependents', 'Partner', 'InternetService', 'DeviceProtection', 'gender'] → indices [14, 5, 8, 13, 10, 11, 12, 15, 4, 1, 7, 3, 2, 6, 9, 0]\r\n",
      "📊 Features sélectionnées: 3 numériques, 16 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 103,777 paramètres\r\n",
      "🔗 Sparsité d'attention: 87.50%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5720 | Val Loss: 0.5333 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5333)\r\n",
      "Epoch 001 | Train Loss: 0.5478 | Val Loss: 0.5278 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5278)\r\n",
      "Epoch 002 | Train Loss: 0.5387 | Val Loss: 0.5006 | Time: 4.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5006)\r\n",
      "Epoch 003 | Train Loss: 0.5413 | Val Loss: 0.5462 | Time: 4.77s\r\n",
      "Epoch 004 | Train Loss: 0.5610 | Val Loss: 0.5867 | Time: 4.84s\r\n",
      "Epoch 005 | Train Loss: 0.5817 | Val Loss: 0.5785 | Time: 4.80s\r\n",
      "Epoch 006 | Train Loss: 0.5806 | Val Loss: 0.5770 | Time: 4.97s\r\n",
      "Epoch 007 | Train Loss: 0.5705 | Val Loss: 0.5146 | Time: 4.78s\r\n",
      "Epoch 008 | Train Loss: 0.5547 | Val Loss: 0.5413 | Time: 4.73s\r\n",
      "Epoch 009 | Train Loss: 0.5526 | Val Loss: 0.5192 | Time: 4.74s\r\n",
      "Epoch 010 | Train Loss: 0.5418 | Val Loss: 0.5131 | Time: 4.68s\r\n",
      "Epoch 011 | Train Loss: 0.5399 | Val Loss: 0.5051 | Time: 4.73s\r\n",
      "Epoch 012 | Train Loss: 0.5280 | Val Loss: 0.4898 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4898)\r\n",
      "Epoch 013 | Train Loss: 0.5498 | Val Loss: 0.5418 | Time: 4.78s\r\n",
      "Epoch 014 | Train Loss: 0.5476 | Val Loss: 0.5070 | Time: 4.74s\r\n",
      "Epoch 015 | Train Loss: 0.5360 | Val Loss: 0.5135 | Time: 4.68s\r\n",
      "Epoch 016 | Train Loss: 0.5413 | Val Loss: 0.5052 | Time: 4.72s\r\n",
      "Epoch 017 | Train Loss: 0.5421 | Val Loss: 0.5334 | Time: 4.69s\r\n",
      "Epoch 018 | Train Loss: 0.5415 | Val Loss: 0.5092 | Time: 4.76s\r\n",
      "Epoch 019 | Train Loss: 0.5390 | Val Loss: 0.5000 | Time: 4.90s\r\n",
      "Epoch 020 | Train Loss: 0.5365 | Val Loss: 0.5090 | Time: 4.73s\r\n",
      "Epoch 021 | Train Loss: 0.5379 | Val Loss: 0.5157 | Time: 4.71s\r\n",
      "Epoch 022 | Train Loss: 0.5346 | Val Loss: 0.4800 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4800)\r\n",
      "Epoch 023 | Train Loss: 0.5299 | Val Loss: 0.4607 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4607)\r\n",
      "Epoch 024 | Train Loss: 0.5222 | Val Loss: 0.5039 | Time: 4.73s\r\n",
      "Epoch 025 | Train Loss: 0.5355 | Val Loss: 0.5920 | Time: 4.72s\r\n",
      "Epoch 026 | Train Loss: 0.5392 | Val Loss: 0.5024 | Time: 4.86s\r\n",
      "Epoch 027 | Train Loss: 0.5396 | Val Loss: 0.4999 | Time: 4.72s\r\n",
      "Epoch 028 | Train Loss: 0.5262 | Val Loss: 0.4764 | Time: 4.76s\r\n",
      "Epoch 029 | Train Loss: 0.5107 | Val Loss: 0.4613 | Time: 4.72s\r\n",
      "Epoch 030 | Train Loss: 0.5145 | Val Loss: 0.4884 | Time: 4.74s\r\n",
      "Epoch 031 | Train Loss: 0.5211 | Val Loss: 0.4915 | Time: 4.70s\r\n",
      "Epoch 032 | Train Loss: 0.5255 | Val Loss: 0.5028 | Time: 4.91s\r\n",
      "Epoch 033 | Train Loss: 0.5240 | Val Loss: 0.4831 | Time: 4.85s\r\n",
      "Epoch 034 | Train Loss: 0.5099 | Val Loss: 0.4702 | Time: 4.70s\r\n",
      "Epoch 035 | Train Loss: 0.5177 | Val Loss: 0.4694 | Time: 4.77s\r\n",
      "Epoch 036 | Train Loss: 0.5196 | Val Loss: 0.4862 | Time: 4.75s\r\n",
      "Epoch 037 | Train Loss: 0.5236 | Val Loss: 0.5046 | Time: 4.76s\r\n",
      "Epoch 038 | Train Loss: 0.5403 | Val Loss: 0.4979 | Time: 4.72s\r\n",
      "Epoch 039 | Train Loss: 0.5165 | Val Loss: 0.4609 | Time: 4.80s\r\n",
      "Epoch 040 | Train Loss: 0.4979 | Val Loss: 0.4575 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4575)\r\n",
      "Epoch 041 | Train Loss: 0.4912 | Val Loss: 0.4591 | Time: 4.74s\r\n",
      "Epoch 042 | Train Loss: 0.5003 | Val Loss: 0.4754 | Time: 4.70s\r\n",
      "Epoch 043 | Train Loss: 0.4990 | Val Loss: 0.4490 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4490)\r\n",
      "Epoch 044 | Train Loss: 0.4880 | Val Loss: 0.4424 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4424)\r\n",
      "Epoch 045 | Train Loss: 0.4921 | Val Loss: 0.4730 | Time: 4.74s\r\n",
      "Epoch 046 | Train Loss: 0.5127 | Val Loss: 0.5282 | Time: 4.79s\r\n",
      "Epoch 047 | Train Loss: 0.5351 | Val Loss: 0.5257 | Time: 4.70s\r\n",
      "Epoch 048 | Train Loss: 0.5409 | Val Loss: 0.5279 | Time: 4.65s\r\n",
      "Epoch 049 | Train Loss: 0.5127 | Val Loss: 0.4709 | Time: 4.76s\r\n",
      "✅ Meilleur modèle Random chargé (époque 44, val_loss: 0.4424)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.1953\r\n",
      "   2. PaymentMethod        (CAT): 0.1451\r\n",
      "   3. InternetService      (CAT): 0.1428\r\n",
      "   4. TechSupport          (CAT): 0.0993\r\n",
      "   5. MultipleLines        (CAT): 0.0756\r\n",
      "   6. Partner              (CAT): 0.0509\r\n",
      "   7. tenure               (NUM): 0.0479\r\n",
      "   8. gender               (CAT): 0.0468\r\n",
      "   9. OnlineSecurity       (CAT): 0.0412\r\n",
      "  10. OnlineBackup         (CAT): 0.0375\r\n",
      "  11. PhoneService         (CAT): 0.0300\r\n",
      "  12. PaperlessBilling     (CAT): 0.0208\r\n",
      "  13. DeviceProtection     (CAT): 0.0204\r\n",
      "  14. StreamingTV          (CAT): 0.0181\r\n",
      "  15. StreamingMovies      (CAT): 0.0116\r\n",
      "  16. Dependents           (CAT): 0.0064\r\n",
      "  17. MonthlyCharges       (NUM): 0.0046\r\n",
      "  18. Contract             (CAT): 0.0037\r\n",
      "  19. TotalCharges         (NUM): 0.0020\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.1953\r\n",
      "   2. PaymentMethod       : 0.1451\r\n",
      "   3. InternetService     : 0.1428\r\n",
      "   4. TechSupport         : 0.0993\r\n",
      "   5. MultipleLines       : 0.0756\r\n",
      "   6. Partner             : 0.0509\r\n",
      "   7. tenure              : 0.0479\r\n",
      "   8. gender              : 0.0468\r\n",
      "   9. OnlineSecurity      : 0.0412\r\n",
      "  10. OnlineBackup        : 0.0375\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_9/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 283.3s ===\r\n",
      "\u001b[32m[I 2025-07-19 21:07:49,254]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'d_token_stage1': 128, 'n_blocks_stage1': 2, 'n_heads_stage1': 2, 'ffn_hidden_stage1': 64, 'attention_dropout_stage1': 0.26255991345150054, 'ffn_dropout_stage1': 0.28944971547677173, 'residual_dropout_stage1': 0.1986001063822871, 'lr_stage1': 0.010316033434719717, 'weight_decay_stage1': 7.608480425834646e-05, 'd_token_stage2': 32, 'n_blocks_stage2': 6, 'n_heads_stage2': 4, 'ffn_hidden_stage2': 128, 'attention_dropout_stage2': 0.2298420604232127, 'ffn_dropout_stage2': 0.24920897585308466, 'residual_dropout_stage2': 0.15833687650971598, 'lr_stage2': 0.07058138788373682, 'weight_decay_stage2': 7.487776927545517e-05, 'batch_size': 64, 'patience': 30, 'embedding_type': 'T-LR', 'M': 20, 'k': 6}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  40%|███▏    | 10/25 [2:54:52<3:31:43, 846.92s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 507,265 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5714 | Val Loss: 0.5138 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5138)\r\n",
      "Epoch 001 | Train Loss: 0.5146 | Val Loss: 0.4748 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4748)\r\n",
      "Epoch 002 | Train Loss: 0.4823 | Val Loss: 0.4538 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4538)\r\n",
      "Epoch 003 | Train Loss: 0.4657 | Val Loss: 0.4437 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4437)\r\n",
      "Epoch 004 | Train Loss: 0.4535 | Val Loss: 0.4389 | Time: 2.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4389)\r\n",
      "Epoch 005 | Train Loss: 0.4480 | Val Loss: 0.4362 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4362)\r\n",
      "Epoch 006 | Train Loss: 0.4443 | Val Loss: 0.4339 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4339)\r\n",
      "Epoch 007 | Train Loss: 0.4431 | Val Loss: 0.4327 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 008 | Train Loss: 0.4355 | Val Loss: 0.4313 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4313)\r\n",
      "Epoch 009 | Train Loss: 0.4354 | Val Loss: 0.4295 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4295)\r\n",
      "Epoch 010 | Train Loss: 0.4327 | Val Loss: 0.4283 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 011 | Train Loss: 0.4336 | Val Loss: 0.4265 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 012 | Train Loss: 0.4309 | Val Loss: 0.4254 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 013 | Train Loss: 0.4298 | Val Loss: 0.4244 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4244)\r\n",
      "Epoch 014 | Train Loss: 0.4281 | Val Loss: 0.4230 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4230)\r\n",
      "Epoch 015 | Train Loss: 0.4277 | Val Loss: 0.4220 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4220)\r\n",
      "Epoch 016 | Train Loss: 0.4260 | Val Loss: 0.4212 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4212)\r\n",
      "Epoch 017 | Train Loss: 0.4273 | Val Loss: 0.4207 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4207)\r\n",
      "Epoch 018 | Train Loss: 0.4257 | Val Loss: 0.4193 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4193)\r\n",
      "Epoch 019 | Train Loss: 0.4236 | Val Loss: 0.4185 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4185)\r\n",
      "Epoch 020 | Train Loss: 0.4216 | Val Loss: 0.4176 | Time: 2.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4176)\r\n",
      "Epoch 021 | Train Loss: 0.4234 | Val Loss: 0.4171 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4171)\r\n",
      "Epoch 022 | Train Loss: 0.4213 | Val Loss: 0.4158 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4158)\r\n",
      "Epoch 023 | Train Loss: 0.4217 | Val Loss: 0.4153 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4153)\r\n",
      "Epoch 024 | Train Loss: 0.4215 | Val Loss: 0.4143 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4143)\r\n",
      "Epoch 025 | Train Loss: 0.4234 | Val Loss: 0.4141 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4141)\r\n",
      "Epoch 026 | Train Loss: 0.4198 | Val Loss: 0.4136 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4136)\r\n",
      "Epoch 027 | Train Loss: 0.4210 | Val Loss: 0.4135 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4135)\r\n",
      "Epoch 028 | Train Loss: 0.4203 | Val Loss: 0.4126 | Time: 2.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4126)\r\n",
      "Epoch 029 | Train Loss: 0.4193 | Val Loss: 0.4132 | Time: 2.60s\r\n",
      "Epoch 030 | Train Loss: 0.4186 | Val Loss: 0.4136 | Time: 2.53s\r\n",
      "Epoch 031 | Train Loss: 0.4196 | Val Loss: 0.4126 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4126)\r\n",
      "Epoch 032 | Train Loss: 0.4196 | Val Loss: 0.4123 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4123)\r\n",
      "Epoch 033 | Train Loss: 0.4181 | Val Loss: 0.4114 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4114)\r\n",
      "Epoch 034 | Train Loss: 0.4187 | Val Loss: 0.4115 | Time: 2.55s\r\n",
      "Epoch 035 | Train Loss: 0.4198 | Val Loss: 0.4118 | Time: 2.58s\r\n",
      "Epoch 036 | Train Loss: 0.4180 | Val Loss: 0.4110 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4110)\r\n",
      "Epoch 037 | Train Loss: 0.4186 | Val Loss: 0.4108 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4108)\r\n",
      "Epoch 038 | Train Loss: 0.4157 | Val Loss: 0.4094 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4094)\r\n",
      "Epoch 039 | Train Loss: 0.4185 | Val Loss: 0.4105 | Time: 2.50s\r\n",
      "Epoch 040 | Train Loss: 0.4195 | Val Loss: 0.4094 | Time: 2.53s\r\n",
      "Epoch 041 | Train Loss: 0.4164 | Val Loss: 0.4096 | Time: 2.50s\r\n",
      "Epoch 042 | Train Loss: 0.4158 | Val Loss: 0.4099 | Time: 2.66s\r\n",
      "Epoch 043 | Train Loss: 0.4154 | Val Loss: 0.4102 | Time: 2.47s\r\n",
      "Epoch 044 | Train Loss: 0.4152 | Val Loss: 0.4097 | Time: 2.46s\r\n",
      "Epoch 045 | Train Loss: 0.4131 | Val Loss: 0.4095 | Time: 2.49s\r\n",
      "Epoch 046 | Train Loss: 0.4172 | Val Loss: 0.4090 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4090)\r\n",
      "Epoch 047 | Train Loss: 0.4154 | Val Loss: 0.4095 | Time: 2.50s\r\n",
      "Epoch 048 | Train Loss: 0.4152 | Val Loss: 0.4090 | Time: 2.47s\r\n",
      "Epoch 049 | Train Loss: 0.4128 | Val Loss: 0.4091 | Time: 2.51s\r\n",
      "✅ Meilleur modèle chargé (époque 46, val_loss: 0.4090)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0658\r\n",
      "   2. TotalCharges        : 0.0550\r\n",
      "   3. MultipleLines       : 0.0550\r\n",
      "   4. Dependents          : 0.0546\r\n",
      "   5. OnlineSecurity      : 0.0543\r\n",
      "   6. PhoneService        : 0.0539\r\n",
      "   7. DeviceProtection    : 0.0535\r\n",
      "   8. tenure              : 0.0530\r\n",
      "   9. InternetService     : 0.0530\r\n",
      "  10. Contract            : 0.0526\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 10 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0658\r\n",
      "   2. TotalCharges         (NUM): 0.0550\r\n",
      "   3. MultipleLines        (CAT): 0.0550\r\n",
      "   4. Dependents           (CAT): 0.0546\r\n",
      "   5. OnlineSecurity       (CAT): 0.0543\r\n",
      "   6. PhoneService         (CAT): 0.0539\r\n",
      "   7. DeviceProtection     (CAT): 0.0535\r\n",
      "   8. tenure               (NUM): 0.0530\r\n",
      "   9. InternetService      (CAT): 0.0530\r\n",
      "  10. Contract             (CAT): 0.0526\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure'] → indices [2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'MultipleLines', 'Dependents', 'OnlineSecurity', 'PhoneService', 'DeviceProtection', 'InternetService', 'Contract'] → indices [15, 5, 3, 7, 4, 9, 6, 13]\r\n",
      "📊 Features sélectionnées: 2 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 336,001 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.25%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.7725 | Val Loss: 0.6467 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6467)\r\n",
      "Epoch 001 | Train Loss: 0.6444 | Val Loss: 0.5845 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5845)\r\n",
      "Epoch 002 | Train Loss: 0.6010 | Val Loss: 0.5697 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5697)\r\n",
      "Epoch 003 | Train Loss: 0.5777 | Val Loss: 0.5611 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5611)\r\n",
      "Epoch 004 | Train Loss: 0.5669 | Val Loss: 0.5525 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5525)\r\n",
      "Epoch 005 | Train Loss: 0.5561 | Val Loss: 0.5414 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5414)\r\n",
      "Epoch 006 | Train Loss: 0.5464 | Val Loss: 0.5301 | Time: 1.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5301)\r\n",
      "Epoch 007 | Train Loss: 0.5400 | Val Loss: 0.5217 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5217)\r\n",
      "Epoch 008 | Train Loss: 0.5374 | Val Loss: 0.5113 | Time: 1.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5113)\r\n",
      "Epoch 009 | Train Loss: 0.5289 | Val Loss: 0.5002 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5002)\r\n",
      "Epoch 010 | Train Loss: 0.5241 | Val Loss: 0.4917 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4917)\r\n",
      "Epoch 011 | Train Loss: 0.5153 | Val Loss: 0.4835 | Time: 1.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4835)\r\n",
      "Epoch 012 | Train Loss: 0.5081 | Val Loss: 0.4742 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4742)\r\n",
      "Epoch 013 | Train Loss: 0.5076 | Val Loss: 0.4677 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4677)\r\n",
      "Epoch 014 | Train Loss: 0.5027 | Val Loss: 0.4598 | Time: 1.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4598)\r\n",
      "Epoch 015 | Train Loss: 0.4953 | Val Loss: 0.4519 | Time: 1.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4519)\r\n",
      "Epoch 016 | Train Loss: 0.4943 | Val Loss: 0.4463 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4463)\r\n",
      "Epoch 017 | Train Loss: 0.4876 | Val Loss: 0.4418 | Time: 1.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4418)\r\n",
      "Epoch 018 | Train Loss: 0.4872 | Val Loss: 0.4398 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4398)\r\n",
      "Epoch 019 | Train Loss: 0.4809 | Val Loss: 0.4385 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 020 | Train Loss: 0.4826 | Val Loss: 0.4380 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4380)\r\n",
      "Epoch 021 | Train Loss: 0.4820 | Val Loss: 0.4362 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4362)\r\n",
      "Epoch 022 | Train Loss: 0.4711 | Val Loss: 0.4365 | Time: 1.70s\r\n",
      "Epoch 023 | Train Loss: 0.4789 | Val Loss: 0.4361 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4361)\r\n",
      "Epoch 024 | Train Loss: 0.4729 | Val Loss: 0.4363 | Time: 1.67s\r\n",
      "Epoch 025 | Train Loss: 0.4765 | Val Loss: 0.4361 | Time: 1.73s\r\n",
      "Epoch 026 | Train Loss: 0.4720 | Val Loss: 0.4359 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4359)\r\n",
      "Epoch 027 | Train Loss: 0.4695 | Val Loss: 0.4362 | Time: 1.65s\r\n",
      "Epoch 028 | Train Loss: 0.4688 | Val Loss: 0.4366 | Time: 1.68s\r\n",
      "Epoch 029 | Train Loss: 0.4751 | Val Loss: 0.4362 | Time: 1.68s\r\n",
      "Epoch 030 | Train Loss: 0.4723 | Val Loss: 0.4355 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4355)\r\n",
      "Epoch 031 | Train Loss: 0.4746 | Val Loss: 0.4352 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4352)\r\n",
      "Epoch 032 | Train Loss: 0.4706 | Val Loss: 0.4340 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4340)\r\n",
      "Epoch 033 | Train Loss: 0.4670 | Val Loss: 0.4337 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4337)\r\n",
      "Epoch 034 | Train Loss: 0.4662 | Val Loss: 0.4339 | Time: 1.67s\r\n",
      "Epoch 035 | Train Loss: 0.4719 | Val Loss: 0.4338 | Time: 1.70s\r\n",
      "Epoch 036 | Train Loss: 0.4717 | Val Loss: 0.4329 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4329)\r\n",
      "Epoch 037 | Train Loss: 0.4744 | Val Loss: 0.4327 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 038 | Train Loss: 0.4700 | Val Loss: 0.4316 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4316)\r\n",
      "Epoch 039 | Train Loss: 0.4657 | Val Loss: 0.4328 | Time: 1.67s\r\n",
      "Epoch 040 | Train Loss: 0.4606 | Val Loss: 0.4327 | Time: 1.68s\r\n",
      "Epoch 041 | Train Loss: 0.4650 | Val Loss: 0.4326 | Time: 1.71s\r\n",
      "Epoch 042 | Train Loss: 0.4675 | Val Loss: 0.4335 | Time: 1.66s\r\n",
      "Epoch 043 | Train Loss: 0.4617 | Val Loss: 0.4324 | Time: 1.67s\r\n",
      "Epoch 044 | Train Loss: 0.4695 | Val Loss: 0.4325 | Time: 1.74s\r\n",
      "Epoch 045 | Train Loss: 0.4641 | Val Loss: 0.4326 | Time: 1.64s\r\n",
      "Epoch 046 | Train Loss: 0.4616 | Val Loss: 0.4334 | Time: 1.65s\r\n",
      "Epoch 047 | Train Loss: 0.4572 | Val Loss: 0.4344 | Time: 1.70s\r\n",
      "Epoch 048 | Train Loss: 0.4611 | Val Loss: 0.4348 | Time: 1.68s\r\n",
      "Epoch 049 | Train Loss: 0.4595 | Val Loss: 0.4355 | Time: 1.69s\r\n",
      "✅ Meilleur modèle Random chargé (époque 38, val_loss: 0.4316)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. tenure               (NUM): 0.1367\r\n",
      "   2. MultipleLines        (CAT): 0.1241\r\n",
      "   3. Dependents           (CAT): 0.1128\r\n",
      "   4. OnlineSecurity       (CAT): 0.1037\r\n",
      "   5. Contract             (CAT): 0.1035\r\n",
      "   6. PhoneService         (CAT): 0.0955\r\n",
      "   7. InternetService      (CAT): 0.0862\r\n",
      "   8. DeviceProtection     (CAT): 0.0849\r\n",
      "   9. TotalCharges         (NUM): 0.0841\r\n",
      "  10. PaymentMethod        (CAT): 0.0685\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.1367\r\n",
      "   2. MultipleLines       : 0.1241\r\n",
      "   3. Dependents          : 0.1128\r\n",
      "   4. OnlineSecurity      : 0.1037\r\n",
      "   5. Contract            : 0.1035\r\n",
      "   6. PhoneService        : 0.0955\r\n",
      "   7. InternetService     : 0.0862\r\n",
      "   8. DeviceProtection    : 0.0849\r\n",
      "   9. TotalCharges        : 0.0841\r\n",
      "  10. PaymentMethod       : 0.0685\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 212.8s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 507,265 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6141 | Val Loss: 0.5435 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5435)\r\n",
      "Epoch 001 | Train Loss: 0.5335 | Val Loss: 0.4909 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4909)\r\n",
      "Epoch 002 | Train Loss: 0.4936 | Val Loss: 0.4620 | Time: 2.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4620)\r\n",
      "Epoch 003 | Train Loss: 0.4667 | Val Loss: 0.4490 | Time: 2.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4490)\r\n",
      "Epoch 004 | Train Loss: 0.4541 | Val Loss: 0.4444 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4444)\r\n",
      "Epoch 005 | Train Loss: 0.4457 | Val Loss: 0.4412 | Time: 2.57s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4412)\r\n",
      "Epoch 006 | Train Loss: 0.4411 | Val Loss: 0.4397 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4397)\r\n",
      "Epoch 007 | Train Loss: 0.4359 | Val Loss: 0.4375 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4375)\r\n",
      "Epoch 008 | Train Loss: 0.4331 | Val Loss: 0.4353 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4353)\r\n",
      "Epoch 009 | Train Loss: 0.4328 | Val Loss: 0.4345 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 010 | Train Loss: 0.4292 | Val Loss: 0.4326 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4326)\r\n",
      "Epoch 011 | Train Loss: 0.4285 | Val Loss: 0.4313 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4313)\r\n",
      "Epoch 012 | Train Loss: 0.4253 | Val Loss: 0.4295 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4295)\r\n",
      "Epoch 013 | Train Loss: 0.4249 | Val Loss: 0.4281 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4281)\r\n",
      "Epoch 014 | Train Loss: 0.4272 | Val Loss: 0.4269 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4269)\r\n",
      "Epoch 015 | Train Loss: 0.4198 | Val Loss: 0.4257 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4257)\r\n",
      "Epoch 016 | Train Loss: 0.4199 | Val Loss: 0.4247 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4247)\r\n",
      "Epoch 017 | Train Loss: 0.4197 | Val Loss: 0.4238 | Time: 2.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4238)\r\n",
      "Epoch 018 | Train Loss: 0.4161 | Val Loss: 0.4228 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4228)\r\n",
      "Epoch 019 | Train Loss: 0.4158 | Val Loss: 0.4223 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4223)\r\n",
      "Epoch 020 | Train Loss: 0.4154 | Val Loss: 0.4217 | Time: 2.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 021 | Train Loss: 0.4159 | Val Loss: 0.4204 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 022 | Train Loss: 0.4136 | Val Loss: 0.4200 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 023 | Train Loss: 0.4150 | Val Loss: 0.4190 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 024 | Train Loss: 0.4132 | Val Loss: 0.4187 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4187)\r\n",
      "Epoch 025 | Train Loss: 0.4150 | Val Loss: 0.4187 | Time: 2.54s\r\n",
      "Epoch 026 | Train Loss: 0.4107 | Val Loss: 0.4177 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4177)\r\n",
      "Epoch 027 | Train Loss: 0.4116 | Val Loss: 0.4174 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "Epoch 028 | Train Loss: 0.4143 | Val Loss: 0.4159 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 029 | Train Loss: 0.4120 | Val Loss: 0.4154 | Time: 2.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4154)\r\n",
      "Epoch 030 | Train Loss: 0.4125 | Val Loss: 0.4148 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4148)\r\n",
      "Epoch 031 | Train Loss: 0.4088 | Val Loss: 0.4146 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 032 | Train Loss: 0.4086 | Val Loss: 0.4143 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4143)\r\n",
      "Epoch 033 | Train Loss: 0.4097 | Val Loss: 0.4138 | Time: 2.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4138)\r\n",
      "Epoch 034 | Train Loss: 0.4101 | Val Loss: 0.4140 | Time: 2.51s\r\n",
      "Epoch 035 | Train Loss: 0.4072 | Val Loss: 0.4137 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 036 | Train Loss: 0.4106 | Val Loss: 0.4136 | Time: 2.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4136)\r\n",
      "Epoch 037 | Train Loss: 0.4090 | Val Loss: 0.4140 | Time: 2.50s\r\n",
      "Epoch 038 | Train Loss: 0.4052 | Val Loss: 0.4136 | Time: 2.50s\r\n",
      "Epoch 039 | Train Loss: 0.4061 | Val Loss: 0.4126 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4126)\r\n",
      "Epoch 040 | Train Loss: 0.4082 | Val Loss: 0.4121 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4121)\r\n",
      "Epoch 041 | Train Loss: 0.4071 | Val Loss: 0.4120 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4120)\r\n",
      "Epoch 042 | Train Loss: 0.4087 | Val Loss: 0.4117 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4117)\r\n",
      "Epoch 043 | Train Loss: 0.4054 | Val Loss: 0.4112 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4112)\r\n",
      "Epoch 044 | Train Loss: 0.4050 | Val Loss: 0.4112 | Time: 2.54s\r\n",
      "Epoch 045 | Train Loss: 0.4054 | Val Loss: 0.4109 | Time: 2.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4109)\r\n",
      "Epoch 046 | Train Loss: 0.4049 | Val Loss: 0.4112 | Time: 2.51s\r\n",
      "Epoch 047 | Train Loss: 0.4039 | Val Loss: 0.4112 | Time: 2.49s\r\n",
      "Epoch 048 | Train Loss: 0.4078 | Val Loss: 0.4106 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 049 | Train Loss: 0.4042 | Val Loss: 0.4108 | Time: 2.49s\r\n",
      "✅ Meilleur modèle chargé (époque 48, val_loss: 0.4106)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MultipleLines       : 0.0589\r\n",
      "   2. OnlineBackup        : 0.0576\r\n",
      "   3. PaperlessBilling    : 0.0574\r\n",
      "   4. StreamingMovies     : 0.0555\r\n",
      "   5. Dependents          : 0.0553\r\n",
      "   6. MonthlyCharges      : 0.0551\r\n",
      "   7. TechSupport         : 0.0547\r\n",
      "   8. OnlineSecurity      : 0.0541\r\n",
      "   9. TotalCharges        : 0.0521\r\n",
      "  10. SeniorCitizen       : 0.0519\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 10 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MultipleLines        (CAT): 0.0589\r\n",
      "   2. OnlineBackup         (CAT): 0.0576\r\n",
      "   3. PaperlessBilling     (CAT): 0.0574\r\n",
      "   4. StreamingMovies      (CAT): 0.0555\r\n",
      "   5. Dependents           (CAT): 0.0553\r\n",
      "   6. MonthlyCharges       (NUM): 0.0551\r\n",
      "   7. TechSupport          (CAT): 0.0547\r\n",
      "   8. OnlineSecurity       (CAT): 0.0541\r\n",
      "   9. TotalCharges         (NUM): 0.0521\r\n",
      "  10. SeniorCitizen        (CAT): 0.0519\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges'] → indices [1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['MultipleLines', 'OnlineBackup', 'PaperlessBilling', 'StreamingMovies', 'Dependents', 'TechSupport', 'OnlineSecurity', 'SeniorCitizen'] → indices [5, 8, 14, 12, 3, 10, 7, 1]\r\n",
      "📊 Features sélectionnées: 2 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 335,745 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.25%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6205 | Val Loss: 0.5669 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5669)\r\n",
      "Epoch 001 | Train Loss: 0.5739 | Val Loss: 0.5352 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5352)\r\n",
      "Epoch 002 | Train Loss: 0.5552 | Val Loss: 0.5170 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5170)\r\n",
      "Epoch 003 | Train Loss: 0.5400 | Val Loss: 0.5038 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5038)\r\n",
      "Epoch 004 | Train Loss: 0.5295 | Val Loss: 0.4946 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4946)\r\n",
      "Epoch 005 | Train Loss: 0.5222 | Val Loss: 0.4863 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4863)\r\n",
      "Epoch 006 | Train Loss: 0.5197 | Val Loss: 0.4791 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4791)\r\n",
      "Epoch 007 | Train Loss: 0.5135 | Val Loss: 0.4746 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4746)\r\n",
      "Epoch 008 | Train Loss: 0.5043 | Val Loss: 0.4703 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4703)\r\n",
      "Epoch 009 | Train Loss: 0.4994 | Val Loss: 0.4663 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4663)\r\n",
      "Epoch 010 | Train Loss: 0.5009 | Val Loss: 0.4628 | Time: 1.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4628)\r\n",
      "Epoch 011 | Train Loss: 0.4967 | Val Loss: 0.4600 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4600)\r\n",
      "Epoch 012 | Train Loss: 0.4915 | Val Loss: 0.4569 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4569)\r\n",
      "Epoch 013 | Train Loss: 0.4874 | Val Loss: 0.4549 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4549)\r\n",
      "Epoch 014 | Train Loss: 0.4899 | Val Loss: 0.4525 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4525)\r\n",
      "Epoch 015 | Train Loss: 0.4804 | Val Loss: 0.4519 | Time: 1.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4519)\r\n",
      "Epoch 016 | Train Loss: 0.4863 | Val Loss: 0.4494 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4494)\r\n",
      "Epoch 017 | Train Loss: 0.4897 | Val Loss: 0.4481 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4481)\r\n",
      "Epoch 018 | Train Loss: 0.4838 | Val Loss: 0.4476 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4476)\r\n",
      "Epoch 019 | Train Loss: 0.4819 | Val Loss: 0.4471 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4471)\r\n",
      "Epoch 020 | Train Loss: 0.4772 | Val Loss: 0.4466 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4466)\r\n",
      "Epoch 021 | Train Loss: 0.4824 | Val Loss: 0.4456 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4456)\r\n",
      "Epoch 022 | Train Loss: 0.4817 | Val Loss: 0.4451 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4451)\r\n",
      "Epoch 023 | Train Loss: 0.4738 | Val Loss: 0.4450 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4450)\r\n",
      "Epoch 024 | Train Loss: 0.4693 | Val Loss: 0.4455 | Time: 1.65s\r\n",
      "Epoch 025 | Train Loss: 0.4764 | Val Loss: 0.4457 | Time: 1.65s\r\n",
      "Epoch 026 | Train Loss: 0.4756 | Val Loss: 0.4460 | Time: 1.66s\r\n",
      "Epoch 027 | Train Loss: 0.4705 | Val Loss: 0.4455 | Time: 1.69s\r\n",
      "Epoch 028 | Train Loss: 0.4738 | Val Loss: 0.4443 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4443)\r\n",
      "Epoch 029 | Train Loss: 0.4699 | Val Loss: 0.4431 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4431)\r\n",
      "Epoch 030 | Train Loss: 0.4681 | Val Loss: 0.4430 | Time: 1.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4430)\r\n",
      "Epoch 031 | Train Loss: 0.4677 | Val Loss: 0.4444 | Time: 1.73s\r\n",
      "Epoch 032 | Train Loss: 0.4687 | Val Loss: 0.4437 | Time: 1.67s\r\n",
      "Epoch 033 | Train Loss: 0.4678 | Val Loss: 0.4447 | Time: 1.71s\r\n",
      "Epoch 034 | Train Loss: 0.4675 | Val Loss: 0.4436 | Time: 1.68s\r\n",
      "Epoch 035 | Train Loss: 0.4699 | Val Loss: 0.4428 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4428)\r\n",
      "Epoch 036 | Train Loss: 0.4712 | Val Loss: 0.4440 | Time: 1.67s\r\n",
      "Epoch 037 | Train Loss: 0.4662 | Val Loss: 0.4452 | Time: 1.70s\r\n",
      "Epoch 038 | Train Loss: 0.4681 | Val Loss: 0.4428 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4428)\r\n",
      "Epoch 039 | Train Loss: 0.4628 | Val Loss: 0.4427 | Time: 1.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4427)\r\n",
      "Epoch 040 | Train Loss: 0.4645 | Val Loss: 0.4411 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4411)\r\n",
      "Epoch 041 | Train Loss: 0.4678 | Val Loss: 0.4412 | Time: 1.65s\r\n",
      "Epoch 042 | Train Loss: 0.4639 | Val Loss: 0.4425 | Time: 1.65s\r\n",
      "Epoch 043 | Train Loss: 0.4618 | Val Loss: 0.4413 | Time: 1.67s\r\n",
      "Epoch 044 | Train Loss: 0.4705 | Val Loss: 0.4405 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4405)\r\n",
      "Epoch 045 | Train Loss: 0.4608 | Val Loss: 0.4405 | Time: 1.76s\r\n",
      "Epoch 046 | Train Loss: 0.4594 | Val Loss: 0.4409 | Time: 1.69s\r\n",
      "Epoch 047 | Train Loss: 0.4637 | Val Loss: 0.4411 | Time: 1.67s\r\n",
      "Epoch 048 | Train Loss: 0.4655 | Val Loss: 0.4405 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4405)\r\n",
      "Epoch 049 | Train Loss: 0.4620 | Val Loss: 0.4396 | Time: 1.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4396)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4396)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. TotalCharges         (NUM): 0.1541\r\n",
      "   2. SeniorCitizen        (CAT): 0.1399\r\n",
      "   3. StreamingMovies      (CAT): 0.1377\r\n",
      "   4. OnlineSecurity       (CAT): 0.1023\r\n",
      "   5. TechSupport          (CAT): 0.0872\r\n",
      "   6. MultipleLines        (CAT): 0.0836\r\n",
      "   7. PaperlessBilling     (CAT): 0.0747\r\n",
      "   8. Dependents           (CAT): 0.0746\r\n",
      "   9. OnlineBackup         (CAT): 0.0730\r\n",
      "  10. MonthlyCharges       (NUM): 0.0728\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TotalCharges        : 0.1541\r\n",
      "   2. SeniorCitizen       : 0.1399\r\n",
      "   3. StreamingMovies     : 0.1377\r\n",
      "   4. OnlineSecurity      : 0.1023\r\n",
      "   5. TechSupport         : 0.0872\r\n",
      "   6. MultipleLines       : 0.0836\r\n",
      "   7. PaperlessBilling    : 0.0747\r\n",
      "   8. Dependents          : 0.0746\r\n",
      "   9. OnlineBackup        : 0.0730\r\n",
      "  10. MonthlyCharges      : 0.0728\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 213.2s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 507,265 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6427 | Val Loss: 0.5513 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5513)\r\n",
      "Epoch 001 | Train Loss: 0.5370 | Val Loss: 0.4925 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4925)\r\n",
      "Epoch 002 | Train Loss: 0.4962 | Val Loss: 0.4563 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4563)\r\n",
      "Epoch 003 | Train Loss: 0.4695 | Val Loss: 0.4369 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4369)\r\n",
      "Epoch 004 | Train Loss: 0.4573 | Val Loss: 0.4281 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4281)\r\n",
      "Epoch 005 | Train Loss: 0.4521 | Val Loss: 0.4240 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4240)\r\n",
      "Epoch 006 | Train Loss: 0.4433 | Val Loss: 0.4214 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 007 | Train Loss: 0.4421 | Val Loss: 0.4192 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 008 | Train Loss: 0.4375 | Val Loss: 0.4173 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4173)\r\n",
      "Epoch 009 | Train Loss: 0.4351 | Val Loss: 0.4155 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4155)\r\n",
      "Epoch 010 | Train Loss: 0.4314 | Val Loss: 0.4152 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4152)\r\n",
      "Epoch 011 | Train Loss: 0.4290 | Val Loss: 0.4136 | Time: 2.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4136)\r\n",
      "Epoch 012 | Train Loss: 0.4292 | Val Loss: 0.4135 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4135)\r\n",
      "Epoch 013 | Train Loss: 0.4272 | Val Loss: 0.4118 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4118)\r\n",
      "Epoch 014 | Train Loss: 0.4305 | Val Loss: 0.4112 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4112)\r\n",
      "Epoch 015 | Train Loss: 0.4264 | Val Loss: 0.4116 | Time: 2.54s\r\n",
      "Epoch 016 | Train Loss: 0.4238 | Val Loss: 0.4106 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 017 | Train Loss: 0.4241 | Val Loss: 0.4096 | Time: 2.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4096)\r\n",
      "Epoch 018 | Train Loss: 0.4220 | Val Loss: 0.4090 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4090)\r\n",
      "Epoch 019 | Train Loss: 0.4221 | Val Loss: 0.4100 | Time: 2.55s\r\n",
      "Epoch 020 | Train Loss: 0.4228 | Val Loss: 0.4094 | Time: 2.50s\r\n",
      "Epoch 021 | Train Loss: 0.4212 | Val Loss: 0.4097 | Time: 2.51s\r\n",
      "Epoch 022 | Train Loss: 0.4236 | Val Loss: 0.4090 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4090)\r\n",
      "Epoch 023 | Train Loss: 0.4188 | Val Loss: 0.4091 | Time: 2.59s\r\n",
      "Epoch 024 | Train Loss: 0.4214 | Val Loss: 0.4080 | Time: 2.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4080)\r\n",
      "Epoch 025 | Train Loss: 0.4179 | Val Loss: 0.4089 | Time: 2.50s\r\n",
      "Epoch 026 | Train Loss: 0.4197 | Val Loss: 0.4086 | Time: 2.50s\r\n",
      "Epoch 027 | Train Loss: 0.4207 | Val Loss: 0.4078 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4078)\r\n",
      "Epoch 028 | Train Loss: 0.4180 | Val Loss: 0.4091 | Time: 2.56s\r\n",
      "Epoch 029 | Train Loss: 0.4153 | Val Loss: 0.4087 | Time: 2.47s\r\n",
      "Epoch 030 | Train Loss: 0.4181 | Val Loss: 0.4084 | Time: 2.45s\r\n",
      "Epoch 031 | Train Loss: 0.4174 | Val Loss: 0.4083 | Time: 2.53s\r\n",
      "Epoch 032 | Train Loss: 0.4190 | Val Loss: 0.4086 | Time: 2.50s\r\n",
      "Epoch 033 | Train Loss: 0.4169 | Val Loss: 0.4087 | Time: 2.54s\r\n",
      "Epoch 034 | Train Loss: 0.4156 | Val Loss: 0.4075 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4075)\r\n",
      "Epoch 035 | Train Loss: 0.4151 | Val Loss: 0.4071 | Time: 2.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4071)\r\n",
      "Epoch 036 | Train Loss: 0.4171 | Val Loss: 0.4073 | Time: 2.57s\r\n",
      "Epoch 037 | Train Loss: 0.4164 | Val Loss: 0.4070 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4070)\r\n",
      "Epoch 038 | Train Loss: 0.4138 | Val Loss: 0.4070 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4070)\r\n",
      "Epoch 039 | Train Loss: 0.4181 | Val Loss: 0.4066 | Time: 2.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4066)\r\n",
      "Epoch 040 | Train Loss: 0.4142 | Val Loss: 0.4065 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4065)\r\n",
      "Epoch 041 | Train Loss: 0.4127 | Val Loss: 0.4066 | Time: 2.50s\r\n",
      "Epoch 042 | Train Loss: 0.4146 | Val Loss: 0.4061 | Time: 2.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4061)\r\n",
      "Epoch 043 | Train Loss: 0.4133 | Val Loss: 0.4067 | Time: 2.55s\r\n",
      "Epoch 044 | Train Loss: 0.4117 | Val Loss: 0.4066 | Time: 2.51s\r\n",
      "Epoch 045 | Train Loss: 0.4127 | Val Loss: 0.4070 | Time: 2.47s\r\n",
      "Epoch 046 | Train Loss: 0.4123 | Val Loss: 0.4070 | Time: 2.50s\r\n",
      "Epoch 047 | Train Loss: 0.4143 | Val Loss: 0.4070 | Time: 2.52s\r\n",
      "Epoch 048 | Train Loss: 0.4138 | Val Loss: 0.4066 | Time: 2.48s\r\n",
      "Epoch 049 | Train Loss: 0.4135 | Val Loss: 0.4067 | Time: 2.61s\r\n",
      "✅ Meilleur modèle chargé (époque 42, val_loss: 0.4061)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.0604\r\n",
      "   2. TotalCharges        : 0.0590\r\n",
      "   3. SeniorCitizen       : 0.0580\r\n",
      "   4. PaperlessBilling    : 0.0569\r\n",
      "   5. Contract            : 0.0552\r\n",
      "   6. StreamingTV         : 0.0540\r\n",
      "   7. OnlineBackup        : 0.0524\r\n",
      "   8. MultipleLines       : 0.0516\r\n",
      "   9. Dependents          : 0.0516\r\n",
      "  10. StreamingMovies     : 0.0514\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 10 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. gender               (CAT): 0.0604\r\n",
      "   2. TotalCharges         (NUM): 0.0590\r\n",
      "   3. SeniorCitizen        (CAT): 0.0580\r\n",
      "   4. PaperlessBilling     (CAT): 0.0569\r\n",
      "   5. Contract             (CAT): 0.0552\r\n",
      "   6. StreamingTV          (CAT): 0.0540\r\n",
      "   7. OnlineBackup         (CAT): 0.0524\r\n",
      "   8. MultipleLines        (CAT): 0.0516\r\n",
      "   9. Dependents           (CAT): 0.0516\r\n",
      "  10. StreamingMovies      (CAT): 0.0514\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['gender', 'SeniorCitizen', 'PaperlessBilling', 'Contract', 'StreamingTV', 'OnlineBackup', 'MultipleLines', 'Dependents', 'StreamingMovies'] → indices [0, 1, 14, 13, 11, 8, 5, 3, 12]\r\n",
      "📊 Features sélectionnées: 1 numériques, 9 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 335,873 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.25%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6735 | Val Loss: 0.6123 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6123)\r\n",
      "Epoch 001 | Train Loss: 0.6113 | Val Loss: 0.5591 | Time: 1.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5591)\r\n",
      "Epoch 002 | Train Loss: 0.5805 | Val Loss: 0.5353 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5353)\r\n",
      "Epoch 003 | Train Loss: 0.5654 | Val Loss: 0.5147 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5147)\r\n",
      "Epoch 004 | Train Loss: 0.5542 | Val Loss: 0.4962 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4962)\r\n",
      "Epoch 005 | Train Loss: 0.5445 | Val Loss: 0.4815 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4815)\r\n",
      "Epoch 006 | Train Loss: 0.5380 | Val Loss: 0.4701 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4701)\r\n",
      "Epoch 007 | Train Loss: 0.5297 | Val Loss: 0.4632 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4632)\r\n",
      "Epoch 008 | Train Loss: 0.5209 | Val Loss: 0.4576 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4576)\r\n",
      "Epoch 009 | Train Loss: 0.5201 | Val Loss: 0.4539 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4539)\r\n",
      "Epoch 010 | Train Loss: 0.5203 | Val Loss: 0.4515 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4515)\r\n",
      "Epoch 011 | Train Loss: 0.5161 | Val Loss: 0.4500 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4500)\r\n",
      "Epoch 012 | Train Loss: 0.5096 | Val Loss: 0.4478 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4478)\r\n",
      "Epoch 013 | Train Loss: 0.5091 | Val Loss: 0.4462 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4462)\r\n",
      "Epoch 014 | Train Loss: 0.5006 | Val Loss: 0.4450 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4450)\r\n",
      "Epoch 015 | Train Loss: 0.5006 | Val Loss: 0.4433 | Time: 1.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4433)\r\n",
      "Epoch 016 | Train Loss: 0.5014 | Val Loss: 0.4426 | Time: 1.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4426)\r\n",
      "Epoch 017 | Train Loss: 0.4973 | Val Loss: 0.4421 | Time: 1.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4421)\r\n",
      "Epoch 018 | Train Loss: 0.4977 | Val Loss: 0.4416 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4416)\r\n",
      "Epoch 019 | Train Loss: 0.4933 | Val Loss: 0.4419 | Time: 1.70s\r\n",
      "Epoch 020 | Train Loss: 0.4974 | Val Loss: 0.4410 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4410)\r\n",
      "Epoch 021 | Train Loss: 0.4911 | Val Loss: 0.4404 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4404)\r\n",
      "Epoch 022 | Train Loss: 0.4938 | Val Loss: 0.4402 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4402)\r\n",
      "Epoch 023 | Train Loss: 0.4974 | Val Loss: 0.4400 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4400)\r\n",
      "Epoch 024 | Train Loss: 0.4913 | Val Loss: 0.4391 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4391)\r\n",
      "Epoch 025 | Train Loss: 0.4911 | Val Loss: 0.4384 | Time: 1.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4384)\r\n",
      "Epoch 026 | Train Loss: 0.4896 | Val Loss: 0.4380 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4380)\r\n",
      "Epoch 027 | Train Loss: 0.4872 | Val Loss: 0.4382 | Time: 1.67s\r\n",
      "Epoch 028 | Train Loss: 0.4896 | Val Loss: 0.4371 | Time: 1.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4371)\r\n",
      "Epoch 029 | Train Loss: 0.4829 | Val Loss: 0.4367 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4367)\r\n",
      "Epoch 030 | Train Loss: 0.4839 | Val Loss: 0.4364 | Time: 1.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 031 | Train Loss: 0.4901 | Val Loss: 0.4359 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4359)\r\n",
      "Epoch 032 | Train Loss: 0.4803 | Val Loss: 0.4360 | Time: 1.66s\r\n",
      "Epoch 033 | Train Loss: 0.4896 | Val Loss: 0.4354 | Time: 1.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4354)\r\n",
      "Epoch 034 | Train Loss: 0.4845 | Val Loss: 0.4349 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4349)\r\n",
      "Epoch 035 | Train Loss: 0.4829 | Val Loss: 0.4350 | Time: 1.71s\r\n",
      "Epoch 036 | Train Loss: 0.4876 | Val Loss: 0.4360 | Time: 1.75s\r\n",
      "Epoch 037 | Train Loss: 0.4833 | Val Loss: 0.4350 | Time: 1.70s\r\n",
      "Epoch 038 | Train Loss: 0.4846 | Val Loss: 0.4354 | Time: 1.68s\r\n",
      "Epoch 039 | Train Loss: 0.4803 | Val Loss: 0.4350 | Time: 1.67s\r\n",
      "Epoch 040 | Train Loss: 0.4804 | Val Loss: 0.4348 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4348)\r\n",
      "Epoch 041 | Train Loss: 0.4852 | Val Loss: 0.4345 | Time: 1.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 042 | Train Loss: 0.4811 | Val Loss: 0.4341 | Time: 1.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4341)\r\n",
      "Epoch 043 | Train Loss: 0.4810 | Val Loss: 0.4330 | Time: 1.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4330)\r\n",
      "Epoch 044 | Train Loss: 0.4828 | Val Loss: 0.4334 | Time: 1.67s\r\n",
      "Epoch 045 | Train Loss: 0.4794 | Val Loss: 0.4319 | Time: 1.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4319)\r\n",
      "Epoch 046 | Train Loss: 0.4799 | Val Loss: 0.4323 | Time: 1.66s\r\n",
      "Epoch 047 | Train Loss: 0.4835 | Val Loss: 0.4330 | Time: 1.68s\r\n",
      "Epoch 048 | Train Loss: 0.4844 | Val Loss: 0.4323 | Time: 1.67s\r\n",
      "Epoch 049 | Train Loss: 0.4797 | Val Loss: 0.4314 | Time: 1.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4314)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4314)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Dependents           (CAT): 0.1759\r\n",
      "   2. OnlineBackup         (CAT): 0.1100\r\n",
      "   3. StreamingMovies      (CAT): 0.1097\r\n",
      "   4. Contract             (CAT): 0.1057\r\n",
      "   5. MultipleLines        (CAT): 0.0942\r\n",
      "   6. TotalCharges         (NUM): 0.0888\r\n",
      "   7. gender               (CAT): 0.0834\r\n",
      "   8. SeniorCitizen        (CAT): 0.0832\r\n",
      "   9. PaperlessBilling     (CAT): 0.0782\r\n",
      "  10. StreamingTV          (CAT): 0.0708\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Dependents          : 0.1759\r\n",
      "   2. OnlineBackup        : 0.1100\r\n",
      "   3. StreamingMovies     : 0.1097\r\n",
      "   4. Contract            : 0.1057\r\n",
      "   5. MultipleLines       : 0.0942\r\n",
      "   6. TotalCharges        : 0.0888\r\n",
      "   7. gender              : 0.0834\r\n",
      "   8. SeniorCitizen       : 0.0832\r\n",
      "   9. PaperlessBilling    : 0.0782\r\n",
      "  10. StreamingTV         : 0.0708\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_10/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 213.1s ===\r\n",
      "\u001b[32m[I 2025-07-19 21:18:29,096]\u001b[0m Trial 10 finished with value: 0.0 and parameters: {'d_token_stage1': 128, 'n_blocks_stage1': 3, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.29653274790498346, 'ffn_dropout_stage1': 0.14649463015428218, 'residual_dropout_stage1': 0.10155947996584562, 'lr_stage1': 1.1200313812893248e-05, 'weight_decay_stage1': 0.0007124252843547827, 'd_token_stage2': 128, 'n_blocks_stage2': 2, 'n_heads_stage2': 4, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.2873719650894405, 'ffn_dropout_stage2': 0.18234473118567376, 'residual_dropout_stage2': 0.10216581311577111, 'lr_stage2': 1.1049518939499159e-05, 'weight_decay_stage2': 0.09787135264730715, 'batch_size': 64, 'patience': 15, 'embedding_type': 'T', 'M': 10, 'k': 8}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  44%|███▌    | 11/25 [3:05:31<3:02:49, 783.55s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR\r\n",
      "Modèle FTT+ créé avec 414,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4851 | Val Loss: 0.4406 | Time: 9.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4406)\r\n",
      "Epoch 001 | Train Loss: 0.4432 | Val Loss: 0.4447 | Time: 9.69s\r\n",
      "Epoch 002 | Train Loss: 0.4334 | Val Loss: 0.4319 | Time: 9.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4319)\r\n",
      "Epoch 003 | Train Loss: 0.4309 | Val Loss: 0.4317 | Time: 9.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 004 | Train Loss: 0.4273 | Val Loss: 0.4272 | Time: 9.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4272)\r\n",
      "Epoch 005 | Train Loss: 0.4237 | Val Loss: 0.4249 | Time: 9.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4249)\r\n",
      "Epoch 006 | Train Loss: 0.4234 | Val Loss: 0.4254 | Time: 9.61s\r\n",
      "Epoch 007 | Train Loss: 0.4216 | Val Loss: 0.4263 | Time: 9.87s\r\n",
      "Epoch 008 | Train Loss: 0.4207 | Val Loss: 0.4197 | Time: 9.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 009 | Train Loss: 0.4185 | Val Loss: 0.4202 | Time: 9.67s\r\n",
      "Epoch 010 | Train Loss: 0.4209 | Val Loss: 0.4191 | Time: 9.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 011 | Train Loss: 0.4176 | Val Loss: 0.4226 | Time: 9.72s\r\n",
      "Epoch 012 | Train Loss: 0.4186 | Val Loss: 0.4170 | Time: 9.57s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 013 | Train Loss: 0.4156 | Val Loss: 0.4212 | Time: 9.68s\r\n",
      "Epoch 014 | Train Loss: 0.4167 | Val Loss: 0.4189 | Time: 9.68s\r\n",
      "Epoch 015 | Train Loss: 0.4133 | Val Loss: 0.4175 | Time: 9.62s\r\n",
      "Epoch 016 | Train Loss: 0.4146 | Val Loss: 0.4232 | Time: 9.64s\r\n",
      "Epoch 017 | Train Loss: 0.4147 | Val Loss: 0.4180 | Time: 9.68s\r\n",
      "Epoch 018 | Train Loss: 0.4105 | Val Loss: 0.4216 | Time: 9.66s\r\n",
      "Epoch 019 | Train Loss: 0.4111 | Val Loss: 0.4196 | Time: 9.64s\r\n",
      "Epoch 020 | Train Loss: 0.4058 | Val Loss: 0.4291 | Time: 9.86s\r\n",
      "Epoch 021 | Train Loss: 0.4086 | Val Loss: 0.4311 | Time: 9.67s\r\n",
      "Epoch 022 | Train Loss: 0.4096 | Val Loss: 0.4240 | Time: 9.62s\r\n",
      "Epoch 023 | Train Loss: 0.4057 | Val Loss: 0.4161 | Time: 9.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4161)\r\n",
      "Epoch 024 | Train Loss: 0.4045 | Val Loss: 0.4218 | Time: 9.63s\r\n",
      "Epoch 025 | Train Loss: 0.4062 | Val Loss: 0.4264 | Time: 9.61s\r\n",
      "Epoch 026 | Train Loss: 0.4056 | Val Loss: 0.4206 | Time: 9.73s\r\n",
      "Epoch 027 | Train Loss: 0.4021 | Val Loss: 0.4266 | Time: 9.68s\r\n",
      "Epoch 028 | Train Loss: 0.3982 | Val Loss: 0.4263 | Time: 9.62s\r\n",
      "Epoch 029 | Train Loss: 0.3995 | Val Loss: 0.4273 | Time: 9.61s\r\n",
      "Epoch 030 | Train Loss: 0.4005 | Val Loss: 0.4324 | Time: 9.92s\r\n",
      "Epoch 031 | Train Loss: 0.3993 | Val Loss: 0.4348 | Time: 9.67s\r\n",
      "Epoch 032 | Train Loss: 0.3967 | Val Loss: 0.4369 | Time: 9.65s\r\n",
      "Epoch 033 | Train Loss: 0.4016 | Val Loss: 0.4367 | Time: 9.80s\r\n",
      "Epoch 034 | Train Loss: 0.3957 | Val Loss: 0.4452 | Time: 9.66s\r\n",
      "Epoch 035 | Train Loss: 0.3946 | Val Loss: 0.4385 | Time: 9.71s\r\n",
      "Epoch 036 | Train Loss: 0.3975 | Val Loss: 0.4378 | Time: 9.83s\r\n",
      "Epoch 037 | Train Loss: 0.3912 | Val Loss: 0.4382 | Time: 9.66s\r\n",
      "Epoch 038 | Train Loss: 0.3931 | Val Loss: 0.4494 | Time: 9.73s\r\n",
      "Epoch 039 | Train Loss: 0.3907 | Val Loss: 0.4486 | Time: 9.83s\r\n",
      "Epoch 040 | Train Loss: 0.3885 | Val Loss: 0.4461 | Time: 9.64s\r\n",
      "Epoch 041 | Train Loss: 0.3919 | Val Loss: 0.4451 | Time: 9.69s\r\n",
      "Epoch 042 | Train Loss: 0.3911 | Val Loss: 0.4452 | Time: 9.73s\r\n",
      "Epoch 043 | Train Loss: 0.3867 | Val Loss: 0.4476 | Time: 9.86s\r\n",
      "\r\n",
      "Early stopping à l'époque 43 (patience: 20)\r\n",
      "✅ Meilleur modèle chargé (époque 23, val_loss: 0.4161)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Dependents          : 0.0554\r\n",
      "   2. TechSupport         : 0.0545\r\n",
      "   3. MonthlyCharges      : 0.0542\r\n",
      "   4. OnlineBackup        : 0.0541\r\n",
      "   5. tenure              : 0.0536\r\n",
      "   6. PhoneService        : 0.0532\r\n",
      "   7. SeniorCitizen       : 0.0532\r\n",
      "   8. StreamingTV         : 0.0532\r\n",
      "   9. PaymentMethod       : 0.0531\r\n",
      "  10. DeviceProtection    : 0.0530\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 19 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Dependents           (CAT): 0.0554\r\n",
      "   2. TechSupport          (CAT): 0.0545\r\n",
      "   3. MonthlyCharges       (NUM): 0.0542\r\n",
      "   4. OnlineBackup         (CAT): 0.0541\r\n",
      "   5. tenure               (NUM): 0.0536\r\n",
      "   6. PhoneService         (CAT): 0.0532\r\n",
      "   7. SeniorCitizen        (CAT): 0.0532\r\n",
      "   8. StreamingTV          (CAT): 0.0532\r\n",
      "   9. PaymentMethod        (CAT): 0.0531\r\n",
      "  10. DeviceProtection     (CAT): 0.0530\r\n",
      "  11. MultipleLines        (CAT): 0.0528\r\n",
      "  12. PaperlessBilling     (CAT): 0.0523\r\n",
      "  13. Contract             (CAT): 0.0523\r\n",
      "  14. InternetService      (CAT): 0.0521\r\n",
      "  15. OnlineSecurity       (CAT): 0.0514\r\n",
      "  16. gender               (CAT): 0.0508\r\n",
      "  17. Partner              (CAT): 0.0507\r\n",
      "  18. TotalCharges         (NUM): 0.0501\r\n",
      "  19. StreamingMovies      (CAT): 0.0501\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure', 'TotalCharges'] → indices [1, 0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['Dependents', 'TechSupport', 'OnlineBackup', 'PhoneService', 'SeniorCitizen', 'StreamingTV', 'PaymentMethod', 'DeviceProtection', 'MultipleLines', 'PaperlessBilling', 'Contract', 'InternetService', 'OnlineSecurity', 'gender', 'Partner', 'StreamingMovies'] → indices [3, 10, 8, 4, 1, 11, 15, 9, 5, 14, 13, 6, 7, 0, 2, 12]\r\n",
      "📊 Features sélectionnées: 3 numériques, 16 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 670,849 paramètres\r\n",
      "🔗 Sparsité d'attention: 88.00%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5362 | Val Loss: 0.4824 | Time: 6.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4824)\r\n",
      "Epoch 001 | Train Loss: 0.5029 | Val Loss: 0.4739 | Time: 6.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4739)\r\n",
      "Epoch 002 | Train Loss: 0.4916 | Val Loss: 0.4833 | Time: 6.32s\r\n",
      "Epoch 003 | Train Loss: 0.4986 | Val Loss: 0.4818 | Time: 6.35s\r\n",
      "Epoch 004 | Train Loss: 0.4842 | Val Loss: 0.4640 | Time: 6.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4640)\r\n",
      "Epoch 005 | Train Loss: 0.4959 | Val Loss: 0.4798 | Time: 6.28s\r\n",
      "Epoch 006 | Train Loss: 0.4898 | Val Loss: 0.4681 | Time: 6.28s\r\n",
      "Epoch 007 | Train Loss: 0.4980 | Val Loss: 0.5163 | Time: 6.30s\r\n",
      "Epoch 008 | Train Loss: 0.5253 | Val Loss: 0.5103 | Time: 6.36s\r\n",
      "Epoch 009 | Train Loss: 0.5098 | Val Loss: 0.5043 | Time: 6.31s\r\n",
      "Epoch 010 | Train Loss: 0.5097 | Val Loss: 0.4680 | Time: 6.31s\r\n",
      "Epoch 011 | Train Loss: 0.5019 | Val Loss: 0.4998 | Time: 6.26s\r\n",
      "Epoch 012 | Train Loss: 0.4953 | Val Loss: 0.4699 | Time: 6.29s\r\n",
      "Epoch 013 | Train Loss: 0.4935 | Val Loss: 0.4902 | Time: 6.37s\r\n",
      "Epoch 014 | Train Loss: 0.4961 | Val Loss: 0.4694 | Time: 6.28s\r\n",
      "Epoch 015 | Train Loss: 0.4964 | Val Loss: 0.4689 | Time: 6.35s\r\n",
      "Epoch 016 | Train Loss: 0.4907 | Val Loss: 0.4687 | Time: 6.28s\r\n",
      "Epoch 017 | Train Loss: 0.4819 | Val Loss: 0.4622 | Time: 6.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4622)\r\n",
      "Epoch 018 | Train Loss: 0.4859 | Val Loss: 0.4458 | Time: 6.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4458)\r\n",
      "Epoch 019 | Train Loss: 0.4866 | Val Loss: 0.4574 | Time: 6.27s\r\n",
      "Epoch 020 | Train Loss: 0.4729 | Val Loss: 0.4508 | Time: 6.31s\r\n",
      "Epoch 021 | Train Loss: 0.4841 | Val Loss: 0.4516 | Time: 6.31s\r\n",
      "Epoch 022 | Train Loss: 0.4864 | Val Loss: 0.4493 | Time: 6.23s\r\n",
      "Epoch 023 | Train Loss: 0.4953 | Val Loss: 0.4798 | Time: 6.33s\r\n",
      "Epoch 024 | Train Loss: 0.4999 | Val Loss: 0.4833 | Time: 6.27s\r\n",
      "Epoch 025 | Train Loss: 0.5241 | Val Loss: 0.4877 | Time: 6.43s\r\n",
      "Epoch 026 | Train Loss: 0.4911 | Val Loss: 0.4543 | Time: 6.32s\r\n",
      "Epoch 027 | Train Loss: 0.4923 | Val Loss: 0.4680 | Time: 6.30s\r\n",
      "Epoch 028 | Train Loss: 0.5273 | Val Loss: 0.5048 | Time: 6.47s\r\n",
      "Epoch 029 | Train Loss: 0.5092 | Val Loss: 0.4785 | Time: 6.29s\r\n",
      "Epoch 030 | Train Loss: 0.5207 | Val Loss: 0.4868 | Time: 6.26s\r\n",
      "Epoch 031 | Train Loss: 0.5205 | Val Loss: 0.4844 | Time: 6.28s\r\n",
      "Epoch 032 | Train Loss: 0.5152 | Val Loss: 0.4817 | Time: 6.28s\r\n",
      "Epoch 033 | Train Loss: 0.5033 | Val Loss: 0.4730 | Time: 6.44s\r\n",
      "Epoch 034 | Train Loss: 0.5063 | Val Loss: 0.4897 | Time: 6.29s\r\n",
      "Epoch 035 | Train Loss: 0.5035 | Val Loss: 0.4823 | Time: 6.29s\r\n",
      "Epoch 036 | Train Loss: 0.5057 | Val Loss: 0.4865 | Time: 6.33s\r\n",
      "Epoch 037 | Train Loss: 0.5114 | Val Loss: 0.4773 | Time: 6.25s\r\n",
      "Epoch 038 | Train Loss: 0.4995 | Val Loss: 0.4696 | Time: 6.36s\r\n",
      "\r\n",
      "Early stopping à l'époque 38 (patience: 20)\r\n",
      "✅ Meilleur modèle Random chargé (époque 18, val_loss: 0.4458)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingMovies      (CAT): 0.1122\r\n",
      "   2. OnlineBackup         (CAT): 0.1046\r\n",
      "   3. OnlineSecurity       (CAT): 0.0748\r\n",
      "   4. DeviceProtection     (CAT): 0.0678\r\n",
      "   5. TechSupport          (CAT): 0.0589\r\n",
      "   6. PaperlessBilling     (CAT): 0.0541\r\n",
      "   7. SeniorCitizen        (CAT): 0.0486\r\n",
      "   8. tenure               (NUM): 0.0459\r\n",
      "   9. MultipleLines        (CAT): 0.0451\r\n",
      "  10. StreamingTV          (CAT): 0.0419\r\n",
      "  11. MonthlyCharges       (NUM): 0.0416\r\n",
      "  12. gender               (CAT): 0.0415\r\n",
      "  13. PaymentMethod        (CAT): 0.0412\r\n",
      "  14. PhoneService         (CAT): 0.0370\r\n",
      "  15. Dependents           (CAT): 0.0370\r\n",
      "  16. InternetService      (CAT): 0.0370\r\n",
      "  17. TotalCharges         (NUM): 0.0370\r\n",
      "  18. Contract             (CAT): 0.0370\r\n",
      "  19. Partner              (CAT): 0.0369\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingMovies     : 0.1122\r\n",
      "   2. OnlineBackup        : 0.1046\r\n",
      "   3. OnlineSecurity      : 0.0748\r\n",
      "   4. DeviceProtection    : 0.0678\r\n",
      "   5. TechSupport         : 0.0589\r\n",
      "   6. PaperlessBilling    : 0.0541\r\n",
      "   7. SeniorCitizen       : 0.0486\r\n",
      "   8. tenure              : 0.0459\r\n",
      "   9. MultipleLines       : 0.0451\r\n",
      "  10. StreamingTV         : 0.0419\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 677.3s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR\r\n",
      "Modèle FTT+ créé avec 414,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4857 | Val Loss: 0.4450 | Time: 9.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4450)\r\n",
      "Epoch 001 | Train Loss: 0.4458 | Val Loss: 0.4303 | Time: 9.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4303)\r\n",
      "Epoch 002 | Train Loss: 0.4329 | Val Loss: 0.4223 | Time: 9.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4223)\r\n",
      "Epoch 003 | Train Loss: 0.4245 | Val Loss: 0.4192 | Time: 9.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 004 | Train Loss: 0.4211 | Val Loss: 0.4161 | Time: 9.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4161)\r\n",
      "Epoch 005 | Train Loss: 0.4168 | Val Loss: 0.4142 | Time: 9.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4142)\r\n",
      "Epoch 006 | Train Loss: 0.4141 | Val Loss: 0.4160 | Time: 9.82s\r\n",
      "Epoch 007 | Train Loss: 0.4130 | Val Loss: 0.4203 | Time: 9.65s\r\n",
      "Epoch 008 | Train Loss: 0.4109 | Val Loss: 0.4200 | Time: 9.64s\r\n",
      "Epoch 009 | Train Loss: 0.4117 | Val Loss: 0.4182 | Time: 9.80s\r\n",
      "Epoch 010 | Train Loss: 0.4084 | Val Loss: 0.4203 | Time: 9.65s\r\n",
      "Epoch 011 | Train Loss: 0.4087 | Val Loss: 0.4196 | Time: 9.65s\r\n",
      "Epoch 012 | Train Loss: 0.4097 | Val Loss: 0.4178 | Time: 9.72s\r\n",
      "Epoch 013 | Train Loss: 0.4063 | Val Loss: 0.4192 | Time: 9.69s\r\n",
      "Epoch 014 | Train Loss: 0.4067 | Val Loss: 0.4152 | Time: 9.66s\r\n",
      "Epoch 015 | Train Loss: 0.4048 | Val Loss: 0.4226 | Time: 9.84s\r\n",
      "Epoch 016 | Train Loss: 0.4051 | Val Loss: 0.4217 | Time: 9.61s\r\n",
      "Epoch 017 | Train Loss: 0.4039 | Val Loss: 0.4202 | Time: 9.70s\r\n",
      "Epoch 018 | Train Loss: 0.4033 | Val Loss: 0.4190 | Time: 9.63s\r\n",
      "Epoch 019 | Train Loss: 0.3990 | Val Loss: 0.4247 | Time: 9.75s\r\n",
      "Epoch 020 | Train Loss: 0.3989 | Val Loss: 0.4247 | Time: 9.68s\r\n",
      "Epoch 021 | Train Loss: 0.3976 | Val Loss: 0.4258 | Time: 9.65s\r\n",
      "Epoch 022 | Train Loss: 0.3964 | Val Loss: 0.4277 | Time: 9.75s\r\n",
      "Epoch 023 | Train Loss: 0.3951 | Val Loss: 0.4261 | Time: 9.65s\r\n",
      "Epoch 024 | Train Loss: 0.3935 | Val Loss: 0.4283 | Time: 9.71s\r\n",
      "Epoch 025 | Train Loss: 0.3941 | Val Loss: 0.4324 | Time: 9.72s\r\n",
      "\r\n",
      "Early stopping à l'époque 25 (patience: 20)\r\n",
      "✅ Meilleur modèle chargé (époque 5, val_loss: 0.4142)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0569\r\n",
      "   2. SeniorCitizen       : 0.0559\r\n",
      "   3. PhoneService        : 0.0559\r\n",
      "   4. StreamingTV         : 0.0556\r\n",
      "   5. Contract            : 0.0537\r\n",
      "   6. StreamingMovies     : 0.0532\r\n",
      "   7. Dependents          : 0.0527\r\n",
      "   8. TechSupport         : 0.0526\r\n",
      "   9. MultipleLines       : 0.0523\r\n",
      "  10. TotalCharges        : 0.0522\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 19 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0569\r\n",
      "   2. SeniorCitizen        (CAT): 0.0559\r\n",
      "   3. PhoneService         (CAT): 0.0559\r\n",
      "   4. StreamingTV          (CAT): 0.0556\r\n",
      "   5. Contract             (CAT): 0.0537\r\n",
      "   6. StreamingMovies      (CAT): 0.0532\r\n",
      "   7. Dependents           (CAT): 0.0527\r\n",
      "   8. TechSupport          (CAT): 0.0526\r\n",
      "   9. MultipleLines        (CAT): 0.0523\r\n",
      "  10. TotalCharges         (NUM): 0.0522\r\n",
      "  11. OnlineSecurity       (CAT): 0.0520\r\n",
      "  12. Partner              (CAT): 0.0519\r\n",
      "  13. InternetService      (CAT): 0.0514\r\n",
      "  14. tenure               (NUM): 0.0510\r\n",
      "  15. OnlineBackup         (CAT): 0.0508\r\n",
      "  16. MonthlyCharges       (NUM): 0.0506\r\n",
      "  17. gender               (CAT): 0.0505\r\n",
      "  18. DeviceProtection     (CAT): 0.0504\r\n",
      "  19. PaymentMethod        (CAT): 0.0501\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure', 'MonthlyCharges'] → indices [2, 0, 1]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'SeniorCitizen', 'PhoneService', 'StreamingTV', 'Contract', 'StreamingMovies', 'Dependents', 'TechSupport', 'MultipleLines', 'OnlineSecurity', 'Partner', 'InternetService', 'OnlineBackup', 'gender', 'DeviceProtection', 'PaymentMethod'] → indices [14, 1, 4, 11, 13, 12, 3, 10, 5, 7, 2, 6, 8, 0, 9, 15]\r\n",
      "📊 Features sélectionnées: 3 numériques, 16 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 670,849 paramètres\r\n",
      "🔗 Sparsité d'attention: 88.00%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5128 | Val Loss: 0.4909 | Time: 6.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4909)\r\n",
      "Epoch 001 | Train Loss: 0.4895 | Val Loss: 0.4560 | Time: 6.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4560)\r\n",
      "Epoch 002 | Train Loss: 0.4880 | Val Loss: 0.4714 | Time: 6.36s\r\n",
      "Epoch 003 | Train Loss: 0.4883 | Val Loss: 0.4692 | Time: 6.31s\r\n",
      "Epoch 004 | Train Loss: 0.4864 | Val Loss: 0.4950 | Time: 6.40s\r\n",
      "Epoch 005 | Train Loss: 0.4891 | Val Loss: 0.4719 | Time: 6.29s\r\n",
      "Epoch 006 | Train Loss: 0.4803 | Val Loss: 0.4484 | Time: 6.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4484)\r\n",
      "Epoch 007 | Train Loss: 0.4696 | Val Loss: 0.4685 | Time: 6.35s\r\n",
      "Epoch 008 | Train Loss: 0.4775 | Val Loss: 0.4699 | Time: 6.28s\r\n",
      "Epoch 009 | Train Loss: 0.4868 | Val Loss: 0.4853 | Time: 6.41s\r\n",
      "Epoch 010 | Train Loss: 0.4838 | Val Loss: 0.4608 | Time: 6.33s\r\n",
      "Epoch 011 | Train Loss: 0.4710 | Val Loss: 0.4534 | Time: 6.37s\r\n",
      "Epoch 012 | Train Loss: 0.4824 | Val Loss: 0.4989 | Time: 6.32s\r\n",
      "Epoch 013 | Train Loss: 0.4979 | Val Loss: 0.5011 | Time: 6.29s\r\n",
      "Epoch 014 | Train Loss: 0.5227 | Val Loss: 0.4862 | Time: 6.31s\r\n",
      "Epoch 015 | Train Loss: 0.4895 | Val Loss: 0.4603 | Time: 6.32s\r\n",
      "Epoch 016 | Train Loss: 0.4819 | Val Loss: 0.4563 | Time: 6.32s\r\n",
      "Epoch 017 | Train Loss: 0.4722 | Val Loss: 0.4506 | Time: 6.30s\r\n",
      "Epoch 018 | Train Loss: 0.4675 | Val Loss: 0.4578 | Time: 6.31s\r\n",
      "Epoch 019 | Train Loss: 0.4690 | Val Loss: 0.4488 | Time: 6.54s\r\n",
      "Epoch 020 | Train Loss: 0.4676 | Val Loss: 0.4540 | Time: 6.25s\r\n",
      "Epoch 021 | Train Loss: 0.4610 | Val Loss: 0.4404 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4404)\r\n",
      "Epoch 022 | Train Loss: 0.4554 | Val Loss: 0.4468 | Time: 6.34s\r\n",
      "Epoch 023 | Train Loss: 0.4533 | Val Loss: 0.4398 | Time: 6.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4398)\r\n",
      "Epoch 024 | Train Loss: 0.4571 | Val Loss: 0.4450 | Time: 6.34s\r\n",
      "Epoch 025 | Train Loss: 0.4525 | Val Loss: 0.4409 | Time: 6.35s\r\n",
      "Epoch 026 | Train Loss: 0.4552 | Val Loss: 0.4436 | Time: 6.32s\r\n",
      "Epoch 027 | Train Loss: 0.4624 | Val Loss: 0.4432 | Time: 6.36s\r\n",
      "Epoch 028 | Train Loss: 0.4661 | Val Loss: 0.4515 | Time: 6.30s\r\n",
      "Epoch 029 | Train Loss: 0.4634 | Val Loss: 0.4347 | Time: 6.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4347)\r\n",
      "Epoch 030 | Train Loss: 0.4537 | Val Loss: 0.4334 | Time: 6.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4334)\r\n",
      "Epoch 031 | Train Loss: 0.4518 | Val Loss: 0.4338 | Time: 6.35s\r\n",
      "Epoch 032 | Train Loss: 0.4557 | Val Loss: 0.4633 | Time: 6.36s\r\n",
      "Epoch 033 | Train Loss: 0.4640 | Val Loss: 0.4389 | Time: 6.29s\r\n",
      "Epoch 034 | Train Loss: 0.4526 | Val Loss: 0.4355 | Time: 6.36s\r\n",
      "Epoch 035 | Train Loss: 0.4528 | Val Loss: 0.4363 | Time: 6.38s\r\n",
      "Epoch 036 | Train Loss: 0.4542 | Val Loss: 0.4456 | Time: 6.28s\r\n",
      "Epoch 037 | Train Loss: 0.4625 | Val Loss: 0.4346 | Time: 6.33s\r\n",
      "Epoch 038 | Train Loss: 0.4645 | Val Loss: 0.4406 | Time: 6.33s\r\n",
      "Epoch 039 | Train Loss: 0.4638 | Val Loss: 0.4363 | Time: 6.33s\r\n",
      "Epoch 040 | Train Loss: 0.4584 | Val Loss: 0.4452 | Time: 6.33s\r\n",
      "Epoch 041 | Train Loss: 0.4583 | Val Loss: 0.4430 | Time: 6.34s\r\n",
      "Epoch 042 | Train Loss: 0.4581 | Val Loss: 0.4371 | Time: 6.29s\r\n",
      "Epoch 043 | Train Loss: 0.4531 | Val Loss: 0.4431 | Time: 6.29s\r\n",
      "Epoch 044 | Train Loss: 0.4469 | Val Loss: 0.4393 | Time: 6.60s\r\n",
      "Epoch 045 | Train Loss: 0.4528 | Val Loss: 0.4457 | Time: 6.31s\r\n",
      "Epoch 046 | Train Loss: 0.4490 | Val Loss: 0.4345 | Time: 6.28s\r\n",
      "Epoch 047 | Train Loss: 0.4458 | Val Loss: 0.4330 | Time: 6.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4330)\r\n",
      "Epoch 048 | Train Loss: 0.4421 | Val Loss: 0.4296 | Time: 6.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 049 | Train Loss: 0.4508 | Val Loss: 0.4369 | Time: 6.39s\r\n",
      "✅ Meilleur modèle Random chargé (époque 48, val_loss: 0.4296)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingTV          (CAT): 0.1003\r\n",
      "   2. Partner              (CAT): 0.0711\r\n",
      "   3. MonthlyCharges       (NUM): 0.0695\r\n",
      "   4. SeniorCitizen        (CAT): 0.0689\r\n",
      "   5. PhoneService         (CAT): 0.0605\r\n",
      "   6. Contract             (CAT): 0.0538\r\n",
      "   7. MultipleLines        (CAT): 0.0536\r\n",
      "   8. OnlineSecurity       (CAT): 0.0502\r\n",
      "   9. gender               (CAT): 0.0461\r\n",
      "  10. TechSupport          (CAT): 0.0439\r\n",
      "  11. tenure               (NUM): 0.0433\r\n",
      "  12. DeviceProtection     (CAT): 0.0430\r\n",
      "  13. PaymentMethod        (CAT): 0.0430\r\n",
      "  14. InternetService      (CAT): 0.0430\r\n",
      "  15. StreamingMovies      (CAT): 0.0430\r\n",
      "  16. PaperlessBilling     (CAT): 0.0430\r\n",
      "  17. TotalCharges         (NUM): 0.0415\r\n",
      "  18. Dependents           (CAT): 0.0412\r\n",
      "  19. OnlineBackup         (CAT): 0.0412\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingTV         : 0.1003\r\n",
      "   2. Partner             : 0.0711\r\n",
      "   3. MonthlyCharges      : 0.0695\r\n",
      "   4. SeniorCitizen       : 0.0689\r\n",
      "   5. PhoneService        : 0.0605\r\n",
      "   6. Contract            : 0.0538\r\n",
      "   7. MultipleLines       : 0.0536\r\n",
      "   8. OnlineSecurity      : 0.0502\r\n",
      "   9. gender              : 0.0461\r\n",
      "  10. TechSupport         : 0.0439\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 572.9s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR\r\n",
      "Modèle FTT+ créé avec 414,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4837 | Val Loss: 0.4676 | Time: 9.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4676)\r\n",
      "Epoch 001 | Train Loss: 0.4471 | Val Loss: 0.4502 | Time: 9.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4502)\r\n",
      "Epoch 002 | Train Loss: 0.4364 | Val Loss: 0.4367 | Time: 9.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4367)\r\n",
      "Epoch 003 | Train Loss: 0.4318 | Val Loss: 0.4293 | Time: 9.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 004 | Train Loss: 0.4260 | Val Loss: 0.4271 | Time: 9.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4271)\r\n",
      "Epoch 005 | Train Loss: 0.4274 | Val Loss: 0.4297 | Time: 9.92s\r\n",
      "Epoch 006 | Train Loss: 0.4236 | Val Loss: 0.4242 | Time: 9.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4242)\r\n",
      "Epoch 007 | Train Loss: 0.4232 | Val Loss: 0.4269 | Time: 9.65s\r\n",
      "Epoch 008 | Train Loss: 0.4208 | Val Loss: 0.4280 | Time: 9.85s\r\n",
      "Epoch 009 | Train Loss: 0.4207 | Val Loss: 0.4243 | Time: 9.68s\r\n",
      "Epoch 010 | Train Loss: 0.4193 | Val Loss: 0.4328 | Time: 9.75s\r\n",
      "Epoch 011 | Train Loss: 0.4176 | Val Loss: 0.4280 | Time: 9.60s\r\n",
      "Epoch 012 | Train Loss: 0.4176 | Val Loss: 0.4336 | Time: 9.86s\r\n",
      "Epoch 013 | Train Loss: 0.4178 | Val Loss: 0.4298 | Time: 9.66s\r\n",
      "Epoch 014 | Train Loss: 0.4160 | Val Loss: 0.4239 | Time: 9.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "Epoch 015 | Train Loss: 0.4151 | Val Loss: 0.4236 | Time: 9.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4236)\r\n",
      "Epoch 016 | Train Loss: 0.4121 | Val Loss: 0.4358 | Time: 9.66s\r\n",
      "Epoch 017 | Train Loss: 0.4122 | Val Loss: 0.4268 | Time: 9.71s\r\n",
      "Epoch 018 | Train Loss: 0.4096 | Val Loss: 0.4412 | Time: 9.82s\r\n",
      "Epoch 019 | Train Loss: 0.4096 | Val Loss: 0.4389 | Time: 9.73s\r\n",
      "Epoch 020 | Train Loss: 0.4080 | Val Loss: 0.4312 | Time: 9.68s\r\n",
      "Epoch 021 | Train Loss: 0.4066 | Val Loss: 0.4361 | Time: 9.80s\r\n",
      "Epoch 022 | Train Loss: 0.4083 | Val Loss: 0.4367 | Time: 9.73s\r\n",
      "Epoch 023 | Train Loss: 0.4037 | Val Loss: 0.4363 | Time: 9.75s\r\n",
      "Epoch 024 | Train Loss: 0.4059 | Val Loss: 0.4423 | Time: 9.81s\r\n",
      "Epoch 025 | Train Loss: 0.4057 | Val Loss: 0.4491 | Time: 9.86s\r\n",
      "Epoch 026 | Train Loss: 0.4035 | Val Loss: 0.4466 | Time: 9.66s\r\n",
      "Epoch 027 | Train Loss: 0.4011 | Val Loss: 0.4522 | Time: 9.74s\r\n",
      "Epoch 028 | Train Loss: 0.4023 | Val Loss: 0.4439 | Time: 9.74s\r\n",
      "Epoch 029 | Train Loss: 0.3991 | Val Loss: 0.4498 | Time: 9.69s\r\n",
      "Epoch 030 | Train Loss: 0.3965 | Val Loss: 0.4546 | Time: 9.69s\r\n",
      "Epoch 031 | Train Loss: 0.3994 | Val Loss: 0.4548 | Time: 9.91s\r\n",
      "Epoch 032 | Train Loss: 0.3973 | Val Loss: 0.4557 | Time: 9.72s\r\n",
      "Epoch 033 | Train Loss: 0.3943 | Val Loss: 0.4653 | Time: 9.72s\r\n",
      "Epoch 034 | Train Loss: 0.3937 | Val Loss: 0.4661 | Time: 9.81s\r\n",
      "Epoch 035 | Train Loss: 0.3934 | Val Loss: 0.4593 | Time: 9.77s\r\n",
      "\r\n",
      "Early stopping à l'époque 35 (patience: 20)\r\n",
      "✅ Meilleur modèle chargé (époque 15, val_loss: 0.4236)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Dependents          : 0.0549\r\n",
      "   2. MultipleLines       : 0.0543\r\n",
      "   3. OnlineSecurity      : 0.0537\r\n",
      "   4. OnlineBackup        : 0.0532\r\n",
      "   5. Contract            : 0.0531\r\n",
      "   6. StreamingMovies     : 0.0530\r\n",
      "   7. SeniorCitizen       : 0.0529\r\n",
      "   8. StreamingTV         : 0.0528\r\n",
      "   9. TotalCharges        : 0.0528\r\n",
      "  10. PaperlessBilling    : 0.0527\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 19 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Dependents           (CAT): 0.0549\r\n",
      "   2. MultipleLines        (CAT): 0.0543\r\n",
      "   3. OnlineSecurity       (CAT): 0.0537\r\n",
      "   4. OnlineBackup         (CAT): 0.0532\r\n",
      "   5. Contract             (CAT): 0.0531\r\n",
      "   6. StreamingMovies      (CAT): 0.0530\r\n",
      "   7. SeniorCitizen        (CAT): 0.0529\r\n",
      "   8. StreamingTV          (CAT): 0.0528\r\n",
      "   9. TotalCharges         (NUM): 0.0528\r\n",
      "  10. PaperlessBilling     (CAT): 0.0527\r\n",
      "  11. DeviceProtection     (CAT): 0.0526\r\n",
      "  12. TechSupport          (CAT): 0.0526\r\n",
      "  13. PhoneService         (CAT): 0.0525\r\n",
      "  14. Partner              (CAT): 0.0524\r\n",
      "  15. InternetService      (CAT): 0.0522\r\n",
      "  16. gender               (CAT): 0.0518\r\n",
      "  17. PaymentMethod        (CAT): 0.0518\r\n",
      "  18. tenure               (NUM): 0.0509\r\n",
      "  19. MonthlyCharges       (NUM): 0.0499\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure', 'MonthlyCharges'] → indices [2, 0, 1]\r\n",
      "   - Catégorielles sélectionnées: ['Dependents', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'Contract', 'StreamingMovies', 'SeniorCitizen', 'StreamingTV', 'PaperlessBilling', 'DeviceProtection', 'TechSupport', 'PhoneService', 'Partner', 'InternetService', 'gender', 'PaymentMethod'] → indices [3, 5, 7, 8, 13, 12, 1, 11, 14, 9, 10, 4, 2, 6, 0, 15]\r\n",
      "📊 Features sélectionnées: 3 numériques, 16 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 670,849 paramètres\r\n",
      "🔗 Sparsité d'attention: 88.00%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5323 | Val Loss: 0.4983 | Time: 6.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4983)\r\n",
      "Epoch 001 | Train Loss: 0.5133 | Val Loss: 0.4998 | Time: 6.33s\r\n",
      "Epoch 002 | Train Loss: 0.5046 | Val Loss: 0.4912 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4912)\r\n",
      "Epoch 003 | Train Loss: 0.5094 | Val Loss: 0.4824 | Time: 6.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4824)\r\n",
      "Epoch 004 | Train Loss: 0.4837 | Val Loss: 0.4400 | Time: 6.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4400)\r\n",
      "Epoch 005 | Train Loss: 0.4869 | Val Loss: 0.6019 | Time: 6.33s\r\n",
      "Epoch 006 | Train Loss: 0.4996 | Val Loss: 0.4746 | Time: 6.34s\r\n",
      "Epoch 007 | Train Loss: 0.4821 | Val Loss: 0.4789 | Time: 6.29s\r\n",
      "Epoch 008 | Train Loss: 0.4862 | Val Loss: 0.4830 | Time: 6.45s\r\n",
      "Epoch 009 | Train Loss: 0.4716 | Val Loss: 0.4957 | Time: 6.34s\r\n",
      "Epoch 010 | Train Loss: 0.4894 | Val Loss: 0.4898 | Time: 6.28s\r\n",
      "Epoch 011 | Train Loss: 0.4891 | Val Loss: 0.4621 | Time: 6.33s\r\n",
      "Epoch 012 | Train Loss: 0.4765 | Val Loss: 0.4895 | Time: 6.30s\r\n",
      "Epoch 013 | Train Loss: 0.4854 | Val Loss: 0.4938 | Time: 6.51s\r\n",
      "Epoch 014 | Train Loss: 0.4754 | Val Loss: 0.4757 | Time: 6.34s\r\n",
      "Epoch 015 | Train Loss: 0.4709 | Val Loss: 0.4681 | Time: 6.28s\r\n",
      "Epoch 016 | Train Loss: 0.4708 | Val Loss: 0.4728 | Time: 6.31s\r\n",
      "Epoch 017 | Train Loss: 0.4643 | Val Loss: 0.4577 | Time: 6.24s\r\n",
      "Epoch 018 | Train Loss: 0.4646 | Val Loss: 0.4511 | Time: 6.37s\r\n",
      "Epoch 019 | Train Loss: 0.4599 | Val Loss: 0.4717 | Time: 6.37s\r\n",
      "Epoch 020 | Train Loss: 0.4637 | Val Loss: 0.4772 | Time: 6.33s\r\n",
      "Epoch 021 | Train Loss: 0.4605 | Val Loss: 0.4782 | Time: 6.31s\r\n",
      "Epoch 022 | Train Loss: 0.4619 | Val Loss: 0.4578 | Time: 6.27s\r\n",
      "Epoch 023 | Train Loss: 0.4535 | Val Loss: 0.4507 | Time: 6.32s\r\n",
      "Epoch 024 | Train Loss: 0.4527 | Val Loss: 0.4526 | Time: 6.36s\r\n",
      "\r\n",
      "Early stopping à l'époque 24 (patience: 20)\r\n",
      "✅ Meilleur modèle Random chargé (époque 4, val_loss: 0.4400)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.1339\r\n",
      "   2. OnlineSecurity       (CAT): 0.1208\r\n",
      "   3. gender               (CAT): 0.1185\r\n",
      "   4. Partner              (CAT): 0.1117\r\n",
      "   5. TotalCharges         (NUM): 0.1004\r\n",
      "   6. DeviceProtection     (CAT): 0.0936\r\n",
      "   7. InternetService      (CAT): 0.0789\r\n",
      "   8. OnlineBackup         (CAT): 0.0682\r\n",
      "   9. MultipleLines        (CAT): 0.0505\r\n",
      "  10. PaymentMethod        (CAT): 0.0384\r\n",
      "  11. PaperlessBilling     (CAT): 0.0287\r\n",
      "  12. tenure               (NUM): 0.0259\r\n",
      "  13. Contract             (CAT): 0.0177\r\n",
      "  14. TechSupport          (CAT): 0.0120\r\n",
      "  15. StreamingTV          (CAT): 0.0002\r\n",
      "  16. MonthlyCharges       (NUM): 0.0002\r\n",
      "  17. Dependents           (CAT): 0.0002\r\n",
      "  18. StreamingMovies      (CAT): 0.0002\r\n",
      "  19. PhoneService         (CAT): 0.0000\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.1339\r\n",
      "   2. OnlineSecurity      : 0.1208\r\n",
      "   3. gender              : 0.1185\r\n",
      "   4. Partner             : 0.1117\r\n",
      "   5. TotalCharges        : 0.1004\r\n",
      "   6. DeviceProtection    : 0.0936\r\n",
      "   7. InternetService     : 0.0789\r\n",
      "   8. OnlineBackup        : 0.0682\r\n",
      "   9. MultipleLines       : 0.0505\r\n",
      "  10. PaymentMethod       : 0.0384\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_11/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 513.5s ===\r\n",
      "\u001b[32m[I 2025-07-19 21:47:53,517]\u001b[0m Trial 11 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 6, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.18691930351405206, 'ffn_dropout_stage1': 0.1476807801017457, 'residual_dropout_stage1': 0.17808678962678703, 'lr_stage1': 0.00020061341783572832, 'weight_decay_stage1': 1.5918609152859145e-05, 'd_token_stage2': 128, 'n_blocks_stage2': 4, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.10262806314356165, 'ffn_dropout_stage2': 0.16299592906959046, 'residual_dropout_stage2': 0.10200757447628817, 'lr_stage2': 0.008733880332634197, 'weight_decay_stage2': 0.004179398074616643, 'batch_size': 32, 'patience': 20, 'embedding_type': 'P-LR', 'M': 19, 'k': 5}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  48%|███▎   | 12/25 [3:34:56<3:54:25, 1081.94s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR-LR\r\n",
      "Modèle FTT+ créé avec 285,201 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4815 | Val Loss: 0.4483 | Time: 6.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4483)\r\n",
      "Epoch 001 | Train Loss: 0.4413 | Val Loss: 0.4419 | Time: 6.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "Epoch 002 | Train Loss: 0.4341 | Val Loss: 0.4417 | Time: 6.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 003 | Train Loss: 0.4282 | Val Loss: 0.4287 | Time: 6.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4287)\r\n",
      "Epoch 004 | Train Loss: 0.4261 | Val Loss: 0.4257 | Time: 6.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4257)\r\n",
      "Epoch 005 | Train Loss: 0.4230 | Val Loss: 0.4342 | Time: 6.63s\r\n",
      "Epoch 006 | Train Loss: 0.4217 | Val Loss: 0.4159 | Time: 6.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 007 | Train Loss: 0.4180 | Val Loss: 0.4274 | Time: 6.71s\r\n",
      "Epoch 008 | Train Loss: 0.4189 | Val Loss: 0.4195 | Time: 6.58s\r\n",
      "Epoch 009 | Train Loss: 0.4165 | Val Loss: 0.4190 | Time: 6.64s\r\n",
      "Epoch 010 | Train Loss: 0.4161 | Val Loss: 0.4183 | Time: 6.61s\r\n",
      "Epoch 011 | Train Loss: 0.4131 | Val Loss: 0.4188 | Time: 6.59s\r\n",
      "Epoch 012 | Train Loss: 0.4160 | Val Loss: 0.4184 | Time: 6.73s\r\n",
      "Epoch 013 | Train Loss: 0.4131 | Val Loss: 0.4176 | Time: 6.57s\r\n",
      "Epoch 014 | Train Loss: 0.4122 | Val Loss: 0.4220 | Time: 6.58s\r\n",
      "Epoch 015 | Train Loss: 0.4135 | Val Loss: 0.4203 | Time: 6.61s\r\n",
      "Epoch 016 | Train Loss: 0.4106 | Val Loss: 0.4162 | Time: 6.67s\r\n",
      "Epoch 017 | Train Loss: 0.4081 | Val Loss: 0.4219 | Time: 6.48s\r\n",
      "Epoch 018 | Train Loss: 0.4079 | Val Loss: 0.4169 | Time: 6.58s\r\n",
      "Epoch 019 | Train Loss: 0.4083 | Val Loss: 0.4156 | Time: 6.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4156)\r\n",
      "Epoch 020 | Train Loss: 0.4068 | Val Loss: 0.4177 | Time: 6.59s\r\n",
      "Epoch 021 | Train Loss: 0.4027 | Val Loss: 0.4164 | Time: 6.84s\r\n",
      "Epoch 022 | Train Loss: 0.4046 | Val Loss: 0.4165 | Time: 6.62s\r\n",
      "Epoch 023 | Train Loss: 0.4014 | Val Loss: 0.4377 | Time: 6.58s\r\n",
      "Epoch 024 | Train Loss: 0.3988 | Val Loss: 0.4303 | Time: 6.66s\r\n",
      "Epoch 025 | Train Loss: 0.4021 | Val Loss: 0.4214 | Time: 6.63s\r\n",
      "Epoch 026 | Train Loss: 0.4042 | Val Loss: 0.4269 | Time: 6.78s\r\n",
      "Epoch 027 | Train Loss: 0.3993 | Val Loss: 0.4329 | Time: 6.56s\r\n",
      "Epoch 028 | Train Loss: 0.3994 | Val Loss: 0.4266 | Time: 6.62s\r\n",
      "Epoch 029 | Train Loss: 0.3972 | Val Loss: 0.4290 | Time: 6.54s\r\n",
      "Epoch 030 | Train Loss: 0.3966 | Val Loss: 0.4380 | Time: 6.70s\r\n",
      "Epoch 031 | Train Loss: 0.3978 | Val Loss: 0.4388 | Time: 6.70s\r\n",
      "Epoch 032 | Train Loss: 0.3961 | Val Loss: 0.4240 | Time: 6.55s\r\n",
      "Epoch 033 | Train Loss: 0.3964 | Val Loss: 0.4383 | Time: 6.60s\r\n",
      "Epoch 034 | Train Loss: 0.3927 | Val Loss: 0.4509 | Time: 6.61s\r\n",
      "Epoch 035 | Train Loss: 0.3914 | Val Loss: 0.4452 | Time: 6.62s\r\n",
      "Epoch 036 | Train Loss: 0.3922 | Val Loss: 0.4368 | Time: 6.61s\r\n",
      "Epoch 037 | Train Loss: 0.3901 | Val Loss: 0.4439 | Time: 6.57s\r\n",
      "\r\n",
      "Early stopping à l'époque 37 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 19, val_loss: 0.4156)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.0598\r\n",
      "   2. Contract            : 0.0592\r\n",
      "   3. OnlineSecurity      : 0.0566\r\n",
      "   4. TotalCharges        : 0.0566\r\n",
      "   5. DeviceProtection    : 0.0565\r\n",
      "   6. Dependents          : 0.0557\r\n",
      "   7. MonthlyCharges      : 0.0551\r\n",
      "   8. PhoneService        : 0.0544\r\n",
      "   9. PaymentMethod       : 0.0543\r\n",
      "  10. InternetService     : 0.0531\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 16 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. tenure               (NUM): 0.0598\r\n",
      "   2. Contract             (CAT): 0.0592\r\n",
      "   3. OnlineSecurity       (CAT): 0.0566\r\n",
      "   4. TotalCharges         (NUM): 0.0566\r\n",
      "   5. DeviceProtection     (CAT): 0.0565\r\n",
      "   6. Dependents           (CAT): 0.0557\r\n",
      "   7. MonthlyCharges       (NUM): 0.0551\r\n",
      "   8. PhoneService         (CAT): 0.0544\r\n",
      "   9. PaymentMethod        (CAT): 0.0543\r\n",
      "  10. InternetService      (CAT): 0.0531\r\n",
      "  11. gender               (CAT): 0.0525\r\n",
      "  12. OnlineBackup         (CAT): 0.0516\r\n",
      "  13. PaperlessBilling     (CAT): 0.0498\r\n",
      "  14. MultipleLines        (CAT): 0.0491\r\n",
      "  15. Partner              (CAT): 0.0484\r\n",
      "  16. StreamingMovies      (CAT): 0.0479\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['Contract', 'OnlineSecurity', 'DeviceProtection', 'Dependents', 'PhoneService', 'PaymentMethod', 'InternetService', 'gender', 'OnlineBackup', 'PaperlessBilling', 'MultipleLines', 'Partner', 'StreamingMovies'] → indices [13, 7, 9, 3, 4, 15, 6, 0, 8, 14, 5, 2, 12]\r\n",
      "📊 Features sélectionnées: 3 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 834,945 paramètres\r\n",
      "🔗 Sparsité d'attention: 84.08%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4576 | Val Loss: 0.4431 | Time: 7.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4431)\r\n",
      "Epoch 001 | Train Loss: 0.4417 | Val Loss: 0.4364 | Time: 7.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 002 | Train Loss: 0.4354 | Val Loss: 0.4378 | Time: 7.83s\r\n",
      "Epoch 003 | Train Loss: 0.4308 | Val Loss: 0.4368 | Time: 7.80s\r\n",
      "Epoch 004 | Train Loss: 0.4307 | Val Loss: 0.4312 | Time: 7.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4312)\r\n",
      "Epoch 005 | Train Loss: 0.4329 | Val Loss: 0.4346 | Time: 7.78s\r\n",
      "Epoch 006 | Train Loss: 0.4250 | Val Loss: 0.4300 | Time: 7.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4300)\r\n",
      "Epoch 007 | Train Loss: 0.4270 | Val Loss: 0.4248 | Time: 7.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4248)\r\n",
      "Epoch 008 | Train Loss: 0.4269 | Val Loss: 0.4219 | Time: 7.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4219)\r\n",
      "Epoch 009 | Train Loss: 0.4285 | Val Loss: 0.4258 | Time: 7.78s\r\n",
      "Epoch 010 | Train Loss: 0.4246 | Val Loss: 0.4290 | Time: 7.82s\r\n",
      "Epoch 011 | Train Loss: 0.4223 | Val Loss: 0.4265 | Time: 7.76s\r\n",
      "Epoch 012 | Train Loss: 0.4251 | Val Loss: 0.4214 | Time: 7.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 013 | Train Loss: 0.4241 | Val Loss: 0.4271 | Time: 7.82s\r\n",
      "Epoch 014 | Train Loss: 0.4222 | Val Loss: 0.4266 | Time: 8.02s\r\n",
      "Epoch 015 | Train Loss: 0.4261 | Val Loss: 0.4278 | Time: 7.80s\r\n",
      "Epoch 016 | Train Loss: 0.4222 | Val Loss: 0.4316 | Time: 7.81s\r\n",
      "Epoch 017 | Train Loss: 0.4225 | Val Loss: 0.4277 | Time: 7.82s\r\n",
      "Epoch 018 | Train Loss: 0.4235 | Val Loss: 0.4271 | Time: 7.78s\r\n",
      "Epoch 019 | Train Loss: 0.4207 | Val Loss: 0.4255 | Time: 7.75s\r\n",
      "Epoch 020 | Train Loss: 0.4243 | Val Loss: 0.4235 | Time: 7.79s\r\n",
      "Epoch 021 | Train Loss: 0.4214 | Val Loss: 0.4195 | Time: 7.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4195)\r\n",
      "Epoch 022 | Train Loss: 0.4225 | Val Loss: 0.4203 | Time: 7.89s\r\n",
      "Epoch 023 | Train Loss: 0.4244 | Val Loss: 0.4254 | Time: 7.75s\r\n",
      "Epoch 024 | Train Loss: 0.4227 | Val Loss: 0.4268 | Time: 7.80s\r\n",
      "Epoch 025 | Train Loss: 0.4215 | Val Loss: 0.4256 | Time: 7.80s\r\n",
      "Epoch 026 | Train Loss: 0.4220 | Val Loss: 0.4226 | Time: 7.82s\r\n",
      "Epoch 027 | Train Loss: 0.4182 | Val Loss: 0.4283 | Time: 7.79s\r\n",
      "Epoch 028 | Train Loss: 0.4191 | Val Loss: 0.4261 | Time: 7.85s\r\n",
      "Epoch 029 | Train Loss: 0.4219 | Val Loss: 0.4210 | Time: 7.86s\r\n",
      "Epoch 030 | Train Loss: 0.4194 | Val Loss: 0.4191 | Time: 7.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 031 | Train Loss: 0.4204 | Val Loss: 0.4252 | Time: 7.77s\r\n",
      "Epoch 032 | Train Loss: 0.4175 | Val Loss: 0.4231 | Time: 7.78s\r\n",
      "Epoch 033 | Train Loss: 0.4206 | Val Loss: 0.4212 | Time: 7.74s\r\n",
      "Epoch 034 | Train Loss: 0.4180 | Val Loss: 0.4252 | Time: 7.90s\r\n",
      "Epoch 035 | Train Loss: 0.4172 | Val Loss: 0.4255 | Time: 7.75s\r\n",
      "Epoch 036 | Train Loss: 0.4202 | Val Loss: 0.4222 | Time: 7.73s\r\n",
      "Epoch 037 | Train Loss: 0.4175 | Val Loss: 0.4209 | Time: 7.79s\r\n",
      "Epoch 038 | Train Loss: 0.4192 | Val Loss: 0.4227 | Time: 7.90s\r\n",
      "Epoch 039 | Train Loss: 0.4182 | Val Loss: 0.4190 | Time: 7.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 040 | Train Loss: 0.4174 | Val Loss: 0.4146 | Time: 7.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 041 | Train Loss: 0.4177 | Val Loss: 0.4227 | Time: 7.80s\r\n",
      "Epoch 042 | Train Loss: 0.4138 | Val Loss: 0.4243 | Time: 7.91s\r\n",
      "Epoch 043 | Train Loss: 0.4189 | Val Loss: 0.4166 | Time: 7.81s\r\n",
      "Epoch 044 | Train Loss: 0.4191 | Val Loss: 0.4172 | Time: 7.80s\r\n",
      "Epoch 045 | Train Loss: 0.4183 | Val Loss: 0.4201 | Time: 7.91s\r\n",
      "Epoch 046 | Train Loss: 0.4160 | Val Loss: 0.4219 | Time: 7.86s\r\n",
      "Epoch 047 | Train Loss: 0.4167 | Val Loss: 0.4243 | Time: 7.78s\r\n",
      "Epoch 048 | Train Loss: 0.4166 | Val Loss: 0.4304 | Time: 7.85s\r\n",
      "Epoch 049 | Train Loss: 0.4161 | Val Loss: 0.4161 | Time: 7.75s\r\n",
      "✅ Meilleur modèle Random chargé (époque 40, val_loss: 0.4146)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. DeviceProtection     (CAT): 0.1053\r\n",
      "   2. InternetService      (CAT): 0.0832\r\n",
      "   3. OnlineSecurity       (CAT): 0.0822\r\n",
      "   4. tenure               (NUM): 0.0710\r\n",
      "   5. MonthlyCharges       (NUM): 0.0694\r\n",
      "   6. StreamingMovies      (CAT): 0.0694\r\n",
      "   7. gender               (CAT): 0.0648\r\n",
      "   8. Contract             (CAT): 0.0609\r\n",
      "   9. Partner              (CAT): 0.0600\r\n",
      "  10. MultipleLines        (CAT): 0.0525\r\n",
      "  11. PhoneService         (CAT): 0.0511\r\n",
      "  12. Dependents           (CAT): 0.0506\r\n",
      "  13. PaymentMethod        (CAT): 0.0463\r\n",
      "  14. TotalCharges         (NUM): 0.0461\r\n",
      "  15. OnlineBackup         (CAT): 0.0440\r\n",
      "  16. PaperlessBilling     (CAT): 0.0430\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.1053\r\n",
      "   2. InternetService     : 0.0832\r\n",
      "   3. OnlineSecurity      : 0.0822\r\n",
      "   4. tenure              : 0.0710\r\n",
      "   5. MonthlyCharges      : 0.0694\r\n",
      "   6. StreamingMovies     : 0.0694\r\n",
      "   7. gender              : 0.0648\r\n",
      "   8. Contract            : 0.0609\r\n",
      "   9. Partner             : 0.0600\r\n",
      "  10. MultipleLines       : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 646.2s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR-LR\r\n",
      "Modèle FTT+ créé avec 285,201 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4860 | Val Loss: 0.4416 | Time: 6.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4416)\r\n",
      "Epoch 001 | Train Loss: 0.4363 | Val Loss: 0.4291 | Time: 6.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4291)\r\n",
      "Epoch 002 | Train Loss: 0.4251 | Val Loss: 0.4227 | Time: 6.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 003 | Train Loss: 0.4228 | Val Loss: 0.4189 | Time: 6.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4189)\r\n",
      "Epoch 004 | Train Loss: 0.4152 | Val Loss: 0.4184 | Time: 6.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 005 | Train Loss: 0.4160 | Val Loss: 0.4195 | Time: 6.86s\r\n",
      "Epoch 006 | Train Loss: 0.4122 | Val Loss: 0.4176 | Time: 6.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4176)\r\n",
      "Epoch 007 | Train Loss: 0.4122 | Val Loss: 0.4218 | Time: 6.57s\r\n",
      "Epoch 008 | Train Loss: 0.4095 | Val Loss: 0.4197 | Time: 6.62s\r\n",
      "Epoch 009 | Train Loss: 0.4068 | Val Loss: 0.4208 | Time: 6.65s\r\n",
      "Epoch 010 | Train Loss: 0.4052 | Val Loss: 0.4194 | Time: 6.64s\r\n",
      "Epoch 011 | Train Loss: 0.4065 | Val Loss: 0.4225 | Time: 6.59s\r\n",
      "Epoch 012 | Train Loss: 0.4030 | Val Loss: 0.4223 | Time: 6.62s\r\n",
      "Epoch 013 | Train Loss: 0.4015 | Val Loss: 0.4212 | Time: 6.59s\r\n",
      "Epoch 014 | Train Loss: 0.4005 | Val Loss: 0.4185 | Time: 6.67s\r\n",
      "Epoch 015 | Train Loss: 0.4009 | Val Loss: 0.4248 | Time: 6.67s\r\n",
      "Epoch 016 | Train Loss: 0.3975 | Val Loss: 0.4198 | Time: 6.58s\r\n",
      "Epoch 017 | Train Loss: 0.3955 | Val Loss: 0.4223 | Time: 6.61s\r\n",
      "Epoch 018 | Train Loss: 0.3957 | Val Loss: 0.4213 | Time: 6.60s\r\n",
      "Epoch 019 | Train Loss: 0.3900 | Val Loss: 0.4234 | Time: 6.60s\r\n",
      "Epoch 020 | Train Loss: 0.3908 | Val Loss: 0.4202 | Time: 6.65s\r\n",
      "Epoch 021 | Train Loss: 0.3903 | Val Loss: 0.4257 | Time: 6.70s\r\n",
      "Epoch 022 | Train Loss: 0.3902 | Val Loss: 0.4313 | Time: 6.66s\r\n",
      "Epoch 023 | Train Loss: 0.3887 | Val Loss: 0.4226 | Time: 6.64s\r\n",
      "Epoch 024 | Train Loss: 0.3852 | Val Loss: 0.4265 | Time: 6.78s\r\n",
      "\r\n",
      "Early stopping à l'époque 24 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 6, val_loss: 0.4176)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.0579\r\n",
      "   2. OnlineSecurity      : 0.0568\r\n",
      "   3. MultipleLines       : 0.0548\r\n",
      "   4. TechSupport         : 0.0539\r\n",
      "   5. OnlineBackup        : 0.0532\r\n",
      "   6. StreamingTV         : 0.0531\r\n",
      "   7. MonthlyCharges      : 0.0530\r\n",
      "   8. StreamingMovies     : 0.0529\r\n",
      "   9. Contract            : 0.0525\r\n",
      "  10. SeniorCitizen       : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 16 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. DeviceProtection     (CAT): 0.0579\r\n",
      "   2. OnlineSecurity       (CAT): 0.0568\r\n",
      "   3. MultipleLines        (CAT): 0.0548\r\n",
      "   4. TechSupport          (CAT): 0.0539\r\n",
      "   5. OnlineBackup         (CAT): 0.0532\r\n",
      "   6. StreamingTV          (CAT): 0.0531\r\n",
      "   7. MonthlyCharges       (NUM): 0.0530\r\n",
      "   8. StreamingMovies      (CAT): 0.0529\r\n",
      "   9. Contract             (CAT): 0.0525\r\n",
      "  10. SeniorCitizen        (CAT): 0.0525\r\n",
      "  11. Partner              (CAT): 0.0525\r\n",
      "  12. tenure               (NUM): 0.0522\r\n",
      "  13. PhoneService         (CAT): 0.0516\r\n",
      "  14. PaymentMethod        (CAT): 0.0511\r\n",
      "  15. Dependents           (CAT): 0.0510\r\n",
      "  16. TotalCharges         (NUM): 0.0510\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure', 'TotalCharges'] → indices [1, 0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['DeviceProtection', 'OnlineSecurity', 'MultipleLines', 'TechSupport', 'OnlineBackup', 'StreamingTV', 'StreamingMovies', 'Contract', 'SeniorCitizen', 'Partner', 'PhoneService', 'PaymentMethod', 'Dependents'] → indices [9, 7, 5, 10, 8, 11, 12, 13, 1, 2, 4, 15, 3]\r\n",
      "📊 Features sélectionnées: 3 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 835,073 paramètres\r\n",
      "🔗 Sparsité d'attention: 84.08%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4678 | Val Loss: 0.4381 | Time: 7.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4381)\r\n",
      "Epoch 001 | Train Loss: 0.4412 | Val Loss: 0.4259 | Time: 7.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4259)\r\n",
      "Epoch 002 | Train Loss: 0.4336 | Val Loss: 0.4252 | Time: 7.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4252)\r\n",
      "Epoch 003 | Train Loss: 0.4298 | Val Loss: 0.4238 | Time: 7.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4238)\r\n",
      "Epoch 004 | Train Loss: 0.4318 | Val Loss: 0.4242 | Time: 7.88s\r\n",
      "Epoch 005 | Train Loss: 0.4260 | Val Loss: 0.4216 | Time: 7.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 006 | Train Loss: 0.4231 | Val Loss: 0.4187 | Time: 7.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4187)\r\n",
      "Epoch 007 | Train Loss: 0.4262 | Val Loss: 0.4211 | Time: 7.93s\r\n",
      "Epoch 008 | Train Loss: 0.4223 | Val Loss: 0.4207 | Time: 7.81s\r\n",
      "Epoch 009 | Train Loss: 0.4208 | Val Loss: 0.4166 | Time: 7.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4166)\r\n",
      "Epoch 010 | Train Loss: 0.4215 | Val Loss: 0.4184 | Time: 7.85s\r\n",
      "Epoch 011 | Train Loss: 0.4217 | Val Loss: 0.4158 | Time: 7.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4158)\r\n",
      "Epoch 012 | Train Loss: 0.4159 | Val Loss: 0.4180 | Time: 7.81s\r\n",
      "Epoch 013 | Train Loss: 0.4191 | Val Loss: 0.4268 | Time: 7.79s\r\n",
      "Epoch 014 | Train Loss: 0.4222 | Val Loss: 0.4200 | Time: 7.84s\r\n",
      "Epoch 015 | Train Loss: 0.4213 | Val Loss: 0.4215 | Time: 7.88s\r\n",
      "Epoch 016 | Train Loss: 0.4191 | Val Loss: 0.4156 | Time: 7.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4156)\r\n",
      "Epoch 017 | Train Loss: 0.4135 | Val Loss: 0.4258 | Time: 7.83s\r\n",
      "Epoch 018 | Train Loss: 0.4171 | Val Loss: 0.4174 | Time: 7.84s\r\n",
      "Epoch 019 | Train Loss: 0.4145 | Val Loss: 0.4131 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4131)\r\n",
      "Epoch 020 | Train Loss: 0.4151 | Val Loss: 0.4180 | Time: 7.81s\r\n",
      "Epoch 021 | Train Loss: 0.4188 | Val Loss: 0.4173 | Time: 7.83s\r\n",
      "Epoch 022 | Train Loss: 0.4157 | Val Loss: 0.4157 | Time: 7.88s\r\n",
      "Epoch 023 | Train Loss: 0.4159 | Val Loss: 0.4145 | Time: 8.01s\r\n",
      "Epoch 024 | Train Loss: 0.4182 | Val Loss: 0.4170 | Time: 7.86s\r\n",
      "Epoch 025 | Train Loss: 0.4165 | Val Loss: 0.4181 | Time: 7.88s\r\n",
      "Epoch 026 | Train Loss: 0.4159 | Val Loss: 0.4169 | Time: 7.88s\r\n",
      "Epoch 027 | Train Loss: 0.4187 | Val Loss: 0.4153 | Time: 8.04s\r\n",
      "Epoch 028 | Train Loss: 0.4197 | Val Loss: 0.4157 | Time: 7.81s\r\n",
      "Epoch 029 | Train Loss: 0.4163 | Val Loss: 0.4148 | Time: 7.88s\r\n",
      "Epoch 030 | Train Loss: 0.4149 | Val Loss: 0.4140 | Time: 7.89s\r\n",
      "Epoch 031 | Train Loss: 0.4153 | Val Loss: 0.4244 | Time: 7.91s\r\n",
      "Epoch 032 | Train Loss: 0.4145 | Val Loss: 0.4233 | Time: 7.78s\r\n",
      "Epoch 033 | Train Loss: 0.4139 | Val Loss: 0.4211 | Time: 7.81s\r\n",
      "Epoch 034 | Train Loss: 0.4147 | Val Loss: 0.4169 | Time: 7.90s\r\n",
      "Epoch 035 | Train Loss: 0.4157 | Val Loss: 0.4231 | Time: 7.98s\r\n",
      "Epoch 036 | Train Loss: 0.4168 | Val Loss: 0.4167 | Time: 7.69s\r\n",
      "Epoch 037 | Train Loss: 0.4180 | Val Loss: 0.4191 | Time: 7.77s\r\n",
      "\r\n",
      "Early stopping à l'époque 37 (patience: 18)\r\n",
      "✅ Meilleur modèle Random chargé (époque 19, val_loss: 0.4131)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. DeviceProtection     (CAT): 0.0986\r\n",
      "   2. SeniorCitizen        (CAT): 0.0811\r\n",
      "   3. MultipleLines        (CAT): 0.0761\r\n",
      "   4. OnlineSecurity       (CAT): 0.0714\r\n",
      "   5. MonthlyCharges       (NUM): 0.0681\r\n",
      "   6. Partner              (CAT): 0.0655\r\n",
      "   7. Dependents           (CAT): 0.0618\r\n",
      "   8. tenure               (NUM): 0.0603\r\n",
      "   9. StreamingMovies      (CAT): 0.0593\r\n",
      "  10. TotalCharges         (NUM): 0.0559\r\n",
      "  11. TechSupport          (CAT): 0.0537\r\n",
      "  12. OnlineBackup         (CAT): 0.0515\r\n",
      "  13. PaymentMethod        (CAT): 0.0506\r\n",
      "  14. PhoneService         (CAT): 0.0497\r\n",
      "  15. Contract             (CAT): 0.0488\r\n",
      "  16. StreamingTV          (CAT): 0.0477\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.0986\r\n",
      "   2. SeniorCitizen       : 0.0811\r\n",
      "   3. MultipleLines       : 0.0761\r\n",
      "   4. OnlineSecurity      : 0.0714\r\n",
      "   5. MonthlyCharges      : 0.0681\r\n",
      "   6. Partner             : 0.0655\r\n",
      "   7. Dependents          : 0.0618\r\n",
      "   8. tenure              : 0.0603\r\n",
      "   9. StreamingMovies     : 0.0593\r\n",
      "  10. TotalCharges        : 0.0559\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 468.2s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR-LR\r\n",
      "Modèle FTT+ créé avec 285,201 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4817 | Val Loss: 0.4352 | Time: 6.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4352)\r\n",
      "Epoch 001 | Train Loss: 0.4392 | Val Loss: 0.4208 | Time: 6.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4208)\r\n",
      "Epoch 002 | Train Loss: 0.4318 | Val Loss: 0.4263 | Time: 6.51s\r\n",
      "Epoch 003 | Train Loss: 0.4303 | Val Loss: 0.4277 | Time: 6.50s\r\n",
      "Epoch 004 | Train Loss: 0.4274 | Val Loss: 0.4250 | Time: 6.48s\r\n",
      "Epoch 005 | Train Loss: 0.4252 | Val Loss: 0.4258 | Time: 6.53s\r\n",
      "Epoch 006 | Train Loss: 0.4238 | Val Loss: 0.4272 | Time: 6.52s\r\n",
      "Epoch 007 | Train Loss: 0.4225 | Val Loss: 0.4274 | Time: 6.50s\r\n",
      "Epoch 008 | Train Loss: 0.4213 | Val Loss: 0.4271 | Time: 6.52s\r\n",
      "Epoch 009 | Train Loss: 0.4187 | Val Loss: 0.4331 | Time: 6.49s\r\n",
      "Epoch 010 | Train Loss: 0.4182 | Val Loss: 0.4333 | Time: 6.50s\r\n",
      "Epoch 011 | Train Loss: 0.4164 | Val Loss: 0.4254 | Time: 6.58s\r\n",
      "Epoch 012 | Train Loss: 0.4151 | Val Loss: 0.4342 | Time: 6.39s\r\n",
      "Epoch 013 | Train Loss: 0.4149 | Val Loss: 0.4315 | Time: 6.46s\r\n",
      "Epoch 014 | Train Loss: 0.4137 | Val Loss: 0.4302 | Time: 6.48s\r\n",
      "Epoch 015 | Train Loss: 0.4117 | Val Loss: 0.4370 | Time: 6.44s\r\n",
      "Epoch 016 | Train Loss: 0.4106 | Val Loss: 0.4415 | Time: 6.62s\r\n",
      "Epoch 017 | Train Loss: 0.4095 | Val Loss: 0.4294 | Time: 6.46s\r\n",
      "Epoch 018 | Train Loss: 0.4064 | Val Loss: 0.4392 | Time: 6.44s\r\n",
      "Epoch 019 | Train Loss: 0.4064 | Val Loss: 0.4336 | Time: 6.48s\r\n",
      "\r\n",
      "Early stopping à l'époque 19 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 1, val_loss: 0.4208)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TechSupport         : 0.0683\r\n",
      "   2. PaperlessBilling    : 0.0638\r\n",
      "   3. SeniorCitizen       : 0.0627\r\n",
      "   4. MultipleLines       : 0.0584\r\n",
      "   5. TotalCharges        : 0.0560\r\n",
      "   6. StreamingTV         : 0.0540\r\n",
      "   7. InternetService     : 0.0532\r\n",
      "   8. PaymentMethod       : 0.0531\r\n",
      "   9. Dependents          : 0.0517\r\n",
      "  10. OnlineBackup        : 0.0513\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 16 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. TechSupport          (CAT): 0.0683\r\n",
      "   2. PaperlessBilling     (CAT): 0.0638\r\n",
      "   3. SeniorCitizen        (CAT): 0.0627\r\n",
      "   4. MultipleLines        (CAT): 0.0584\r\n",
      "   5. TotalCharges         (NUM): 0.0560\r\n",
      "   6. StreamingTV          (CAT): 0.0540\r\n",
      "   7. InternetService      (CAT): 0.0532\r\n",
      "   8. PaymentMethod        (CAT): 0.0531\r\n",
      "   9. Dependents           (CAT): 0.0517\r\n",
      "  10. OnlineBackup         (CAT): 0.0513\r\n",
      "  11. gender               (CAT): 0.0503\r\n",
      "  12. DeviceProtection     (CAT): 0.0485\r\n",
      "  13. PhoneService         (CAT): 0.0480\r\n",
      "  14. OnlineSecurity       (CAT): 0.0473\r\n",
      "  15. tenure               (NUM): 0.0471\r\n",
      "  16. Partner              (CAT): 0.0469\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure'] → indices [2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['TechSupport', 'PaperlessBilling', 'SeniorCitizen', 'MultipleLines', 'StreamingTV', 'InternetService', 'PaymentMethod', 'Dependents', 'OnlineBackup', 'gender', 'DeviceProtection', 'PhoneService', 'OnlineSecurity', 'Partner'] → indices [10, 14, 1, 5, 11, 6, 15, 3, 8, 0, 9, 4, 7, 2]\r\n",
      "📊 Features sélectionnées: 2 numériques, 14 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 835,073 paramètres\r\n",
      "🔗 Sparsité d'attention: 84.08%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4726 | Val Loss: 0.4742 | Time: 7.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4742)\r\n",
      "Epoch 001 | Train Loss: 0.4561 | Val Loss: 0.4537 | Time: 7.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4537)\r\n",
      "Epoch 002 | Train Loss: 0.4533 | Val Loss: 0.4472 | Time: 7.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4472)\r\n",
      "Epoch 003 | Train Loss: 0.4478 | Val Loss: 0.4695 | Time: 7.62s\r\n",
      "Epoch 004 | Train Loss: 0.4469 | Val Loss: 0.4423 | Time: 7.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4423)\r\n",
      "Epoch 005 | Train Loss: 0.4424 | Val Loss: 0.4349 | Time: 7.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4349)\r\n",
      "Epoch 006 | Train Loss: 0.4421 | Val Loss: 0.4367 | Time: 7.63s\r\n",
      "Epoch 007 | Train Loss: 0.4404 | Val Loss: 0.4413 | Time: 7.59s\r\n",
      "Epoch 008 | Train Loss: 0.4397 | Val Loss: 0.4346 | Time: 7.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4346)\r\n",
      "Epoch 009 | Train Loss: 0.4379 | Val Loss: 0.4342 | Time: 7.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 010 | Train Loss: 0.4368 | Val Loss: 0.4340 | Time: 7.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4340)\r\n",
      "Epoch 011 | Train Loss: 0.4359 | Val Loss: 0.4317 | Time: 7.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 012 | Train Loss: 0.4317 | Val Loss: 0.4339 | Time: 7.64s\r\n",
      "Epoch 013 | Train Loss: 0.4338 | Val Loss: 0.4366 | Time: 7.80s\r\n",
      "Epoch 014 | Train Loss: 0.4314 | Val Loss: 0.4332 | Time: 7.64s\r\n",
      "Epoch 015 | Train Loss: 0.4358 | Val Loss: 0.4314 | Time: 7.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4314)\r\n",
      "Epoch 016 | Train Loss: 0.4331 | Val Loss: 0.4274 | Time: 7.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4274)\r\n",
      "Epoch 017 | Train Loss: 0.4307 | Val Loss: 0.4305 | Time: 7.90s\r\n",
      "Epoch 018 | Train Loss: 0.4325 | Val Loss: 0.4261 | Time: 7.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4261)\r\n",
      "Epoch 019 | Train Loss: 0.4282 | Val Loss: 0.4280 | Time: 7.63s\r\n",
      "Epoch 020 | Train Loss: 0.4257 | Val Loss: 0.4333 | Time: 7.69s\r\n",
      "Epoch 021 | Train Loss: 0.4312 | Val Loss: 0.4340 | Time: 7.97s\r\n",
      "Epoch 022 | Train Loss: 0.4295 | Val Loss: 0.4340 | Time: 7.77s\r\n",
      "Epoch 023 | Train Loss: 0.4331 | Val Loss: 0.4287 | Time: 7.62s\r\n",
      "Epoch 024 | Train Loss: 0.4258 | Val Loss: 0.4354 | Time: 7.66s\r\n",
      "Epoch 025 | Train Loss: 0.4318 | Val Loss: 0.4345 | Time: 7.89s\r\n",
      "Epoch 026 | Train Loss: 0.4318 | Val Loss: 0.4289 | Time: 7.56s\r\n",
      "Epoch 027 | Train Loss: 0.4325 | Val Loss: 0.4267 | Time: 7.53s\r\n",
      "Epoch 028 | Train Loss: 0.4301 | Val Loss: 0.4305 | Time: 7.60s\r\n",
      "Epoch 029 | Train Loss: 0.4296 | Val Loss: 0.4248 | Time: 7.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4248)\r\n",
      "Epoch 030 | Train Loss: 0.4277 | Val Loss: 0.4288 | Time: 7.67s\r\n",
      "Epoch 031 | Train Loss: 0.4302 | Val Loss: 0.4339 | Time: 7.52s\r\n",
      "Epoch 032 | Train Loss: 0.4317 | Val Loss: 0.4292 | Time: 7.68s\r\n",
      "Epoch 033 | Train Loss: 0.4290 | Val Loss: 0.4313 | Time: 7.75s\r\n",
      "Epoch 034 | Train Loss: 0.4274 | Val Loss: 0.4207 | Time: 7.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4207)\r\n",
      "Epoch 035 | Train Loss: 0.4288 | Val Loss: 0.4237 | Time: 7.66s\r\n",
      "Epoch 036 | Train Loss: 0.4308 | Val Loss: 0.4247 | Time: 7.60s\r\n",
      "Epoch 037 | Train Loss: 0.4280 | Val Loss: 0.4254 | Time: 7.65s\r\n",
      "Epoch 038 | Train Loss: 0.4291 | Val Loss: 0.4271 | Time: 7.69s\r\n",
      "Epoch 039 | Train Loss: 0.4316 | Val Loss: 0.4261 | Time: 7.68s\r\n",
      "Epoch 040 | Train Loss: 0.4288 | Val Loss: 0.4316 | Time: 7.62s\r\n",
      "Epoch 041 | Train Loss: 0.4305 | Val Loss: 0.4268 | Time: 7.65s\r\n",
      "Epoch 042 | Train Loss: 0.4305 | Val Loss: 0.4242 | Time: 7.69s\r\n",
      "Epoch 043 | Train Loss: 0.4289 | Val Loss: 0.4285 | Time: 7.69s\r\n",
      "Epoch 044 | Train Loss: 0.4284 | Val Loss: 0.4323 | Time: 7.64s\r\n",
      "Epoch 045 | Train Loss: 0.4273 | Val Loss: 0.4305 | Time: 7.70s\r\n",
      "Epoch 046 | Train Loss: 0.4340 | Val Loss: 0.4286 | Time: 7.77s\r\n",
      "Epoch 047 | Train Loss: 0.4247 | Val Loss: 0.4311 | Time: 7.67s\r\n",
      "Epoch 048 | Train Loss: 0.4294 | Val Loss: 0.4291 | Time: 7.61s\r\n",
      "Epoch 049 | Train Loss: 0.4271 | Val Loss: 0.4343 | Time: 7.61s\r\n",
      "✅ Meilleur modèle Random chargé (époque 34, val_loss: 0.4207)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.1046\r\n",
      "   2. InternetService      (CAT): 0.0837\r\n",
      "   3. PaperlessBilling     (CAT): 0.0757\r\n",
      "   4. tenure               (NUM): 0.0650\r\n",
      "   5. PaymentMethod        (CAT): 0.0626\r\n",
      "   6. OnlineBackup         (CAT): 0.0620\r\n",
      "   7. gender               (CAT): 0.0602\r\n",
      "   8. StreamingTV          (CAT): 0.0569\r\n",
      "   9. TotalCharges         (NUM): 0.0568\r\n",
      "  10. MultipleLines        (CAT): 0.0566\r\n",
      "  11. Partner              (CAT): 0.0540\r\n",
      "  12. Dependents           (CAT): 0.0538\r\n",
      "  13. DeviceProtection     (CAT): 0.0529\r\n",
      "  14. TechSupport          (CAT): 0.0520\r\n",
      "  15. OnlineSecurity       (CAT): 0.0519\r\n",
      "  16. PhoneService         (CAT): 0.0513\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.1046\r\n",
      "   2. InternetService     : 0.0837\r\n",
      "   3. PaperlessBilling    : 0.0757\r\n",
      "   4. tenure              : 0.0650\r\n",
      "   5. PaymentMethod       : 0.0626\r\n",
      "   6. OnlineBackup        : 0.0620\r\n",
      "   7. gender              : 0.0602\r\n",
      "   8. StreamingTV         : 0.0569\r\n",
      "   9. TotalCharges        : 0.0568\r\n",
      "  10. MultipleLines       : 0.0566\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_12/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 517.9s ===\r\n",
      "\u001b[32m[I 2025-07-19 22:15:06,406]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 4, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.1556043393952527, 'ffn_dropout_stage1': 0.13845026357932944, 'residual_dropout_stage1': 0.13985652247304803, 'lr_stage1': 0.00032001382108310804, 'weight_decay_stage1': 1.8723264675904285e-05, 'd_token_stage2': 128, 'n_blocks_stage2': 5, 'n_heads_stage2': 2, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.10283411553678011, 'ffn_dropout_stage2': 0.19409295560106846, 'residual_dropout_stage2': 0.12031749280282186, 'lr_stage2': 0.0006429344007428626, 'weight_decay_stage2': 0.004421754547952891, 'batch_size': 32, 'patience': 18, 'embedding_type': 'P-LR-LR', 'M': 16, 'k': 7}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  52%|███▋   | 13/25 [4:02:09<4:09:46, 1248.84s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: LR\r\n",
      "Modèle FTT+ créé avec 204,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5086 | Val Loss: 0.4510 | Time: 5.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4510)\r\n",
      "Epoch 001 | Train Loss: 0.4476 | Val Loss: 0.4409 | Time: 4.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4409)\r\n",
      "Epoch 002 | Train Loss: 0.4381 | Val Loss: 0.4333 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4333)\r\n",
      "Epoch 003 | Train Loss: 0.4320 | Val Loss: 0.4296 | Time: 4.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 004 | Train Loss: 0.4296 | Val Loss: 0.4255 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 005 | Train Loss: 0.4278 | Val Loss: 0.4241 | Time: 4.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 006 | Train Loss: 0.4262 | Val Loss: 0.4217 | Time: 4.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 007 | Train Loss: 0.4245 | Val Loss: 0.4200 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 008 | Train Loss: 0.4212 | Val Loss: 0.4198 | Time: 4.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4198)\r\n",
      "Epoch 009 | Train Loss: 0.4229 | Val Loss: 0.4177 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4177)\r\n",
      "Epoch 010 | Train Loss: 0.4213 | Val Loss: 0.4191 | Time: 4.79s\r\n",
      "Epoch 011 | Train Loss: 0.4202 | Val Loss: 0.4173 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4173)\r\n",
      "Epoch 012 | Train Loss: 0.4177 | Val Loss: 0.4188 | Time: 4.82s\r\n",
      "Epoch 013 | Train Loss: 0.4232 | Val Loss: 0.4146 | Time: 4.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 014 | Train Loss: 0.4155 | Val Loss: 0.4170 | Time: 4.81s\r\n",
      "Epoch 015 | Train Loss: 0.4155 | Val Loss: 0.4194 | Time: 4.76s\r\n",
      "Epoch 016 | Train Loss: 0.4148 | Val Loss: 0.4189 | Time: 4.86s\r\n",
      "Epoch 017 | Train Loss: 0.4156 | Val Loss: 0.4167 | Time: 4.83s\r\n",
      "Epoch 018 | Train Loss: 0.4138 | Val Loss: 0.4195 | Time: 4.86s\r\n",
      "Epoch 019 | Train Loss: 0.4127 | Val Loss: 0.4187 | Time: 4.97s\r\n",
      "Epoch 020 | Train Loss: 0.4126 | Val Loss: 0.4219 | Time: 4.86s\r\n",
      "Epoch 021 | Train Loss: 0.4101 | Val Loss: 0.4212 | Time: 4.82s\r\n",
      "Epoch 022 | Train Loss: 0.4123 | Val Loss: 0.4222 | Time: 4.96s\r\n",
      "Epoch 023 | Train Loss: 0.4098 | Val Loss: 0.4230 | Time: 4.83s\r\n",
      "Epoch 024 | Train Loss: 0.4112 | Val Loss: 0.4260 | Time: 4.82s\r\n",
      "Epoch 025 | Train Loss: 0.4086 | Val Loss: 0.4245 | Time: 4.79s\r\n",
      "Epoch 026 | Train Loss: 0.4089 | Val Loss: 0.4226 | Time: 5.12s\r\n",
      "Epoch 027 | Train Loss: 0.4085 | Val Loss: 0.4234 | Time: 4.82s\r\n",
      "Epoch 028 | Train Loss: 0.4076 | Val Loss: 0.4224 | Time: 4.81s\r\n",
      "Epoch 029 | Train Loss: 0.4073 | Val Loss: 0.4252 | Time: 4.76s\r\n",
      "Epoch 030 | Train Loss: 0.4048 | Val Loss: 0.4263 | Time: 4.79s\r\n",
      "Epoch 031 | Train Loss: 0.4045 | Val Loss: 0.4265 | Time: 4.80s\r\n",
      "Epoch 032 | Train Loss: 0.4054 | Val Loss: 0.4252 | Time: 4.92s\r\n",
      "Epoch 033 | Train Loss: 0.4038 | Val Loss: 0.4265 | Time: 4.88s\r\n",
      "Epoch 034 | Train Loss: 0.4036 | Val Loss: 0.4268 | Time: 4.86s\r\n",
      "\r\n",
      "Early stopping à l'époque 34 (patience: 21)\r\n",
      "✅ Meilleur modèle chargé (époque 13, val_loss: 0.4146)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.0579\r\n",
      "   2. PaymentMethod       : 0.0565\r\n",
      "   3. SeniorCitizen       : 0.0560\r\n",
      "   4. Dependents          : 0.0557\r\n",
      "   5. StreamingMovies     : 0.0539\r\n",
      "   6. PhoneService        : 0.0534\r\n",
      "   7. MonthlyCharges      : 0.0532\r\n",
      "   8. gender              : 0.0532\r\n",
      "   9. MultipleLines       : 0.0530\r\n",
      "  10. StreamingTV         : 0.0530\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. InternetService      (CAT): 0.0579\r\n",
      "   2. PaymentMethod        (CAT): 0.0565\r\n",
      "   3. SeniorCitizen        (CAT): 0.0560\r\n",
      "   4. Dependents           (CAT): 0.0557\r\n",
      "   5. StreamingMovies      (CAT): 0.0539\r\n",
      "   6. PhoneService         (CAT): 0.0534\r\n",
      "   7. MonthlyCharges       (NUM): 0.0532\r\n",
      "   8. gender               (CAT): 0.0532\r\n",
      "   9. MultipleLines        (CAT): 0.0530\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['InternetService', 'PaymentMethod', 'SeniorCitizen', 'Dependents', 'StreamingMovies', 'PhoneService', 'gender', 'MultipleLines'] → indices [6, 15, 1, 3, 12, 4, 0, 5]\r\n",
      "📊 Features sélectionnées: 1 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 335,361 paramètres\r\n",
      "🔗 Sparsité d'attention: 72.00%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5265 | Val Loss: 0.5101 | Time: 7.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5101)\r\n",
      "Epoch 001 | Train Loss: 0.5064 | Val Loss: 0.4952 | Time: 7.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4952)\r\n",
      "Epoch 002 | Train Loss: 0.5011 | Val Loss: 0.5088 | Time: 7.88s\r\n",
      "Epoch 003 | Train Loss: 0.4961 | Val Loss: 0.5032 | Time: 7.53s\r\n",
      "Epoch 004 | Train Loss: 0.4919 | Val Loss: 0.4924 | Time: 7.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4924)\r\n",
      "Epoch 005 | Train Loss: 0.4918 | Val Loss: 0.5032 | Time: 7.50s\r\n",
      "Epoch 006 | Train Loss: 0.4912 | Val Loss: 0.5022 | Time: 7.69s\r\n",
      "Epoch 007 | Train Loss: 0.4945 | Val Loss: 0.4965 | Time: 7.49s\r\n",
      "Epoch 008 | Train Loss: 0.4894 | Val Loss: 0.4899 | Time: 7.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4899)\r\n",
      "Epoch 009 | Train Loss: 0.4893 | Val Loss: 0.4881 | Time: 7.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4881)\r\n",
      "Epoch 010 | Train Loss: 0.4892 | Val Loss: 0.4883 | Time: 7.56s\r\n",
      "Epoch 011 | Train Loss: 0.4896 | Val Loss: 0.4884 | Time: 7.66s\r\n",
      "Epoch 012 | Train Loss: 0.4891 | Val Loss: 0.4903 | Time: 7.46s\r\n",
      "Epoch 013 | Train Loss: 0.4830 | Val Loss: 0.5003 | Time: 7.42s\r\n",
      "Epoch 014 | Train Loss: 0.4898 | Val Loss: 0.4897 | Time: 7.51s\r\n",
      "Epoch 015 | Train Loss: 0.4884 | Val Loss: 0.4883 | Time: 7.66s\r\n",
      "Epoch 016 | Train Loss: 0.4852 | Val Loss: 0.4907 | Time: 7.47s\r\n",
      "Epoch 017 | Train Loss: 0.4884 | Val Loss: 0.4874 | Time: 7.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4874)\r\n",
      "Epoch 018 | Train Loss: 0.4851 | Val Loss: 0.4915 | Time: 7.49s\r\n",
      "Epoch 019 | Train Loss: 0.4841 | Val Loss: 0.4855 | Time: 7.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4855)\r\n",
      "Epoch 020 | Train Loss: 0.4809 | Val Loss: 0.4904 | Time: 7.54s\r\n",
      "Epoch 021 | Train Loss: 0.4842 | Val Loss: 0.4898 | Time: 7.49s\r\n",
      "Epoch 022 | Train Loss: 0.4852 | Val Loss: 0.4892 | Time: 7.53s\r\n",
      "Epoch 023 | Train Loss: 0.4824 | Val Loss: 0.4877 | Time: 7.56s\r\n",
      "Epoch 024 | Train Loss: 0.4824 | Val Loss: 0.4918 | Time: 7.43s\r\n",
      "Epoch 025 | Train Loss: 0.4816 | Val Loss: 0.4938 | Time: 7.49s\r\n",
      "Epoch 026 | Train Loss: 0.4820 | Val Loss: 0.4896 | Time: 7.56s\r\n",
      "Epoch 027 | Train Loss: 0.4827 | Val Loss: 0.4860 | Time: 7.64s\r\n",
      "Epoch 028 | Train Loss: 0.4819 | Val Loss: 0.4880 | Time: 7.52s\r\n",
      "Epoch 029 | Train Loss: 0.4844 | Val Loss: 0.4861 | Time: 7.43s\r\n",
      "Epoch 030 | Train Loss: 0.4803 | Val Loss: 0.4899 | Time: 7.56s\r\n",
      "Epoch 031 | Train Loss: 0.4810 | Val Loss: 0.4845 | Time: 7.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4845)\r\n",
      "Epoch 032 | Train Loss: 0.4803 | Val Loss: 0.4860 | Time: 7.64s\r\n",
      "Epoch 033 | Train Loss: 0.4801 | Val Loss: 0.4880 | Time: 7.52s\r\n",
      "Epoch 034 | Train Loss: 0.4800 | Val Loss: 0.4881 | Time: 7.51s\r\n",
      "Epoch 035 | Train Loss: 0.4833 | Val Loss: 0.4820 | Time: 7.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4820)\r\n",
      "Epoch 036 | Train Loss: 0.4788 | Val Loss: 0.4874 | Time: 7.59s\r\n",
      "Epoch 037 | Train Loss: 0.4806 | Val Loss: 0.4874 | Time: 7.55s\r\n",
      "Epoch 038 | Train Loss: 0.4811 | Val Loss: 0.4861 | Time: 7.53s\r\n",
      "Epoch 039 | Train Loss: 0.4794 | Val Loss: 0.4822 | Time: 7.58s\r\n",
      "Epoch 040 | Train Loss: 0.4804 | Val Loss: 0.4829 | Time: 7.66s\r\n",
      "Epoch 041 | Train Loss: 0.4795 | Val Loss: 0.4829 | Time: 7.49s\r\n",
      "Epoch 042 | Train Loss: 0.4778 | Val Loss: 0.4843 | Time: 7.54s\r\n",
      "Epoch 043 | Train Loss: 0.4799 | Val Loss: 0.4884 | Time: 7.56s\r\n",
      "Epoch 044 | Train Loss: 0.4742 | Val Loss: 0.4860 | Time: 7.66s\r\n",
      "Epoch 045 | Train Loss: 0.4791 | Val Loss: 0.4831 | Time: 7.54s\r\n",
      "Epoch 046 | Train Loss: 0.4790 | Val Loss: 0.4872 | Time: 7.54s\r\n",
      "Epoch 047 | Train Loss: 0.4772 | Val Loss: 0.4838 | Time: 7.52s\r\n",
      "Epoch 048 | Train Loss: 0.4753 | Val Loss: 0.4914 | Time: 7.67s\r\n",
      "Epoch 049 | Train Loss: 0.4824 | Val Loss: 0.4851 | Time: 7.50s\r\n",
      "✅ Meilleur modèle Random chargé (époque 35, val_loss: 0.4820)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. InternetService      (CAT): 0.1368\r\n",
      "   2. PaymentMethod        (CAT): 0.1251\r\n",
      "   3. MonthlyCharges       (NUM): 0.1197\r\n",
      "   4. gender               (CAT): 0.1153\r\n",
      "   5. PhoneService         (CAT): 0.1087\r\n",
      "   6. StreamingMovies      (CAT): 0.1041\r\n",
      "   7. SeniorCitizen        (CAT): 0.1014\r\n",
      "   8. Dependents           (CAT): 0.0984\r\n",
      "   9. MultipleLines        (CAT): 0.0905\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.1368\r\n",
      "   2. PaymentMethod       : 0.1251\r\n",
      "   3. MonthlyCharges      : 0.1197\r\n",
      "   4. gender              : 0.1153\r\n",
      "   5. PhoneService        : 0.1087\r\n",
      "   6. StreamingMovies     : 0.1041\r\n",
      "   7. SeniorCitizen       : 0.1014\r\n",
      "   8. Dependents          : 0.0984\r\n",
      "   9. MultipleLines       : 0.0905\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 550.4s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: LR\r\n",
      "Modèle FTT+ créé avec 204,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5082 | Val Loss: 0.4458 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4458)\r\n",
      "Epoch 001 | Train Loss: 0.4432 | Val Loss: 0.4374 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4374)\r\n",
      "Epoch 002 | Train Loss: 0.4302 | Val Loss: 0.4337 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4337)\r\n",
      "Epoch 003 | Train Loss: 0.4271 | Val Loss: 0.4345 | Time: 4.82s\r\n",
      "Epoch 004 | Train Loss: 0.4268 | Val Loss: 0.4298 | Time: 5.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4298)\r\n",
      "Epoch 005 | Train Loss: 0.4175 | Val Loss: 0.4254 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 006 | Train Loss: 0.4168 | Val Loss: 0.4218 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4218)\r\n",
      "Epoch 007 | Train Loss: 0.4146 | Val Loss: 0.4217 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 008 | Train Loss: 0.4122 | Val Loss: 0.4224 | Time: 4.82s\r\n",
      "Epoch 009 | Train Loss: 0.4121 | Val Loss: 0.4216 | Time: 4.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 010 | Train Loss: 0.4102 | Val Loss: 0.4214 | Time: 4.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 011 | Train Loss: 0.4118 | Val Loss: 0.4185 | Time: 4.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4185)\r\n",
      "Epoch 012 | Train Loss: 0.4090 | Val Loss: 0.4176 | Time: 4.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4176)\r\n",
      "Epoch 013 | Train Loss: 0.4059 | Val Loss: 0.4199 | Time: 4.84s\r\n",
      "Epoch 014 | Train Loss: 0.4093 | Val Loss: 0.4181 | Time: 4.86s\r\n",
      "Epoch 015 | Train Loss: 0.4047 | Val Loss: 0.4196 | Time: 4.84s\r\n",
      "Epoch 016 | Train Loss: 0.4045 | Val Loss: 0.4191 | Time: 4.81s\r\n",
      "Epoch 017 | Train Loss: 0.4071 | Val Loss: 0.4175 | Time: 4.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4175)\r\n",
      "Epoch 018 | Train Loss: 0.4060 | Val Loss: 0.4188 | Time: 4.80s\r\n",
      "Epoch 019 | Train Loss: 0.4023 | Val Loss: 0.4211 | Time: 4.77s\r\n",
      "Epoch 020 | Train Loss: 0.4002 | Val Loss: 0.4192 | Time: 4.84s\r\n",
      "Epoch 021 | Train Loss: 0.3993 | Val Loss: 0.4233 | Time: 4.82s\r\n",
      "Epoch 022 | Train Loss: 0.3999 | Val Loss: 0.4223 | Time: 4.83s\r\n",
      "Epoch 023 | Train Loss: 0.3962 | Val Loss: 0.4253 | Time: 4.86s\r\n",
      "Epoch 024 | Train Loss: 0.3972 | Val Loss: 0.4242 | Time: 4.83s\r\n",
      "Epoch 025 | Train Loss: 0.3972 | Val Loss: 0.4258 | Time: 4.77s\r\n",
      "Epoch 026 | Train Loss: 0.3981 | Val Loss: 0.4238 | Time: 4.83s\r\n",
      "Epoch 027 | Train Loss: 0.3931 | Val Loss: 0.4250 | Time: 4.76s\r\n",
      "Epoch 028 | Train Loss: 0.3964 | Val Loss: 0.4246 | Time: 4.80s\r\n",
      "Epoch 029 | Train Loss: 0.3921 | Val Loss: 0.4247 | Time: 4.80s\r\n",
      "Epoch 030 | Train Loss: 0.3922 | Val Loss: 0.4260 | Time: 4.96s\r\n",
      "Epoch 031 | Train Loss: 0.3931 | Val Loss: 0.4256 | Time: 4.81s\r\n",
      "Epoch 032 | Train Loss: 0.3908 | Val Loss: 0.4265 | Time: 4.86s\r\n",
      "Epoch 033 | Train Loss: 0.3930 | Val Loss: 0.4288 | Time: 4.85s\r\n",
      "Epoch 034 | Train Loss: 0.3920 | Val Loss: 0.4309 | Time: 4.85s\r\n",
      "Epoch 035 | Train Loss: 0.3856 | Val Loss: 0.4287 | Time: 4.81s\r\n",
      "Epoch 036 | Train Loss: 0.3921 | Val Loss: 0.4312 | Time: 4.87s\r\n",
      "Epoch 037 | Train Loss: 0.3879 | Val Loss: 0.4310 | Time: 4.91s\r\n",
      "Epoch 038 | Train Loss: 0.3861 | Val Loss: 0.4360 | Time: 4.86s\r\n",
      "\r\n",
      "Early stopping à l'époque 38 (patience: 21)\r\n",
      "✅ Meilleur modèle chargé (époque 17, val_loss: 0.4175)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0599\r\n",
      "   2. OnlineBackup        : 0.0598\r\n",
      "   3. MonthlyCharges      : 0.0577\r\n",
      "   4. OnlineSecurity      : 0.0541\r\n",
      "   5. TechSupport         : 0.0541\r\n",
      "   6. DeviceProtection    : 0.0540\r\n",
      "   7. InternetService     : 0.0527\r\n",
      "   8. Contract            : 0.0526\r\n",
      "   9. Dependents          : 0.0523\r\n",
      "  10. TotalCharges        : 0.0519\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0599\r\n",
      "   2. OnlineBackup         (CAT): 0.0598\r\n",
      "   3. MonthlyCharges       (NUM): 0.0577\r\n",
      "   4. OnlineSecurity       (CAT): 0.0541\r\n",
      "   5. TechSupport          (CAT): 0.0541\r\n",
      "   6. DeviceProtection     (CAT): 0.0540\r\n",
      "   7. InternetService      (CAT): 0.0527\r\n",
      "   8. Contract             (CAT): 0.0526\r\n",
      "   9. Dependents           (CAT): 0.0523\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'OnlineBackup', 'OnlineSecurity', 'TechSupport', 'DeviceProtection', 'InternetService', 'Contract', 'Dependents'] → indices [14, 8, 7, 10, 9, 6, 13, 3]\r\n",
      "📊 Features sélectionnées: 1 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 335,425 paramètres\r\n",
      "🔗 Sparsité d'attention: 72.00%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4989 | Val Loss: 0.4549 | Time: 7.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4549)\r\n",
      "Epoch 001 | Train Loss: 0.4605 | Val Loss: 0.4506 | Time: 7.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4506)\r\n",
      "Epoch 002 | Train Loss: 0.4536 | Val Loss: 0.4463 | Time: 7.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4463)\r\n",
      "Epoch 003 | Train Loss: 0.4488 | Val Loss: 0.4425 | Time: 7.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 004 | Train Loss: 0.4473 | Val Loss: 0.4488 | Time: 7.53s\r\n",
      "Epoch 005 | Train Loss: 0.4416 | Val Loss: 0.4431 | Time: 7.48s\r\n",
      "Epoch 006 | Train Loss: 0.4404 | Val Loss: 0.4440 | Time: 7.57s\r\n",
      "Epoch 007 | Train Loss: 0.4411 | Val Loss: 0.4393 | Time: 7.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4393)\r\n",
      "Epoch 008 | Train Loss: 0.4396 | Val Loss: 0.4413 | Time: 7.65s\r\n",
      "Epoch 009 | Train Loss: 0.4384 | Val Loss: 0.4412 | Time: 7.52s\r\n",
      "Epoch 010 | Train Loss: 0.4378 | Val Loss: 0.4399 | Time: 7.57s\r\n",
      "Epoch 011 | Train Loss: 0.4405 | Val Loss: 0.4392 | Time: 7.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4392)\r\n",
      "Epoch 012 | Train Loss: 0.4364 | Val Loss: 0.4382 | Time: 7.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4382)\r\n",
      "Epoch 013 | Train Loss: 0.4388 | Val Loss: 0.4439 | Time: 7.44s\r\n",
      "Epoch 014 | Train Loss: 0.4376 | Val Loss: 0.4460 | Time: 7.59s\r\n",
      "Epoch 015 | Train Loss: 0.4363 | Val Loss: 0.4421 | Time: 7.67s\r\n",
      "Epoch 016 | Train Loss: 0.4355 | Val Loss: 0.4384 | Time: 7.58s\r\n",
      "Epoch 017 | Train Loss: 0.4359 | Val Loss: 0.4406 | Time: 7.50s\r\n",
      "Epoch 018 | Train Loss: 0.4355 | Val Loss: 0.4357 | Time: 7.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4357)\r\n",
      "Epoch 019 | Train Loss: 0.4351 | Val Loss: 0.4361 | Time: 7.69s\r\n",
      "Epoch 020 | Train Loss: 0.4335 | Val Loss: 0.4430 | Time: 7.50s\r\n",
      "Epoch 021 | Train Loss: 0.4354 | Val Loss: 0.4408 | Time: 7.47s\r\n",
      "Epoch 022 | Train Loss: 0.4357 | Val Loss: 0.4369 | Time: 7.51s\r\n",
      "Epoch 023 | Train Loss: 0.4349 | Val Loss: 0.4379 | Time: 7.66s\r\n",
      "Epoch 024 | Train Loss: 0.4345 | Val Loss: 0.4403 | Time: 7.47s\r\n",
      "Epoch 025 | Train Loss: 0.4349 | Val Loss: 0.4401 | Time: 7.50s\r\n",
      "Epoch 026 | Train Loss: 0.4373 | Val Loss: 0.4417 | Time: 7.57s\r\n",
      "Epoch 027 | Train Loss: 0.4324 | Val Loss: 0.4410 | Time: 7.56s\r\n",
      "Epoch 028 | Train Loss: 0.4336 | Val Loss: 0.4401 | Time: 7.55s\r\n",
      "Epoch 029 | Train Loss: 0.4319 | Val Loss: 0.4407 | Time: 7.52s\r\n",
      "Epoch 030 | Train Loss: 0.4317 | Val Loss: 0.4427 | Time: 7.54s\r\n",
      "Epoch 031 | Train Loss: 0.4354 | Val Loss: 0.4406 | Time: 7.46s\r\n",
      "Epoch 032 | Train Loss: 0.4349 | Val Loss: 0.4385 | Time: 7.65s\r\n",
      "Epoch 033 | Train Loss: 0.4330 | Val Loss: 0.4390 | Time: 7.40s\r\n",
      "Epoch 034 | Train Loss: 0.4331 | Val Loss: 0.4390 | Time: 7.53s\r\n",
      "Epoch 035 | Train Loss: 0.4307 | Val Loss: 0.4393 | Time: 7.58s\r\n",
      "Epoch 036 | Train Loss: 0.4319 | Val Loss: 0.4403 | Time: 7.67s\r\n",
      "Epoch 037 | Train Loss: 0.4337 | Val Loss: 0.4389 | Time: 7.55s\r\n",
      "Epoch 038 | Train Loss: 0.4349 | Val Loss: 0.4380 | Time: 7.55s\r\n",
      "Epoch 039 | Train Loss: 0.4311 | Val Loss: 0.4436 | Time: 7.53s\r\n",
      "\r\n",
      "Early stopping à l'époque 39 (patience: 21)\r\n",
      "✅ Meilleur modèle Random chargé (époque 18, val_loss: 0.4357)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineBackup         (CAT): 0.2477\r\n",
      "   2. Dependents           (CAT): 0.1497\r\n",
      "   3. MonthlyCharges       (NUM): 0.1356\r\n",
      "   4. TechSupport          (CAT): 0.1183\r\n",
      "   5. PaperlessBilling     (CAT): 0.0895\r\n",
      "   6. Contract             (CAT): 0.0679\r\n",
      "   7. InternetService      (CAT): 0.0676\r\n",
      "   8. DeviceProtection     (CAT): 0.0630\r\n",
      "   9. OnlineSecurity       (CAT): 0.0608\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.2477\r\n",
      "   2. Dependents          : 0.1497\r\n",
      "   3. MonthlyCharges      : 0.1356\r\n",
      "   4. TechSupport         : 0.1183\r\n",
      "   5. PaperlessBilling    : 0.0895\r\n",
      "   6. Contract            : 0.0679\r\n",
      "   7. InternetService     : 0.0676\r\n",
      "   8. DeviceProtection    : 0.0630\r\n",
      "   9. OnlineSecurity      : 0.0608\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 494.4s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: LR\r\n",
      "Modèle FTT+ créé avec 204,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5075 | Val Loss: 0.4353 | Time: 4.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4353)\r\n",
      "Epoch 001 | Train Loss: 0.4482 | Val Loss: 0.4278 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4278)\r\n",
      "Epoch 002 | Train Loss: 0.4383 | Val Loss: 0.4188 | Time: 4.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 003 | Train Loss: 0.4328 | Val Loss: 0.4171 | Time: 4.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4171)\r\n",
      "Epoch 004 | Train Loss: 0.4311 | Val Loss: 0.4141 | Time: 4.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4141)\r\n",
      "Epoch 005 | Train Loss: 0.4267 | Val Loss: 0.4127 | Time: 4.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4127)\r\n",
      "Epoch 006 | Train Loss: 0.4273 | Val Loss: 0.4117 | Time: 4.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4117)\r\n",
      "Epoch 007 | Train Loss: 0.4248 | Val Loss: 0.4130 | Time: 4.91s\r\n",
      "Epoch 008 | Train Loss: 0.4247 | Val Loss: 0.4117 | Time: 4.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4117)\r\n",
      "Epoch 009 | Train Loss: 0.4204 | Val Loss: 0.4142 | Time: 4.84s\r\n",
      "Epoch 010 | Train Loss: 0.4191 | Val Loss: 0.4131 | Time: 4.80s\r\n",
      "Epoch 011 | Train Loss: 0.4186 | Val Loss: 0.4134 | Time: 4.91s\r\n",
      "Epoch 012 | Train Loss: 0.4172 | Val Loss: 0.4138 | Time: 4.80s\r\n",
      "Epoch 013 | Train Loss: 0.4157 | Val Loss: 0.4132 | Time: 4.98s\r\n",
      "Epoch 014 | Train Loss: 0.4153 | Val Loss: 0.4141 | Time: 4.80s\r\n",
      "Epoch 015 | Train Loss: 0.4140 | Val Loss: 0.4161 | Time: 4.86s\r\n",
      "Epoch 016 | Train Loss: 0.4142 | Val Loss: 0.4115 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4115)\r\n",
      "Epoch 017 | Train Loss: 0.4114 | Val Loss: 0.4137 | Time: 4.82s\r\n",
      "Epoch 018 | Train Loss: 0.4115 | Val Loss: 0.4191 | Time: 4.81s\r\n",
      "Epoch 019 | Train Loss: 0.4114 | Val Loss: 0.4154 | Time: 4.98s\r\n",
      "Epoch 020 | Train Loss: 0.4106 | Val Loss: 0.4187 | Time: 4.87s\r\n",
      "Epoch 021 | Train Loss: 0.4099 | Val Loss: 0.4147 | Time: 4.81s\r\n",
      "Epoch 022 | Train Loss: 0.4060 | Val Loss: 0.4129 | Time: 4.78s\r\n",
      "Epoch 023 | Train Loss: 0.4066 | Val Loss: 0.4167 | Time: 4.85s\r\n",
      "Epoch 024 | Train Loss: 0.4036 | Val Loss: 0.4176 | Time: 4.86s\r\n",
      "Epoch 025 | Train Loss: 0.4057 | Val Loss: 0.4255 | Time: 4.83s\r\n",
      "Epoch 026 | Train Loss: 0.4052 | Val Loss: 0.4201 | Time: 4.88s\r\n",
      "Epoch 027 | Train Loss: 0.4013 | Val Loss: 0.4200 | Time: 4.84s\r\n",
      "Epoch 028 | Train Loss: 0.4004 | Val Loss: 0.4272 | Time: 4.79s\r\n",
      "Epoch 029 | Train Loss: 0.4015 | Val Loss: 0.4219 | Time: 4.83s\r\n",
      "Epoch 030 | Train Loss: 0.3991 | Val Loss: 0.4256 | Time: 4.79s\r\n",
      "Epoch 031 | Train Loss: 0.3981 | Val Loss: 0.4274 | Time: 4.85s\r\n",
      "Epoch 032 | Train Loss: 0.3980 | Val Loss: 0.4271 | Time: 4.85s\r\n",
      "Epoch 033 | Train Loss: 0.3983 | Val Loss: 0.4266 | Time: 4.94s\r\n",
      "Epoch 034 | Train Loss: 0.3945 | Val Loss: 0.4306 | Time: 4.83s\r\n",
      "Epoch 035 | Train Loss: 0.3953 | Val Loss: 0.4312 | Time: 4.85s\r\n",
      "Epoch 036 | Train Loss: 0.3910 | Val Loss: 0.4287 | Time: 4.86s\r\n",
      "Epoch 037 | Train Loss: 0.3915 | Val Loss: 0.4331 | Time: 4.86s\r\n",
      "\r\n",
      "Early stopping à l'époque 37 (patience: 21)\r\n",
      "✅ Meilleur modèle chargé (époque 16, val_loss: 0.4115)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingMovies     : 0.0585\r\n",
      "   2. DeviceProtection    : 0.0552\r\n",
      "   3. Partner             : 0.0552\r\n",
      "   4. TotalCharges        : 0.0541\r\n",
      "   5. TechSupport         : 0.0540\r\n",
      "   6. MultipleLines       : 0.0537\r\n",
      "   7. SeniorCitizen       : 0.0531\r\n",
      "   8. MonthlyCharges      : 0.0528\r\n",
      "   9. OnlineBackup        : 0.0527\r\n",
      "  10. PaymentMethod       : 0.0527\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. StreamingMovies      (CAT): 0.0585\r\n",
      "   2. DeviceProtection     (CAT): 0.0552\r\n",
      "   3. Partner              (CAT): 0.0552\r\n",
      "   4. TotalCharges         (NUM): 0.0541\r\n",
      "   5. TechSupport          (CAT): 0.0540\r\n",
      "   6. MultipleLines        (CAT): 0.0537\r\n",
      "   7. SeniorCitizen        (CAT): 0.0531\r\n",
      "   8. MonthlyCharges       (NUM): 0.0528\r\n",
      "   9. OnlineBackup         (CAT): 0.0527\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'MonthlyCharges'] → indices [2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['StreamingMovies', 'DeviceProtection', 'Partner', 'TechSupport', 'MultipleLines', 'SeniorCitizen', 'OnlineBackup'] → indices [12, 9, 2, 10, 5, 1, 8]\r\n",
      "📊 Features sélectionnées: 2 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 335,297 paramètres\r\n",
      "🔗 Sparsité d'attention: 72.00%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4932 | Val Loss: 0.4571 | Time: 7.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4571)\r\n",
      "Epoch 001 | Train Loss: 0.4687 | Val Loss: 0.4441 | Time: 7.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4441)\r\n",
      "Epoch 002 | Train Loss: 0.4636 | Val Loss: 0.4617 | Time: 7.58s\r\n",
      "Epoch 003 | Train Loss: 0.4599 | Val Loss: 0.4491 | Time: 7.50s\r\n",
      "Epoch 004 | Train Loss: 0.4592 | Val Loss: 0.4447 | Time: 7.57s\r\n",
      "Epoch 005 | Train Loss: 0.4573 | Val Loss: 0.4249 | Time: 7.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4249)\r\n",
      "Epoch 006 | Train Loss: 0.4555 | Val Loss: 0.4269 | Time: 7.55s\r\n",
      "Epoch 007 | Train Loss: 0.4525 | Val Loss: 0.4356 | Time: 7.49s\r\n",
      "Epoch 008 | Train Loss: 0.4531 | Val Loss: 0.4293 | Time: 7.59s\r\n",
      "Epoch 009 | Train Loss: 0.4498 | Val Loss: 0.4313 | Time: 7.60s\r\n",
      "Epoch 010 | Train Loss: 0.4499 | Val Loss: 0.4367 | Time: 7.56s\r\n",
      "Epoch 011 | Train Loss: 0.4462 | Val Loss: 0.4409 | Time: 7.54s\r\n",
      "Epoch 012 | Train Loss: 0.4479 | Val Loss: 0.4253 | Time: 7.55s\r\n",
      "Epoch 013 | Train Loss: 0.4449 | Val Loss: 0.4234 | Time: 7.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4234)\r\n",
      "Epoch 014 | Train Loss: 0.4477 | Val Loss: 0.4271 | Time: 7.64s\r\n",
      "Epoch 015 | Train Loss: 0.4456 | Val Loss: 0.4188 | Time: 7.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 016 | Train Loss: 0.4437 | Val Loss: 0.4262 | Time: 7.56s\r\n",
      "Epoch 017 | Train Loss: 0.4474 | Val Loss: 0.4275 | Time: 7.59s\r\n",
      "Epoch 018 | Train Loss: 0.4440 | Val Loss: 0.4181 | Time: 7.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4181)\r\n",
      "Epoch 019 | Train Loss: 0.4459 | Val Loss: 0.4266 | Time: 7.52s\r\n",
      "Epoch 020 | Train Loss: 0.4436 | Val Loss: 0.4211 | Time: 7.56s\r\n",
      "Epoch 021 | Train Loss: 0.4384 | Val Loss: 0.4292 | Time: 7.58s\r\n",
      "Epoch 022 | Train Loss: 0.4456 | Val Loss: 0.4198 | Time: 7.59s\r\n",
      "Epoch 023 | Train Loss: 0.4438 | Val Loss: 0.4251 | Time: 7.56s\r\n",
      "Epoch 024 | Train Loss: 0.4424 | Val Loss: 0.4276 | Time: 7.59s\r\n",
      "Epoch 025 | Train Loss: 0.4414 | Val Loss: 0.4231 | Time: 7.60s\r\n",
      "Epoch 026 | Train Loss: 0.4376 | Val Loss: 0.4191 | Time: 7.54s\r\n",
      "Epoch 027 | Train Loss: 0.4419 | Val Loss: 0.4154 | Time: 7.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4154)\r\n",
      "Epoch 028 | Train Loss: 0.4406 | Val Loss: 0.4311 | Time: 7.48s\r\n",
      "Epoch 029 | Train Loss: 0.4440 | Val Loss: 0.4239 | Time: 7.52s\r\n",
      "Epoch 030 | Train Loss: 0.4441 | Val Loss: 0.4168 | Time: 7.81s\r\n",
      "Epoch 031 | Train Loss: 0.4426 | Val Loss: 0.4224 | Time: 7.46s\r\n",
      "Epoch 032 | Train Loss: 0.4444 | Val Loss: 0.4262 | Time: 7.57s\r\n",
      "Epoch 033 | Train Loss: 0.4381 | Val Loss: 0.4207 | Time: 7.62s\r\n",
      "Epoch 034 | Train Loss: 0.4388 | Val Loss: 0.4263 | Time: 7.63s\r\n",
      "Epoch 035 | Train Loss: 0.4415 | Val Loss: 0.4234 | Time: 7.55s\r\n",
      "Epoch 036 | Train Loss: 0.4404 | Val Loss: 0.4193 | Time: 7.56s\r\n",
      "Epoch 037 | Train Loss: 0.4379 | Val Loss: 0.4235 | Time: 7.47s\r\n",
      "Epoch 038 | Train Loss: 0.4405 | Val Loss: 0.4318 | Time: 7.73s\r\n",
      "Epoch 039 | Train Loss: 0.4345 | Val Loss: 0.4260 | Time: 7.55s\r\n",
      "Epoch 040 | Train Loss: 0.4433 | Val Loss: 0.4214 | Time: 7.57s\r\n",
      "Epoch 041 | Train Loss: 0.4379 | Val Loss: 0.4200 | Time: 7.60s\r\n",
      "Epoch 042 | Train Loss: 0.4389 | Val Loss: 0.4247 | Time: 7.65s\r\n",
      "Epoch 043 | Train Loss: 0.4417 | Val Loss: 0.4234 | Time: 7.57s\r\n",
      "Epoch 044 | Train Loss: 0.4391 | Val Loss: 0.4326 | Time: 7.55s\r\n",
      "Epoch 045 | Train Loss: 0.4417 | Val Loss: 0.4305 | Time: 7.63s\r\n",
      "Epoch 046 | Train Loss: 0.4379 | Val Loss: 0.4277 | Time: 7.64s\r\n",
      "Epoch 047 | Train Loss: 0.4342 | Val Loss: 0.4189 | Time: 7.54s\r\n",
      "Epoch 048 | Train Loss: 0.4338 | Val Loss: 0.4250 | Time: 7.59s\r\n",
      "\r\n",
      "Early stopping à l'époque 48 (patience: 21)\r\n",
      "✅ Meilleur modèle Random chargé (époque 27, val_loss: 0.4154)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. TotalCharges         (NUM): 0.1410\r\n",
      "   2. DeviceProtection     (CAT): 0.1272\r\n",
      "   3. MonthlyCharges       (NUM): 0.1166\r\n",
      "   4. TechSupport          (CAT): 0.1116\r\n",
      "   5. SeniorCitizen        (CAT): 0.1046\r\n",
      "   6. OnlineBackup         (CAT): 0.1010\r\n",
      "   7. StreamingMovies      (CAT): 0.1009\r\n",
      "   8. MultipleLines        (CAT): 0.0997\r\n",
      "   9. Partner              (CAT): 0.0974\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TotalCharges        : 0.1410\r\n",
      "   2. DeviceProtection    : 0.1272\r\n",
      "   3. MonthlyCharges      : 0.1166\r\n",
      "   4. TechSupport         : 0.1116\r\n",
      "   5. SeniorCitizen       : 0.1046\r\n",
      "   6. OnlineBackup        : 0.1010\r\n",
      "   7. StreamingMovies     : 0.1009\r\n",
      "   8. MultipleLines       : 0.0997\r\n",
      "   9. Partner             : 0.0974\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_13/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 559.2s ===\r\n",
      "\u001b[32m[I 2025-07-19 22:41:51,097]\u001b[0m Trial 13 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 3, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.21374786344991567, 'ffn_dropout_stage1': 0.17947879689515628, 'residual_dropout_stage1': 0.17425758344526893, 'lr_stage1': 0.00011952479696392907, 'weight_decay_stage1': 8.21614766086521e-05, 'd_token_stage2': 64, 'n_blocks_stage2': 5, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.12222677304738203, 'ffn_dropout_stage2': 0.15229587455835733, 'residual_dropout_stage2': 0.16289473151241765, 'lr_stage2': 0.0007117070117573993, 'weight_decay_stage2': 0.0009117056143289838, 'batch_size': 32, 'patience': 21, 'embedding_type': 'LR', 'M': 9, 'k': 5}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  56%|███▉   | 14/25 [4:28:53<4:08:39, 1356.32s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q-LR\r\n",
      "Modèle FTT+ créé avec 150,913 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6020 | Val Loss: 0.5574 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5574)\r\n",
      "Epoch 001 | Train Loss: 0.5527 | Val Loss: 0.5166 | Time: 8.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5166)\r\n",
      "Epoch 002 | Train Loss: 0.5217 | Val Loss: 0.4848 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4848)\r\n",
      "Epoch 003 | Train Loss: 0.4978 | Val Loss: 0.4692 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4692)\r\n",
      "Epoch 004 | Train Loss: 0.4803 | Val Loss: 0.4579 | Time: 8.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4579)\r\n",
      "Epoch 005 | Train Loss: 0.4703 | Val Loss: 0.4517 | Time: 8.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4517)\r\n",
      "Epoch 006 | Train Loss: 0.4619 | Val Loss: 0.4464 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4464)\r\n",
      "Epoch 007 | Train Loss: 0.4613 | Val Loss: 0.4426 | Time: 7.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4426)\r\n",
      "Epoch 008 | Train Loss: 0.4549 | Val Loss: 0.4408 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4408)\r\n",
      "Epoch 009 | Train Loss: 0.4495 | Val Loss: 0.4384 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4384)\r\n",
      "Epoch 010 | Train Loss: 0.4519 | Val Loss: 0.4368 | Time: 8.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 011 | Train Loss: 0.4491 | Val Loss: 0.4327 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 012 | Train Loss: 0.4475 | Val Loss: 0.4335 | Time: 7.94s\r\n",
      "Epoch 013 | Train Loss: 0.4425 | Val Loss: 0.4294 | Time: 8.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4294)\r\n",
      "Epoch 014 | Train Loss: 0.4413 | Val Loss: 0.4286 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4286)\r\n",
      "Epoch 015 | Train Loss: 0.4423 | Val Loss: 0.4307 | Time: 8.02s\r\n",
      "Epoch 016 | Train Loss: 0.4365 | Val Loss: 0.4267 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4267)\r\n",
      "Epoch 017 | Train Loss: 0.4418 | Val Loss: 0.4264 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4264)\r\n",
      "Epoch 018 | Train Loss: 0.4381 | Val Loss: 0.4246 | Time: 7.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4246)\r\n",
      "Epoch 019 | Train Loss: 0.4372 | Val Loss: 0.4226 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4226)\r\n",
      "Epoch 020 | Train Loss: 0.4356 | Val Loss: 0.4247 | Time: 7.94s\r\n",
      "Epoch 021 | Train Loss: 0.4324 | Val Loss: 0.4249 | Time: 7.98s\r\n",
      "Epoch 022 | Train Loss: 0.4346 | Val Loss: 0.4208 | Time: 7.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4208)\r\n",
      "Epoch 023 | Train Loss: 0.4338 | Val Loss: 0.4211 | Time: 7.99s\r\n",
      "Epoch 024 | Train Loss: 0.4284 | Val Loss: 0.4209 | Time: 7.95s\r\n",
      "Epoch 025 | Train Loss: 0.4325 | Val Loss: 0.4196 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4196)\r\n",
      "Epoch 026 | Train Loss: 0.4280 | Val Loss: 0.4182 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4182)\r\n",
      "Epoch 027 | Train Loss: 0.4316 | Val Loss: 0.4192 | Time: 7.98s\r\n",
      "Epoch 028 | Train Loss: 0.4325 | Val Loss: 0.4187 | Time: 8.00s\r\n",
      "Epoch 029 | Train Loss: 0.4275 | Val Loss: 0.4174 | Time: 8.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "Epoch 030 | Train Loss: 0.4288 | Val Loss: 0.4182 | Time: 7.91s\r\n",
      "Epoch 031 | Train Loss: 0.4318 | Val Loss: 0.4165 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4165)\r\n",
      "Epoch 032 | Train Loss: 0.4291 | Val Loss: 0.4155 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4155)\r\n",
      "Epoch 033 | Train Loss: 0.4280 | Val Loss: 0.4149 | Time: 8.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4149)\r\n",
      "Epoch 034 | Train Loss: 0.4245 | Val Loss: 0.4147 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "Epoch 035 | Train Loss: 0.4259 | Val Loss: 0.4139 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4139)\r\n",
      "Epoch 036 | Train Loss: 0.4256 | Val Loss: 0.4139 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4139)\r\n",
      "Epoch 037 | Train Loss: 0.4231 | Val Loss: 0.4128 | Time: 8.15s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4128)\r\n",
      "Epoch 038 | Train Loss: 0.4220 | Val Loss: 0.4120 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4120)\r\n",
      "Epoch 039 | Train Loss: 0.4278 | Val Loss: 0.4120 | Time: 7.98s\r\n",
      "Epoch 040 | Train Loss: 0.4233 | Val Loss: 0.4122 | Time: 7.97s\r\n",
      "Epoch 041 | Train Loss: 0.4257 | Val Loss: 0.4109 | Time: 8.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4109)\r\n",
      "Epoch 042 | Train Loss: 0.4233 | Val Loss: 0.4121 | Time: 7.92s\r\n",
      "Epoch 043 | Train Loss: 0.4242 | Val Loss: 0.4133 | Time: 7.99s\r\n",
      "Epoch 044 | Train Loss: 0.4223 | Val Loss: 0.4109 | Time: 8.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4109)\r\n",
      "Epoch 045 | Train Loss: 0.4232 | Val Loss: 0.4104 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4104)\r\n",
      "Epoch 046 | Train Loss: 0.4230 | Val Loss: 0.4118 | Time: 7.90s\r\n",
      "Epoch 047 | Train Loss: 0.4237 | Val Loss: 0.4107 | Time: 7.91s\r\n",
      "Epoch 048 | Train Loss: 0.4216 | Val Loss: 0.4118 | Time: 7.96s\r\n",
      "Epoch 049 | Train Loss: 0.4231 | Val Loss: 0.4114 | Time: 8.03s\r\n",
      "✅ Meilleur modèle chargé (époque 45, val_loss: 0.4104)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TechSupport         : 0.0556\r\n",
      "   2. PaperlessBilling    : 0.0549\r\n",
      "   3. InternetService     : 0.0538\r\n",
      "   4. Contract            : 0.0535\r\n",
      "   5. Dependents          : 0.0530\r\n",
      "   6. StreamingMovies     : 0.0529\r\n",
      "   7. StreamingTV         : 0.0529\r\n",
      "   8. PhoneService        : 0.0528\r\n",
      "   9. Partner             : 0.0528\r\n",
      "  10. TotalCharges        : 0.0526\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 17 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. TechSupport          (CAT): 0.0556\r\n",
      "   2. PaperlessBilling     (CAT): 0.0549\r\n",
      "   3. InternetService      (CAT): 0.0538\r\n",
      "   4. Contract             (CAT): 0.0535\r\n",
      "   5. Dependents           (CAT): 0.0530\r\n",
      "   6. StreamingMovies      (CAT): 0.0529\r\n",
      "   7. StreamingTV          (CAT): 0.0529\r\n",
      "   8. PhoneService         (CAT): 0.0528\r\n",
      "   9. Partner              (CAT): 0.0528\r\n",
      "  10. TotalCharges         (NUM): 0.0526\r\n",
      "  11. PaymentMethod        (CAT): 0.0524\r\n",
      "  12. OnlineSecurity       (CAT): 0.0524\r\n",
      "  13. SeniorCitizen        (CAT): 0.0524\r\n",
      "  14. DeviceProtection     (CAT): 0.0519\r\n",
      "  15. tenure               (NUM): 0.0518\r\n",
      "  16. gender               (CAT): 0.0517\r\n",
      "  17. OnlineBackup         (CAT): 0.0511\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure'] → indices [2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['TechSupport', 'PaperlessBilling', 'InternetService', 'Contract', 'Dependents', 'StreamingMovies', 'StreamingTV', 'PhoneService', 'Partner', 'PaymentMethod', 'OnlineSecurity', 'SeniorCitizen', 'DeviceProtection', 'gender', 'OnlineBackup'] → indices [10, 14, 6, 13, 3, 12, 11, 4, 2, 15, 7, 1, 9, 0, 8]\r\n",
      "📊 Features sélectionnées: 2 numériques, 15 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 504,577 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.19%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5200 | Val Loss: 0.4804 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4804)\r\n",
      "Epoch 001 | Train Loss: 0.5011 | Val Loss: 0.5102 | Time: 4.73s\r\n",
      "Epoch 002 | Train Loss: 0.4979 | Val Loss: 0.4598 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4598)\r\n",
      "Epoch 003 | Train Loss: 0.4793 | Val Loss: 0.4628 | Time: 4.74s\r\n",
      "Epoch 004 | Train Loss: 0.4794 | Val Loss: 0.4578 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4578)\r\n",
      "Epoch 005 | Train Loss: 0.4780 | Val Loss: 0.4629 | Time: 4.72s\r\n",
      "Epoch 006 | Train Loss: 0.4991 | Val Loss: 0.4887 | Time: 4.71s\r\n",
      "Epoch 007 | Train Loss: 0.5014 | Val Loss: 0.4805 | Time: 4.72s\r\n",
      "Epoch 008 | Train Loss: 0.4942 | Val Loss: 0.4730 | Time: 4.72s\r\n",
      "Epoch 009 | Train Loss: 0.4850 | Val Loss: 0.4634 | Time: 4.78s\r\n",
      "Epoch 010 | Train Loss: 0.4829 | Val Loss: 0.4632 | Time: 4.80s\r\n",
      "Epoch 011 | Train Loss: 0.4763 | Val Loss: 0.4740 | Time: 4.82s\r\n",
      "Epoch 012 | Train Loss: 0.4797 | Val Loss: 0.4589 | Time: 4.72s\r\n",
      "Epoch 013 | Train Loss: 0.4810 | Val Loss: 0.4509 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4509)\r\n",
      "Epoch 014 | Train Loss: 0.4842 | Val Loss: 0.4713 | Time: 4.71s\r\n",
      "Epoch 015 | Train Loss: 0.4842 | Val Loss: 0.4642 | Time: 4.74s\r\n",
      "Epoch 016 | Train Loss: 0.4804 | Val Loss: 0.4643 | Time: 4.71s\r\n",
      "Epoch 017 | Train Loss: 0.4748 | Val Loss: 0.4589 | Time: 4.73s\r\n",
      "Epoch 018 | Train Loss: 0.4750 | Val Loss: 0.4662 | Time: 4.94s\r\n",
      "Epoch 019 | Train Loss: 0.4761 | Val Loss: 0.4644 | Time: 4.76s\r\n",
      "Epoch 020 | Train Loss: 0.4739 | Val Loss: 0.4699 | Time: 4.70s\r\n",
      "Epoch 021 | Train Loss: 0.4745 | Val Loss: 0.4615 | Time: 4.72s\r\n",
      "Epoch 022 | Train Loss: 0.4717 | Val Loss: 0.4713 | Time: 4.64s\r\n",
      "Epoch 023 | Train Loss: 0.4881 | Val Loss: 0.4766 | Time: 4.69s\r\n",
      "Epoch 024 | Train Loss: 0.4920 | Val Loss: 0.4715 | Time: 4.72s\r\n",
      "Epoch 025 | Train Loss: 0.4850 | Val Loss: 0.4611 | Time: 4.80s\r\n",
      "Epoch 026 | Train Loss: 0.4795 | Val Loss: 0.4658 | Time: 4.69s\r\n",
      "Epoch 027 | Train Loss: 0.4775 | Val Loss: 0.4653 | Time: 4.68s\r\n",
      "Epoch 028 | Train Loss: 0.4817 | Val Loss: 0.4574 | Time: 4.77s\r\n",
      "Epoch 029 | Train Loss: 0.4802 | Val Loss: 0.4634 | Time: 4.71s\r\n",
      "Epoch 030 | Train Loss: 0.4876 | Val Loss: 0.4828 | Time: 4.76s\r\n",
      "\r\n",
      "Early stopping à l'époque 30 (patience: 17)\r\n",
      "✅ Meilleur modèle Random chargé (époque 13, val_loss: 0.4509)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Partner              (CAT): 0.0745\r\n",
      "   2. StreamingMovies      (CAT): 0.0741\r\n",
      "   3. DeviceProtection     (CAT): 0.0650\r\n",
      "   4. PhoneService         (CAT): 0.0635\r\n",
      "   5. Dependents           (CAT): 0.0633\r\n",
      "   6. gender               (CAT): 0.0608\r\n",
      "   7. OnlineBackup         (CAT): 0.0589\r\n",
      "   8. tenure               (NUM): 0.0552\r\n",
      "   9. TechSupport          (CAT): 0.0552\r\n",
      "  10. StreamingTV          (CAT): 0.0552\r\n",
      "  11. Contract             (CAT): 0.0548\r\n",
      "  12. SeniorCitizen        (CAT): 0.0547\r\n",
      "  13. InternetService      (CAT): 0.0546\r\n",
      "  14. PaperlessBilling     (CAT): 0.0540\r\n",
      "  15. TotalCharges         (NUM): 0.0521\r\n",
      "  16. OnlineSecurity       (CAT): 0.0521\r\n",
      "  17. PaymentMethod        (CAT): 0.0521\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Partner             : 0.0745\r\n",
      "   2. StreamingMovies     : 0.0741\r\n",
      "   3. DeviceProtection    : 0.0650\r\n",
      "   4. PhoneService        : 0.0635\r\n",
      "   5. Dependents          : 0.0633\r\n",
      "   6. gender              : 0.0608\r\n",
      "   7. OnlineBackup        : 0.0589\r\n",
      "   8. tenure              : 0.0552\r\n",
      "   9. TechSupport         : 0.0552\r\n",
      "  10. StreamingTV         : 0.0552\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 550.8s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q-LR\r\n",
      "Modèle FTT+ créé avec 150,913 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5945 | Val Loss: 0.5354 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5354)\r\n",
      "Epoch 001 | Train Loss: 0.5268 | Val Loss: 0.4878 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4878)\r\n",
      "Epoch 002 | Train Loss: 0.4960 | Val Loss: 0.4673 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4673)\r\n",
      "Epoch 003 | Train Loss: 0.4741 | Val Loss: 0.4552 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4552)\r\n",
      "Epoch 004 | Train Loss: 0.4682 | Val Loss: 0.4492 | Time: 8.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4492)\r\n",
      "Epoch 005 | Train Loss: 0.4608 | Val Loss: 0.4443 | Time: 8.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4443)\r\n",
      "Epoch 006 | Train Loss: 0.4553 | Val Loss: 0.4417 | Time: 8.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 007 | Train Loss: 0.4518 | Val Loss: 0.4403 | Time: 8.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4403)\r\n",
      "Epoch 008 | Train Loss: 0.4478 | Val Loss: 0.4386 | Time: 8.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4386)\r\n",
      "Epoch 009 | Train Loss: 0.4440 | Val Loss: 0.4397 | Time: 8.01s\r\n",
      "Epoch 010 | Train Loss: 0.4411 | Val Loss: 0.4393 | Time: 8.00s\r\n",
      "Epoch 011 | Train Loss: 0.4392 | Val Loss: 0.4368 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 012 | Train Loss: 0.4373 | Val Loss: 0.4350 | Time: 8.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4350)\r\n",
      "Epoch 013 | Train Loss: 0.4348 | Val Loss: 0.4352 | Time: 8.00s\r\n",
      "Epoch 014 | Train Loss: 0.4340 | Val Loss: 0.4337 | Time: 8.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4337)\r\n",
      "Epoch 015 | Train Loss: 0.4330 | Val Loss: 0.4337 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4337)\r\n",
      "Epoch 016 | Train Loss: 0.4294 | Val Loss: 0.4310 | Time: 7.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4310)\r\n",
      "Epoch 017 | Train Loss: 0.4260 | Val Loss: 0.4323 | Time: 8.06s\r\n",
      "Epoch 018 | Train Loss: 0.4293 | Val Loss: 0.4304 | Time: 7.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4304)\r\n",
      "Epoch 019 | Train Loss: 0.4262 | Val Loss: 0.4301 | Time: 8.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4301)\r\n",
      "Epoch 020 | Train Loss: 0.4279 | Val Loss: 0.4302 | Time: 8.01s\r\n",
      "Epoch 021 | Train Loss: 0.4261 | Val Loss: 0.4289 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 022 | Train Loss: 0.4249 | Val Loss: 0.4278 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4278)\r\n",
      "Epoch 023 | Train Loss: 0.4216 | Val Loss: 0.4283 | Time: 8.02s\r\n",
      "Epoch 024 | Train Loss: 0.4236 | Val Loss: 0.4282 | Time: 8.03s\r\n",
      "Epoch 025 | Train Loss: 0.4191 | Val Loss: 0.4261 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4261)\r\n",
      "Epoch 026 | Train Loss: 0.4187 | Val Loss: 0.4264 | Time: 7.97s\r\n",
      "Epoch 027 | Train Loss: 0.4196 | Val Loss: 0.4256 | Time: 8.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4256)\r\n",
      "Epoch 028 | Train Loss: 0.4231 | Val Loss: 0.4247 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4247)\r\n",
      "Epoch 029 | Train Loss: 0.4189 | Val Loss: 0.4233 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4233)\r\n",
      "Epoch 030 | Train Loss: 0.4191 | Val Loss: 0.4236 | Time: 8.03s\r\n",
      "Epoch 031 | Train Loss: 0.4196 | Val Loss: 0.4226 | Time: 8.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4226)\r\n",
      "Epoch 032 | Train Loss: 0.4180 | Val Loss: 0.4225 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4225)\r\n",
      "Epoch 033 | Train Loss: 0.4141 | Val Loss: 0.4226 | Time: 7.95s\r\n",
      "Epoch 034 | Train Loss: 0.4168 | Val Loss: 0.4233 | Time: 7.92s\r\n",
      "Epoch 035 | Train Loss: 0.4156 | Val Loss: 0.4229 | Time: 8.08s\r\n",
      "Epoch 036 | Train Loss: 0.4168 | Val Loss: 0.4214 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 037 | Train Loss: 0.4139 | Val Loss: 0.4210 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4210)\r\n",
      "Epoch 038 | Train Loss: 0.4157 | Val Loss: 0.4198 | Time: 7.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4198)\r\n",
      "Epoch 039 | Train Loss: 0.4147 | Val Loss: 0.4208 | Time: 8.12s\r\n",
      "Epoch 040 | Train Loss: 0.4129 | Val Loss: 0.4196 | Time: 8.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4196)\r\n",
      "Epoch 041 | Train Loss: 0.4147 | Val Loss: 0.4198 | Time: 8.00s\r\n",
      "Epoch 042 | Train Loss: 0.4142 | Val Loss: 0.4192 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 043 | Train Loss: 0.4112 | Val Loss: 0.4186 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 044 | Train Loss: 0.4066 | Val Loss: 0.4189 | Time: 7.99s\r\n",
      "Epoch 045 | Train Loss: 0.4110 | Val Loss: 0.4207 | Time: 7.93s\r\n",
      "Epoch 046 | Train Loss: 0.4121 | Val Loss: 0.4198 | Time: 7.96s\r\n",
      "Epoch 047 | Train Loss: 0.4113 | Val Loss: 0.4183 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 048 | Train Loss: 0.4140 | Val Loss: 0.4192 | Time: 8.05s\r\n",
      "Epoch 049 | Train Loss: 0.4116 | Val Loss: 0.4186 | Time: 8.00s\r\n",
      "✅ Meilleur modèle chargé (époque 47, val_loss: 0.4183)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.0602\r\n",
      "   2. OnlineBackup        : 0.0574\r\n",
      "   3. gender              : 0.0574\r\n",
      "   4. Contract            : 0.0560\r\n",
      "   5. MonthlyCharges      : 0.0555\r\n",
      "   6. DeviceProtection    : 0.0549\r\n",
      "   7. TechSupport         : 0.0540\r\n",
      "   8. Partner             : 0.0537\r\n",
      "   9. StreamingTV         : 0.0536\r\n",
      "  10. Dependents          : 0.0522\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 17 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. SeniorCitizen        (CAT): 0.0602\r\n",
      "   2. OnlineBackup         (CAT): 0.0574\r\n",
      "   3. gender               (CAT): 0.0574\r\n",
      "   4. Contract             (CAT): 0.0560\r\n",
      "   5. MonthlyCharges       (NUM): 0.0555\r\n",
      "   6. DeviceProtection     (CAT): 0.0549\r\n",
      "   7. TechSupport          (CAT): 0.0540\r\n",
      "   8. Partner              (CAT): 0.0537\r\n",
      "   9. StreamingTV          (CAT): 0.0536\r\n",
      "  10. Dependents           (CAT): 0.0522\r\n",
      "  11. PaymentMethod        (CAT): 0.0519\r\n",
      "  12. PhoneService         (CAT): 0.0517\r\n",
      "  13. StreamingMovies      (CAT): 0.0510\r\n",
      "  14. OnlineSecurity       (CAT): 0.0504\r\n",
      "  15. tenure               (NUM): 0.0494\r\n",
      "  16. InternetService      (CAT): 0.0487\r\n",
      "  17. PaperlessBilling     (CAT): 0.0485\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure'] → indices [1, 0]\r\n",
      "   - Catégorielles sélectionnées: ['SeniorCitizen', 'OnlineBackup', 'gender', 'Contract', 'DeviceProtection', 'TechSupport', 'Partner', 'StreamingTV', 'Dependents', 'PaymentMethod', 'PhoneService', 'StreamingMovies', 'OnlineSecurity', 'InternetService', 'PaperlessBilling'] → indices [1, 8, 0, 13, 9, 10, 2, 11, 3, 15, 4, 12, 7, 6, 14]\r\n",
      "📊 Features sélectionnées: 2 numériques, 15 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 504,577 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.19%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5077 | Val Loss: 0.4698 | Time: 4.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4698)\r\n",
      "Epoch 001 | Train Loss: 0.5031 | Val Loss: 0.5043 | Time: 4.73s\r\n",
      "Epoch 002 | Train Loss: 0.5038 | Val Loss: 0.4727 | Time: 4.95s\r\n",
      "Epoch 003 | Train Loss: 0.5019 | Val Loss: 0.4753 | Time: 4.72s\r\n",
      "Epoch 004 | Train Loss: 0.4852 | Val Loss: 0.4699 | Time: 4.80s\r\n",
      "Epoch 005 | Train Loss: 0.4826 | Val Loss: 0.4628 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4628)\r\n",
      "Epoch 006 | Train Loss: 0.4855 | Val Loss: 0.4726 | Time: 4.72s\r\n",
      "Epoch 007 | Train Loss: 0.4888 | Val Loss: 0.4668 | Time: 4.71s\r\n",
      "Epoch 008 | Train Loss: 0.4826 | Val Loss: 0.4674 | Time: 4.86s\r\n",
      "Epoch 009 | Train Loss: 0.4907 | Val Loss: 0.4640 | Time: 4.73s\r\n",
      "Epoch 010 | Train Loss: 0.4856 | Val Loss: 0.4700 | Time: 4.75s\r\n",
      "Epoch 011 | Train Loss: 0.4866 | Val Loss: 0.4586 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4586)\r\n",
      "Epoch 012 | Train Loss: 0.4855 | Val Loss: 0.4611 | Time: 4.77s\r\n",
      "Epoch 013 | Train Loss: 0.4821 | Val Loss: 0.4560 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4560)\r\n",
      "Epoch 014 | Train Loss: 0.5124 | Val Loss: 0.5195 | Time: 4.76s\r\n",
      "Epoch 015 | Train Loss: 0.5129 | Val Loss: 0.4787 | Time: 4.81s\r\n",
      "Epoch 016 | Train Loss: 0.4987 | Val Loss: 0.4656 | Time: 4.75s\r\n",
      "Epoch 017 | Train Loss: 0.4893 | Val Loss: 0.4633 | Time: 4.76s\r\n",
      "Epoch 018 | Train Loss: 0.4916 | Val Loss: 0.4767 | Time: 4.74s\r\n",
      "Epoch 019 | Train Loss: 0.4898 | Val Loss: 0.4660 | Time: 4.77s\r\n",
      "Epoch 020 | Train Loss: 0.4893 | Val Loss: 0.4664 | Time: 4.72s\r\n",
      "Epoch 021 | Train Loss: 0.4873 | Val Loss: 0.4505 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4505)\r\n",
      "Epoch 022 | Train Loss: 0.4856 | Val Loss: 0.4533 | Time: 4.88s\r\n",
      "Epoch 023 | Train Loss: 0.4872 | Val Loss: 0.4445 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4445)\r\n",
      "Epoch 024 | Train Loss: 0.4785 | Val Loss: 0.4497 | Time: 4.74s\r\n",
      "Epoch 025 | Train Loss: 0.4778 | Val Loss: 0.4480 | Time: 4.73s\r\n",
      "Epoch 026 | Train Loss: 0.4759 | Val Loss: 0.4499 | Time: 4.65s\r\n",
      "Epoch 027 | Train Loss: 0.4736 | Val Loss: 0.4378 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4378)\r\n",
      "Epoch 028 | Train Loss: 0.4704 | Val Loss: 0.4353 | Time: 4.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4353)\r\n",
      "Epoch 029 | Train Loss: 0.4756 | Val Loss: 0.4466 | Time: 4.86s\r\n",
      "Epoch 030 | Train Loss: 0.4851 | Val Loss: 0.4497 | Time: 4.73s\r\n",
      "Epoch 031 | Train Loss: 0.4853 | Val Loss: 0.4541 | Time: 4.73s\r\n",
      "Epoch 032 | Train Loss: 0.4757 | Val Loss: 0.4520 | Time: 4.69s\r\n",
      "Epoch 033 | Train Loss: 0.4702 | Val Loss: 0.4557 | Time: 4.72s\r\n",
      "Epoch 034 | Train Loss: 0.4713 | Val Loss: 0.4403 | Time: 4.73s\r\n",
      "Epoch 035 | Train Loss: 0.4741 | Val Loss: 0.4394 | Time: 4.86s\r\n",
      "Epoch 036 | Train Loss: 0.4731 | Val Loss: 0.4452 | Time: 4.73s\r\n",
      "Epoch 037 | Train Loss: 0.4690 | Val Loss: 0.4392 | Time: 4.75s\r\n",
      "Epoch 038 | Train Loss: 0.4678 | Val Loss: 0.4534 | Time: 4.71s\r\n",
      "Epoch 039 | Train Loss: 0.4814 | Val Loss: 0.4512 | Time: 4.68s\r\n",
      "Epoch 040 | Train Loss: 0.4863 | Val Loss: 0.4642 | Time: 4.72s\r\n",
      "Epoch 041 | Train Loss: 0.4774 | Val Loss: 0.4556 | Time: 4.74s\r\n",
      "Epoch 042 | Train Loss: 0.4803 | Val Loss: 0.4519 | Time: 4.89s\r\n",
      "Epoch 043 | Train Loss: 0.4729 | Val Loss: 0.4527 | Time: 4.73s\r\n",
      "Epoch 044 | Train Loss: 0.4794 | Val Loss: 0.4417 | Time: 4.71s\r\n",
      "Epoch 045 | Train Loss: 0.4680 | Val Loss: 0.4416 | Time: 4.68s\r\n",
      "\r\n",
      "Early stopping à l'époque 45 (patience: 17)\r\n",
      "✅ Meilleur modèle Random chargé (époque 28, val_loss: 0.4353)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. TechSupport          (CAT): 0.0840\r\n",
      "   2. tenure               (NUM): 0.0730\r\n",
      "   3. StreamingTV          (CAT): 0.0608\r\n",
      "   4. DeviceProtection     (CAT): 0.0605\r\n",
      "   5. gender               (CAT): 0.0601\r\n",
      "   6. MonthlyCharges       (NUM): 0.0600\r\n",
      "   7. PaymentMethod        (CAT): 0.0594\r\n",
      "   8. PaperlessBilling     (CAT): 0.0594\r\n",
      "   9. Partner              (CAT): 0.0565\r\n",
      "  10. OnlineBackup         (CAT): 0.0550\r\n",
      "  11. StreamingMovies      (CAT): 0.0547\r\n",
      "  12. Contract             (CAT): 0.0542\r\n",
      "  13. PhoneService         (CAT): 0.0535\r\n",
      "  14. Dependents           (CAT): 0.0530\r\n",
      "  15. SeniorCitizen        (CAT): 0.0526\r\n",
      "  16. OnlineSecurity       (CAT): 0.0518\r\n",
      "  17. InternetService      (CAT): 0.0515\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TechSupport         : 0.0840\r\n",
      "   2. tenure              : 0.0730\r\n",
      "   3. StreamingTV         : 0.0608\r\n",
      "   4. DeviceProtection    : 0.0605\r\n",
      "   5. gender              : 0.0601\r\n",
      "   6. MonthlyCharges      : 0.0600\r\n",
      "   7. PaymentMethod       : 0.0594\r\n",
      "   8. PaperlessBilling    : 0.0594\r\n",
      "   9. Partner             : 0.0565\r\n",
      "  10. OnlineBackup        : 0.0550\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 624.4s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q-LR\r\n",
      "Modèle FTT+ créé avec 150,913 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6172 | Val Loss: 0.5669 | Time: 8.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5669)\r\n",
      "Epoch 001 | Train Loss: 0.5653 | Val Loss: 0.5374 | Time: 8.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5374)\r\n",
      "Epoch 002 | Train Loss: 0.5280 | Val Loss: 0.4874 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4874)\r\n",
      "Epoch 003 | Train Loss: 0.4947 | Val Loss: 0.4585 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4585)\r\n",
      "Epoch 004 | Train Loss: 0.4813 | Val Loss: 0.4449 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4449)\r\n",
      "Epoch 005 | Train Loss: 0.4641 | Val Loss: 0.4359 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4359)\r\n",
      "Epoch 006 | Train Loss: 0.4628 | Val Loss: 0.4305 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4305)\r\n",
      "Epoch 007 | Train Loss: 0.4485 | Val Loss: 0.4252 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4252)\r\n",
      "Epoch 008 | Train Loss: 0.4463 | Val Loss: 0.4220 | Time: 8.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4220)\r\n",
      "Epoch 009 | Train Loss: 0.4439 | Val Loss: 0.4192 | Time: 8.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 010 | Train Loss: 0.4455 | Val Loss: 0.4178 | Time: 7.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 011 | Train Loss: 0.4426 | Val Loss: 0.4174 | Time: 8.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "Epoch 012 | Train Loss: 0.4379 | Val Loss: 0.4162 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4162)\r\n",
      "Epoch 013 | Train Loss: 0.4384 | Val Loss: 0.4160 | Time: 8.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4160)\r\n",
      "Epoch 014 | Train Loss: 0.4382 | Val Loss: 0.4146 | Time: 8.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 015 | Train Loss: 0.4336 | Val Loss: 0.4146 | Time: 8.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 016 | Train Loss: 0.4358 | Val Loss: 0.4130 | Time: 7.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4130)\r\n",
      "Epoch 017 | Train Loss: 0.4330 | Val Loss: 0.4120 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4120)\r\n",
      "Epoch 018 | Train Loss: 0.4300 | Val Loss: 0.4111 | Time: 7.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4111)\r\n",
      "Epoch 019 | Train Loss: 0.4337 | Val Loss: 0.4112 | Time: 8.00s\r\n",
      "Epoch 020 | Train Loss: 0.4306 | Val Loss: 0.4106 | Time: 8.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 021 | Train Loss: 0.4286 | Val Loss: 0.4106 | Time: 8.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 022 | Train Loss: 0.4288 | Val Loss: 0.4097 | Time: 8.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4097)\r\n",
      "Epoch 023 | Train Loss: 0.4291 | Val Loss: 0.4093 | Time: 8.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4093)\r\n",
      "Epoch 024 | Train Loss: 0.4280 | Val Loss: 0.4085 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4085)\r\n",
      "Epoch 025 | Train Loss: 0.4272 | Val Loss: 0.4086 | Time: 7.98s\r\n",
      "Epoch 026 | Train Loss: 0.4256 | Val Loss: 0.4086 | Time: 7.95s\r\n",
      "Epoch 027 | Train Loss: 0.4263 | Val Loss: 0.4080 | Time: 8.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4080)\r\n",
      "Epoch 028 | Train Loss: 0.4262 | Val Loss: 0.4077 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4077)\r\n",
      "Epoch 029 | Train Loss: 0.4266 | Val Loss: 0.4082 | Time: 7.97s\r\n",
      "Epoch 030 | Train Loss: 0.4236 | Val Loss: 0.4076 | Time: 8.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4076)\r\n",
      "Epoch 031 | Train Loss: 0.4261 | Val Loss: 0.4078 | Time: 7.95s\r\n",
      "Epoch 032 | Train Loss: 0.4248 | Val Loss: 0.4080 | Time: 8.07s\r\n",
      "Epoch 033 | Train Loss: 0.4211 | Val Loss: 0.4082 | Time: 8.04s\r\n",
      "Epoch 034 | Train Loss: 0.4224 | Val Loss: 0.4077 | Time: 7.97s\r\n",
      "Epoch 035 | Train Loss: 0.4191 | Val Loss: 0.4083 | Time: 7.97s\r\n",
      "Epoch 036 | Train Loss: 0.4218 | Val Loss: 0.4075 | Time: 8.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4075)\r\n",
      "Epoch 037 | Train Loss: 0.4212 | Val Loss: 0.4082 | Time: 7.97s\r\n",
      "Epoch 038 | Train Loss: 0.4197 | Val Loss: 0.4078 | Time: 7.96s\r\n",
      "Epoch 039 | Train Loss: 0.4197 | Val Loss: 0.4072 | Time: 8.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4072)\r\n",
      "Epoch 040 | Train Loss: 0.4230 | Val Loss: 0.4064 | Time: 8.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4064)\r\n",
      "Epoch 041 | Train Loss: 0.4196 | Val Loss: 0.4069 | Time: 7.90s\r\n",
      "Epoch 042 | Train Loss: 0.4210 | Val Loss: 0.4070 | Time: 7.96s\r\n",
      "Epoch 043 | Train Loss: 0.4139 | Val Loss: 0.4068 | Time: 8.06s\r\n",
      "Epoch 044 | Train Loss: 0.4193 | Val Loss: 0.4065 | Time: 8.01s\r\n",
      "Epoch 045 | Train Loss: 0.4165 | Val Loss: 0.4077 | Time: 8.01s\r\n",
      "Epoch 046 | Train Loss: 0.4159 | Val Loss: 0.4084 | Time: 7.95s\r\n",
      "Epoch 047 | Train Loss: 0.4176 | Val Loss: 0.4073 | Time: 8.00s\r\n",
      "Epoch 048 | Train Loss: 0.4174 | Val Loss: 0.4075 | Time: 8.12s\r\n",
      "Epoch 049 | Train Loss: 0.4193 | Val Loss: 0.4074 | Time: 7.99s\r\n",
      "✅ Meilleur modèle chargé (époque 40, val_loss: 0.4064)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.0551\r\n",
      "   2. Dependents          : 0.0547\r\n",
      "   3. TechSupport         : 0.0545\r\n",
      "   4. PaperlessBilling    : 0.0544\r\n",
      "   5. MonthlyCharges      : 0.0539\r\n",
      "   6. TotalCharges        : 0.0539\r\n",
      "   7. PaymentMethod       : 0.0531\r\n",
      "   8. Contract            : 0.0528\r\n",
      "   9. gender              : 0.0527\r\n",
      "  10. StreamingMovies     : 0.0527\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 17 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. OnlineSecurity       (CAT): 0.0551\r\n",
      "   2. Dependents           (CAT): 0.0547\r\n",
      "   3. TechSupport          (CAT): 0.0545\r\n",
      "   4. PaperlessBilling     (CAT): 0.0544\r\n",
      "   5. MonthlyCharges       (NUM): 0.0539\r\n",
      "   6. TotalCharges         (NUM): 0.0539\r\n",
      "   7. PaymentMethod        (CAT): 0.0531\r\n",
      "   8. Contract             (CAT): 0.0528\r\n",
      "   9. gender               (CAT): 0.0527\r\n",
      "  10. StreamingMovies      (CAT): 0.0527\r\n",
      "  11. DeviceProtection     (CAT): 0.0521\r\n",
      "  12. Partner              (CAT): 0.0518\r\n",
      "  13. SeniorCitizen        (CAT): 0.0518\r\n",
      "  14. InternetService      (CAT): 0.0518\r\n",
      "  15. StreamingTV          (CAT): 0.0515\r\n",
      "  16. MultipleLines        (CAT): 0.0514\r\n",
      "  17. PhoneService         (CAT): 0.0513\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'TotalCharges'] → indices [1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['OnlineSecurity', 'Dependents', 'TechSupport', 'PaperlessBilling', 'PaymentMethod', 'Contract', 'gender', 'StreamingMovies', 'DeviceProtection', 'Partner', 'SeniorCitizen', 'InternetService', 'StreamingTV', 'MultipleLines', 'PhoneService'] → indices [7, 3, 10, 14, 15, 13, 0, 12, 9, 2, 1, 6, 11, 5, 4]\r\n",
      "📊 Features sélectionnées: 2 numériques, 15 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 504,577 paramètres\r\n",
      "🔗 Sparsité d'attention: 85.19%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5155 | Val Loss: 0.4490 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4490)\r\n",
      "Epoch 001 | Train Loss: 0.5004 | Val Loss: 0.5181 | Time: 4.78s\r\n",
      "Epoch 002 | Train Loss: 0.5071 | Val Loss: 0.4946 | Time: 4.73s\r\n",
      "Epoch 003 | Train Loss: 0.5042 | Val Loss: 0.4872 | Time: 4.79s\r\n",
      "Epoch 004 | Train Loss: 0.5278 | Val Loss: 0.5326 | Time: 4.99s\r\n",
      "Epoch 005 | Train Loss: 0.5337 | Val Loss: 0.5520 | Time: 4.73s\r\n",
      "Epoch 006 | Train Loss: 0.5265 | Val Loss: 0.5219 | Time: 4.72s\r\n",
      "Epoch 007 | Train Loss: 0.5242 | Val Loss: 0.5471 | Time: 4.73s\r\n",
      "Epoch 008 | Train Loss: 0.5454 | Val Loss: 0.5507 | Time: 4.64s\r\n",
      "Epoch 009 | Train Loss: 0.5414 | Val Loss: 0.5462 | Time: 4.70s\r\n",
      "Epoch 010 | Train Loss: 0.5318 | Val Loss: 0.5424 | Time: 4.80s\r\n",
      "Epoch 011 | Train Loss: 0.5185 | Val Loss: 0.5088 | Time: 4.73s\r\n",
      "Epoch 012 | Train Loss: 0.5047 | Val Loss: 0.5082 | Time: 4.69s\r\n",
      "Epoch 013 | Train Loss: 0.5118 | Val Loss: 0.5183 | Time: 4.76s\r\n",
      "Epoch 014 | Train Loss: 0.5149 | Val Loss: 0.5139 | Time: 4.69s\r\n",
      "Epoch 015 | Train Loss: 0.5100 | Val Loss: 0.5035 | Time: 4.66s\r\n",
      "Epoch 016 | Train Loss: 0.5080 | Val Loss: 0.5364 | Time: 4.78s\r\n",
      "Epoch 017 | Train Loss: 0.5102 | Val Loss: 0.5037 | Time: 4.86s\r\n",
      "\r\n",
      "Early stopping à l'époque 17 (patience: 17)\r\n",
      "✅ Meilleur modèle Random chargé (époque 0, val_loss: 0.4490)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Contract             (CAT): 0.2283\r\n",
      "   2. MultipleLines        (CAT): 0.1372\r\n",
      "   3. TechSupport          (CAT): 0.1262\r\n",
      "   4. InternetService      (CAT): 0.1188\r\n",
      "   5. Partner              (CAT): 0.1006\r\n",
      "   6. OnlineSecurity       (CAT): 0.0747\r\n",
      "   7. gender               (CAT): 0.0442\r\n",
      "   8. SeniorCitizen        (CAT): 0.0346\r\n",
      "   9. MonthlyCharges       (NUM): 0.0331\r\n",
      "  10. DeviceProtection     (CAT): 0.0278\r\n",
      "  11. TotalCharges         (NUM): 0.0236\r\n",
      "  12. StreamingTV          (CAT): 0.0216\r\n",
      "  13. Dependents           (CAT): 0.0071\r\n",
      "  14. StreamingMovies      (CAT): 0.0065\r\n",
      "  15. PhoneService         (CAT): 0.0056\r\n",
      "  16. PaperlessBilling     (CAT): 0.0052\r\n",
      "  17. PaymentMethod        (CAT): 0.0049\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.2283\r\n",
      "   2. MultipleLines       : 0.1372\r\n",
      "   3. TechSupport         : 0.1262\r\n",
      "   4. InternetService     : 0.1188\r\n",
      "   5. Partner             : 0.1006\r\n",
      "   6. OnlineSecurity      : 0.0747\r\n",
      "   7. gender              : 0.0442\r\n",
      "   8. SeniorCitizen       : 0.0346\r\n",
      "   9. MonthlyCharges      : 0.0331\r\n",
      "  10. DeviceProtection    : 0.0278\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_14/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 491.0s ===\r\n",
      "\u001b[32m[I 2025-07-19 23:09:37,841]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 5, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.23961932800108027, 'ffn_dropout_stage1': 0.12187724301957087, 'residual_dropout_stage1': 0.1993842785049709, 'lr_stage1': 2.835345960500365e-05, 'weight_decay_stage1': 6.557000312279795e-06, 'd_token_stage2': 128, 'n_blocks_stage2': 3, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.24805899070804235, 'ffn_dropout_stage2': 0.21621748801944007, 'residual_dropout_stage2': 0.1332367645374738, 'lr_stage2': 0.004316771866635475, 'weight_decay_stage2': 0.017893365507322644, 'batch_size': 32, 'patience': 17, 'embedding_type': 'Q-LR', 'M': 17, 'k': 7}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  60%|████▏  | 15/25 [4:56:40<4:01:38, 1449.89s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 93,505 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4628 | Val Loss: 0.4213 | Time: 2.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4213)\r\n",
      "Epoch 001 | Train Loss: 0.4345 | Val Loss: 0.4177 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4177)\r\n",
      "Epoch 002 | Train Loss: 0.4239 | Val Loss: 0.4170 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 003 | Train Loss: 0.4219 | Val Loss: 0.4188 | Time: 2.48s\r\n",
      "Epoch 004 | Train Loss: 0.4205 | Val Loss: 0.4245 | Time: 2.43s\r\n",
      "Epoch 005 | Train Loss: 0.4173 | Val Loss: 0.4219 | Time: 2.44s\r\n",
      "Epoch 006 | Train Loss: 0.4170 | Val Loss: 0.4259 | Time: 2.44s\r\n",
      "Epoch 007 | Train Loss: 0.4159 | Val Loss: 0.4237 | Time: 2.48s\r\n",
      "Epoch 008 | Train Loss: 0.4158 | Val Loss: 0.4225 | Time: 2.45s\r\n",
      "Epoch 009 | Train Loss: 0.4163 | Val Loss: 0.4177 | Time: 2.42s\r\n",
      "Epoch 010 | Train Loss: 0.4140 | Val Loss: 0.4162 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4162)\r\n",
      "Epoch 011 | Train Loss: 0.4133 | Val Loss: 0.4177 | Time: 2.54s\r\n",
      "Epoch 012 | Train Loss: 0.4109 | Val Loss: 0.4132 | Time: 2.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4132)\r\n",
      "Epoch 013 | Train Loss: 0.4105 | Val Loss: 0.4153 | Time: 2.45s\r\n",
      "Epoch 014 | Train Loss: 0.4118 | Val Loss: 0.4283 | Time: 2.45s\r\n",
      "Epoch 015 | Train Loss: 0.4126 | Val Loss: 0.4143 | Time: 2.48s\r\n",
      "Epoch 016 | Train Loss: 0.4091 | Val Loss: 0.4214 | Time: 2.44s\r\n",
      "Epoch 017 | Train Loss: 0.4066 | Val Loss: 0.4116 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4116)\r\n",
      "Epoch 018 | Train Loss: 0.4059 | Val Loss: 0.4176 | Time: 2.44s\r\n",
      "Epoch 019 | Train Loss: 0.4062 | Val Loss: 0.4328 | Time: 2.48s\r\n",
      "Epoch 020 | Train Loss: 0.4028 | Val Loss: 0.4160 | Time: 2.44s\r\n",
      "Epoch 021 | Train Loss: 0.4063 | Val Loss: 0.4177 | Time: 2.44s\r\n",
      "Epoch 022 | Train Loss: 0.4037 | Val Loss: 0.4120 | Time: 2.43s\r\n",
      "Epoch 023 | Train Loss: 0.4061 | Val Loss: 0.4052 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4052)\r\n",
      "Epoch 024 | Train Loss: 0.4011 | Val Loss: 0.4166 | Time: 2.42s\r\n",
      "Epoch 025 | Train Loss: 0.4000 | Val Loss: 0.4183 | Time: 2.40s\r\n",
      "Epoch 026 | Train Loss: 0.3973 | Val Loss: 0.4178 | Time: 2.40s\r\n",
      "Epoch 027 | Train Loss: 0.3994 | Val Loss: 0.4135 | Time: 2.46s\r\n",
      "Epoch 028 | Train Loss: 0.3944 | Val Loss: 0.4270 | Time: 2.44s\r\n",
      "Epoch 029 | Train Loss: 0.3957 | Val Loss: 0.4226 | Time: 2.41s\r\n",
      "Epoch 030 | Train Loss: 0.3973 | Val Loss: 0.4282 | Time: 2.40s\r\n",
      "Epoch 031 | Train Loss: 0.3941 | Val Loss: 0.4356 | Time: 2.52s\r\n",
      "Epoch 032 | Train Loss: 0.4012 | Val Loss: 0.4346 | Time: 2.43s\r\n",
      "Epoch 033 | Train Loss: 0.3982 | Val Loss: 0.4252 | Time: 2.45s\r\n",
      "Epoch 034 | Train Loss: 0.3942 | Val Loss: 0.4292 | Time: 2.44s\r\n",
      "Epoch 035 | Train Loss: 0.3962 | Val Loss: 0.4267 | Time: 2.46s\r\n",
      "Epoch 036 | Train Loss: 0.3936 | Val Loss: 0.4216 | Time: 2.52s\r\n",
      "Epoch 037 | Train Loss: 0.3944 | Val Loss: 0.4276 | Time: 2.52s\r\n",
      "Epoch 038 | Train Loss: 0.3958 | Val Loss: 0.4200 | Time: 2.45s\r\n",
      "Epoch 039 | Train Loss: 0.3945 | Val Loss: 0.4284 | Time: 2.46s\r\n",
      "Epoch 040 | Train Loss: 0.3905 | Val Loss: 0.4337 | Time: 2.47s\r\n",
      "Epoch 041 | Train Loss: 0.3958 | Val Loss: 0.4266 | Time: 2.41s\r\n",
      "\r\n",
      "Early stopping à l'époque 41 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 23, val_loss: 0.4052)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0995\r\n",
      "   2. StreamingMovies     : 0.0708\r\n",
      "   3. TotalCharges        : 0.0686\r\n",
      "   4. PhoneService        : 0.0595\r\n",
      "   5. OnlineSecurity      : 0.0593\r\n",
      "   6. DeviceProtection    : 0.0580\r\n",
      "   7. gender              : 0.0563\r\n",
      "   8. StreamingTV         : 0.0552\r\n",
      "   9. Contract            : 0.0542\r\n",
      "  10. Dependents          : 0.0523\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0995\r\n",
      "   2. StreamingMovies      (CAT): 0.0708\r\n",
      "   3. TotalCharges         (NUM): 0.0686\r\n",
      "   4. PhoneService         (CAT): 0.0595\r\n",
      "   5. OnlineSecurity       (CAT): 0.0593\r\n",
      "   6. DeviceProtection     (CAT): 0.0580\r\n",
      "   7. gender               (CAT): 0.0563\r\n",
      "   8. StreamingTV          (CAT): 0.0552\r\n",
      "   9. Contract             (CAT): 0.0542\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'StreamingMovies', 'PhoneService', 'OnlineSecurity', 'DeviceProtection', 'gender', 'StreamingTV', 'Contract'] → indices [14, 12, 4, 7, 9, 0, 11, 13]\r\n",
      "📊 Features sélectionnées: 1 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 832,001 paramètres\r\n",
      "🔗 Sparsité d'attention: 74.00%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5881 | Val Loss: 0.5693 | Time: 3.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5693)\r\n",
      "Epoch 001 | Train Loss: 0.5680 | Val Loss: 0.5732 | Time: 3.77s\r\n",
      "Epoch 002 | Train Loss: 0.5693 | Val Loss: 0.5855 | Time: 3.78s\r\n",
      "Epoch 003 | Train Loss: 0.5691 | Val Loss: 0.5951 | Time: 3.82s\r\n",
      "Epoch 004 | Train Loss: 0.5661 | Val Loss: 0.5088 | Time: 3.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5088)\r\n",
      "Epoch 005 | Train Loss: 0.5387 | Val Loss: 0.5283 | Time: 3.78s\r\n",
      "Epoch 006 | Train Loss: 0.5417 | Val Loss: 0.5018 | Time: 3.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5018)\r\n",
      "Epoch 007 | Train Loss: 0.5477 | Val Loss: 0.5225 | Time: 3.80s\r\n",
      "Epoch 008 | Train Loss: 0.5248 | Val Loss: 0.5100 | Time: 3.80s\r\n",
      "Epoch 009 | Train Loss: 0.5271 | Val Loss: 0.5254 | Time: 3.76s\r\n",
      "Epoch 010 | Train Loss: 0.5218 | Val Loss: 0.5140 | Time: 3.80s\r\n",
      "Epoch 011 | Train Loss: 0.5216 | Val Loss: 0.4880 | Time: 3.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4880)\r\n",
      "Epoch 012 | Train Loss: 0.5200 | Val Loss: 0.5151 | Time: 3.84s\r\n",
      "Epoch 013 | Train Loss: 0.5133 | Val Loss: 0.4858 | Time: 3.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4858)\r\n",
      "Epoch 014 | Train Loss: 0.5423 | Val Loss: 0.5162 | Time: 3.83s\r\n",
      "Epoch 015 | Train Loss: 0.5213 | Val Loss: 0.5138 | Time: 3.77s\r\n",
      "Epoch 016 | Train Loss: 0.5169 | Val Loss: 0.5196 | Time: 3.84s\r\n",
      "Epoch 017 | Train Loss: 0.5247 | Val Loss: 0.5139 | Time: 3.80s\r\n",
      "Epoch 018 | Train Loss: 0.5121 | Val Loss: 0.5772 | Time: 3.82s\r\n",
      "Epoch 019 | Train Loss: 0.5366 | Val Loss: 0.4959 | Time: 3.87s\r\n",
      "Epoch 020 | Train Loss: 0.5210 | Val Loss: 0.4928 | Time: 3.74s\r\n",
      "Epoch 021 | Train Loss: 0.5113 | Val Loss: 0.5131 | Time: 3.89s\r\n",
      "Epoch 022 | Train Loss: 0.5266 | Val Loss: 0.4965 | Time: 3.79s\r\n",
      "Epoch 023 | Train Loss: 0.5313 | Val Loss: 0.5639 | Time: 3.80s\r\n",
      "Epoch 024 | Train Loss: 0.5331 | Val Loss: 0.5179 | Time: 3.82s\r\n",
      "Epoch 025 | Train Loss: 0.5291 | Val Loss: 0.5131 | Time: 3.81s\r\n",
      "Epoch 026 | Train Loss: 0.5187 | Val Loss: 0.5740 | Time: 3.80s\r\n",
      "Epoch 027 | Train Loss: 0.5308 | Val Loss: 0.5047 | Time: 3.82s\r\n",
      "Epoch 028 | Train Loss: 0.5218 | Val Loss: 0.5065 | Time: 3.76s\r\n",
      "Epoch 029 | Train Loss: 0.5232 | Val Loss: 0.5075 | Time: 3.91s\r\n",
      "Epoch 030 | Train Loss: 0.5203 | Val Loss: 0.5034 | Time: 3.77s\r\n",
      "Epoch 031 | Train Loss: 0.5519 | Val Loss: 0.5610 | Time: 3.75s\r\n",
      "\r\n",
      "Early stopping à l'époque 31 (patience: 18)\r\n",
      "✅ Meilleur modèle Random chargé (époque 13, val_loss: 0.4858)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. gender               (CAT): 0.4734\r\n",
      "   2. StreamingMovies      (CAT): 0.1449\r\n",
      "   3. OnlineSecurity       (CAT): 0.0677\r\n",
      "   4. TotalCharges         (NUM): 0.0654\r\n",
      "   5. PhoneService         (CAT): 0.0574\r\n",
      "   6. Contract             (CAT): 0.0494\r\n",
      "   7. PaperlessBilling     (CAT): 0.0493\r\n",
      "   8. DeviceProtection     (CAT): 0.0465\r\n",
      "   9. StreamingTV          (CAT): 0.0460\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.4734\r\n",
      "   2. StreamingMovies     : 0.1449\r\n",
      "   3. OnlineSecurity      : 0.0677\r\n",
      "   4. TotalCharges        : 0.0654\r\n",
      "   5. PhoneService        : 0.0574\r\n",
      "   6. Contract            : 0.0494\r\n",
      "   7. PaperlessBilling    : 0.0493\r\n",
      "   8. DeviceProtection    : 0.0465\r\n",
      "   9. StreamingTV         : 0.0460\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 228.1s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 93,505 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4634 | Val Loss: 0.4307 | Time: 2.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 001 | Train Loss: 0.4243 | Val Loss: 0.4249 | Time: 2.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4249)\r\n",
      "Epoch 002 | Train Loss: 0.4194 | Val Loss: 0.4246 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4246)\r\n",
      "Epoch 003 | Train Loss: 0.4147 | Val Loss: 0.4190 | Time: 2.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 004 | Train Loss: 0.4110 | Val Loss: 0.4183 | Time: 2.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 005 | Train Loss: 0.4100 | Val Loss: 0.4228 | Time: 2.45s\r\n",
      "Epoch 006 | Train Loss: 0.4069 | Val Loss: 0.4112 | Time: 2.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4112)\r\n",
      "Epoch 007 | Train Loss: 0.4053 | Val Loss: 0.4134 | Time: 2.47s\r\n",
      "Epoch 008 | Train Loss: 0.4059 | Val Loss: 0.4159 | Time: 2.54s\r\n",
      "Epoch 009 | Train Loss: 0.4028 | Val Loss: 0.4115 | Time: 2.44s\r\n",
      "Epoch 010 | Train Loss: 0.4002 | Val Loss: 0.4115 | Time: 2.45s\r\n",
      "Epoch 011 | Train Loss: 0.4006 | Val Loss: 0.4105 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4105)\r\n",
      "Epoch 012 | Train Loss: 0.4016 | Val Loss: 0.4120 | Time: 2.46s\r\n",
      "Epoch 013 | Train Loss: 0.3977 | Val Loss: 0.4139 | Time: 2.43s\r\n",
      "Epoch 014 | Train Loss: 0.3971 | Val Loss: 0.4111 | Time: 2.45s\r\n",
      "Epoch 015 | Train Loss: 0.3964 | Val Loss: 0.4130 | Time: 2.44s\r\n",
      "Epoch 016 | Train Loss: 0.3975 | Val Loss: 0.4154 | Time: 2.46s\r\n",
      "Epoch 017 | Train Loss: 0.3950 | Val Loss: 0.4121 | Time: 2.47s\r\n",
      "Epoch 018 | Train Loss: 0.3911 | Val Loss: 0.4135 | Time: 2.43s\r\n",
      "Epoch 019 | Train Loss: 0.3918 | Val Loss: 0.4155 | Time: 2.43s\r\n",
      "Epoch 020 | Train Loss: 0.3918 | Val Loss: 0.4148 | Time: 2.48s\r\n",
      "Epoch 021 | Train Loss: 0.3895 | Val Loss: 0.4158 | Time: 2.57s\r\n",
      "Epoch 022 | Train Loss: 0.3873 | Val Loss: 0.4185 | Time: 2.40s\r\n",
      "Epoch 023 | Train Loss: 0.3891 | Val Loss: 0.4192 | Time: 2.39s\r\n",
      "Epoch 024 | Train Loss: 0.3890 | Val Loss: 0.4153 | Time: 2.47s\r\n",
      "Epoch 025 | Train Loss: 0.3847 | Val Loss: 0.4171 | Time: 2.45s\r\n",
      "Epoch 026 | Train Loss: 0.3871 | Val Loss: 0.4181 | Time: 2.45s\r\n",
      "Epoch 027 | Train Loss: 0.3856 | Val Loss: 0.4201 | Time: 2.46s\r\n",
      "Epoch 028 | Train Loss: 0.3817 | Val Loss: 0.4263 | Time: 2.44s\r\n",
      "Epoch 029 | Train Loss: 0.3847 | Val Loss: 0.4203 | Time: 2.42s\r\n",
      "\r\n",
      "Early stopping à l'époque 29 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 11, val_loss: 0.4105)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0794\r\n",
      "   2. SeniorCitizen       : 0.0747\r\n",
      "   3. InternetService     : 0.0721\r\n",
      "   4. StreamingTV         : 0.0688\r\n",
      "   5. TechSupport         : 0.0586\r\n",
      "   6. DeviceProtection    : 0.0545\r\n",
      "   7. PaymentMethod       : 0.0539\r\n",
      "   8. gender              : 0.0533\r\n",
      "   9. PhoneService        : 0.0530\r\n",
      "  10. OnlineSecurity      : 0.0493\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0794\r\n",
      "   2. SeniorCitizen        (CAT): 0.0747\r\n",
      "   3. InternetService      (CAT): 0.0721\r\n",
      "   4. StreamingTV          (CAT): 0.0688\r\n",
      "   5. TechSupport          (CAT): 0.0586\r\n",
      "   6. DeviceProtection     (CAT): 0.0545\r\n",
      "   7. PaymentMethod        (CAT): 0.0539\r\n",
      "   8. gender               (CAT): 0.0533\r\n",
      "   9. PhoneService         (CAT): 0.0530\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: [] → indices []\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'SeniorCitizen', 'InternetService', 'StreamingTV', 'TechSupport', 'DeviceProtection', 'PaymentMethod', 'gender', 'PhoneService'] → indices [14, 1, 6, 11, 10, 9, 15, 0, 4]\r\n",
      "📊 Features sélectionnées: 0 numériques, 9 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 832,257 paramètres\r\n",
      "🔗 Sparsité d'attention: 74.00%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6000 | Val Loss: 0.5781 | Time: 3.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5781)\r\n",
      "Epoch 001 | Train Loss: 0.5841 | Val Loss: 0.5797 | Time: 3.83s\r\n",
      "Epoch 002 | Train Loss: 0.5825 | Val Loss: 0.5778 | Time: 3.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5778)\r\n",
      "Epoch 003 | Train Loss: 0.5819 | Val Loss: 0.5750 | Time: 3.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5750)\r\n",
      "Epoch 004 | Train Loss: 0.5484 | Val Loss: 0.5414 | Time: 3.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5414)\r\n",
      "Epoch 005 | Train Loss: 0.5359 | Val Loss: 0.5295 | Time: 3.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5295)\r\n",
      "Epoch 006 | Train Loss: 0.5293 | Val Loss: 0.5320 | Time: 3.79s\r\n",
      "Epoch 007 | Train Loss: 0.5349 | Val Loss: 0.5280 | Time: 3.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5280)\r\n",
      "Epoch 008 | Train Loss: 0.5324 | Val Loss: 0.5343 | Time: 3.82s\r\n",
      "Epoch 009 | Train Loss: 0.5314 | Val Loss: 0.5304 | Time: 3.82s\r\n",
      "Epoch 010 | Train Loss: 0.5305 | Val Loss: 0.5366 | Time: 3.85s\r\n",
      "Epoch 011 | Train Loss: 0.5351 | Val Loss: 0.5424 | Time: 3.78s\r\n",
      "Epoch 012 | Train Loss: 0.5306 | Val Loss: 0.5497 | Time: 3.77s\r\n",
      "Epoch 013 | Train Loss: 0.5320 | Val Loss: 0.5370 | Time: 3.79s\r\n",
      "Epoch 014 | Train Loss: 0.5309 | Val Loss: 0.5312 | Time: 3.80s\r\n",
      "Epoch 015 | Train Loss: 0.5332 | Val Loss: 0.5337 | Time: 3.79s\r\n",
      "Epoch 016 | Train Loss: 0.5331 | Val Loss: 0.5412 | Time: 3.77s\r\n",
      "Epoch 017 | Train Loss: 0.5305 | Val Loss: 0.5424 | Time: 3.78s\r\n",
      "Epoch 018 | Train Loss: 0.5328 | Val Loss: 0.5326 | Time: 3.78s\r\n",
      "Epoch 019 | Train Loss: 0.5315 | Val Loss: 0.5453 | Time: 3.92s\r\n",
      "Epoch 020 | Train Loss: 0.5328 | Val Loss: 0.5361 | Time: 3.76s\r\n",
      "Epoch 021 | Train Loss: 0.5292 | Val Loss: 0.5326 | Time: 3.78s\r\n",
      "Epoch 022 | Train Loss: 0.5333 | Val Loss: 0.5380 | Time: 3.82s\r\n",
      "Epoch 023 | Train Loss: 0.5329 | Val Loss: 0.5368 | Time: 3.77s\r\n",
      "Epoch 024 | Train Loss: 0.5321 | Val Loss: 0.5375 | Time: 3.79s\r\n",
      "Epoch 025 | Train Loss: 0.5335 | Val Loss: 0.5422 | Time: 3.76s\r\n",
      "\r\n",
      "Early stopping à l'époque 25 (patience: 18)\r\n",
      "✅ Meilleur modèle Random chargé (époque 7, val_loss: 0.5280)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PaperlessBilling     (CAT): 0.2235\r\n",
      "   2. SeniorCitizen        (CAT): 0.1623\r\n",
      "   3. InternetService      (CAT): 0.1471\r\n",
      "   4. PhoneService         (CAT): 0.1468\r\n",
      "   5. TechSupport          (CAT): 0.0757\r\n",
      "   6. gender               (CAT): 0.0612\r\n",
      "   7. DeviceProtection     (CAT): 0.0611\r\n",
      "   8. PaymentMethod        (CAT): 0.0611\r\n",
      "   9. StreamingTV          (CAT): 0.0611\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.2235\r\n",
      "   2. SeniorCitizen       : 0.1623\r\n",
      "   3. InternetService     : 0.1471\r\n",
      "   4. PhoneService        : 0.1468\r\n",
      "   5. TechSupport         : 0.0757\r\n",
      "   6. gender              : 0.0612\r\n",
      "   7. DeviceProtection    : 0.0611\r\n",
      "   8. PaymentMethod       : 0.0611\r\n",
      "   9. StreamingTV         : 0.0611\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 175.3s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 93,505 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4706 | Val Loss: 0.4663 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4663)\r\n",
      "Epoch 001 | Train Loss: 0.4333 | Val Loss: 0.4398 | Time: 2.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4398)\r\n",
      "Epoch 002 | Train Loss: 0.4253 | Val Loss: 0.4390 | Time: 2.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4390)\r\n",
      "Epoch 003 | Train Loss: 0.4206 | Val Loss: 0.4289 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 004 | Train Loss: 0.4217 | Val Loss: 0.4263 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4263)\r\n",
      "Epoch 005 | Train Loss: 0.4167 | Val Loss: 0.4140 | Time: 2.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4140)\r\n",
      "Epoch 006 | Train Loss: 0.4120 | Val Loss: 0.4191 | Time: 2.45s\r\n",
      "Epoch 007 | Train Loss: 0.4097 | Val Loss: 0.4170 | Time: 2.47s\r\n",
      "Epoch 008 | Train Loss: 0.4118 | Val Loss: 0.4180 | Time: 2.42s\r\n",
      "Epoch 009 | Train Loss: 0.4108 | Val Loss: 0.4176 | Time: 2.54s\r\n",
      "Epoch 010 | Train Loss: 0.4092 | Val Loss: 0.4157 | Time: 2.46s\r\n",
      "Epoch 011 | Train Loss: 0.4078 | Val Loss: 0.4160 | Time: 2.44s\r\n",
      "Epoch 012 | Train Loss: 0.4045 | Val Loss: 0.4129 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 013 | Train Loss: 0.4063 | Val Loss: 0.4222 | Time: 2.49s\r\n",
      "Epoch 014 | Train Loss: 0.4044 | Val Loss: 0.4190 | Time: 2.51s\r\n",
      "Epoch 015 | Train Loss: 0.4041 | Val Loss: 0.4254 | Time: 2.44s\r\n",
      "Epoch 016 | Train Loss: 0.4008 | Val Loss: 0.4312 | Time: 2.44s\r\n",
      "Epoch 017 | Train Loss: 0.4003 | Val Loss: 0.4293 | Time: 2.49s\r\n",
      "Epoch 018 | Train Loss: 0.3999 | Val Loss: 0.4178 | Time: 2.44s\r\n",
      "Epoch 019 | Train Loss: 0.3970 | Val Loss: 0.4198 | Time: 2.44s\r\n",
      "Epoch 020 | Train Loss: 0.3984 | Val Loss: 0.4282 | Time: 2.45s\r\n",
      "Epoch 021 | Train Loss: 0.3970 | Val Loss: 0.4206 | Time: 2.47s\r\n",
      "Epoch 022 | Train Loss: 0.3932 | Val Loss: 0.4267 | Time: 2.46s\r\n",
      "Epoch 023 | Train Loss: 0.3935 | Val Loss: 0.4257 | Time: 2.47s\r\n",
      "Epoch 024 | Train Loss: 0.3929 | Val Loss: 0.4309 | Time: 2.46s\r\n",
      "Epoch 025 | Train Loss: 0.3888 | Val Loss: 0.4270 | Time: 2.50s\r\n",
      "Epoch 026 | Train Loss: 0.3882 | Val Loss: 0.4387 | Time: 2.48s\r\n",
      "Epoch 027 | Train Loss: 0.3895 | Val Loss: 0.4361 | Time: 2.46s\r\n",
      "Epoch 028 | Train Loss: 0.3883 | Val Loss: 0.4319 | Time: 2.45s\r\n",
      "Epoch 029 | Train Loss: 0.3849 | Val Loss: 0.4367 | Time: 2.49s\r\n",
      "Epoch 030 | Train Loss: 0.3865 | Val Loss: 0.4315 | Time: 2.45s\r\n",
      "\r\n",
      "Early stopping à l'époque 30 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 12, val_loss: 0.4129)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0687\r\n",
      "   2. PhoneService        : 0.0630\r\n",
      "   3. TechSupport         : 0.0619\r\n",
      "   4. OnlineBackup        : 0.0581\r\n",
      "   5. TotalCharges        : 0.0580\r\n",
      "   6. SeniorCitizen       : 0.0572\r\n",
      "   7. MultipleLines       : 0.0562\r\n",
      "   8. MonthlyCharges      : 0.0536\r\n",
      "   9. gender              : 0.0526\r\n",
      "  10. InternetService     : 0.0521\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 9 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0687\r\n",
      "   2. PhoneService         (CAT): 0.0630\r\n",
      "   3. TechSupport          (CAT): 0.0619\r\n",
      "   4. OnlineBackup         (CAT): 0.0581\r\n",
      "   5. TotalCharges         (NUM): 0.0580\r\n",
      "   6. SeniorCitizen        (CAT): 0.0572\r\n",
      "   7. MultipleLines        (CAT): 0.0562\r\n",
      "   8. MonthlyCharges       (NUM): 0.0536\r\n",
      "   9. gender               (CAT): 0.0526\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'MonthlyCharges'] → indices [2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'PhoneService', 'TechSupport', 'OnlineBackup', 'SeniorCitizen', 'MultipleLines', 'gender'] → indices [15, 4, 10, 8, 1, 5, 0]\r\n",
      "📊 Features sélectionnées: 2 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 831,873 paramètres\r\n",
      "🔗 Sparsité d'attention: 74.00%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5952 | Val Loss: 0.5981 | Time: 3.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5981)\r\n",
      "Epoch 001 | Train Loss: 0.5713 | Val Loss: 0.5404 | Time: 3.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5404)\r\n",
      "Epoch 002 | Train Loss: 0.5416 | Val Loss: 0.5624 | Time: 3.79s\r\n",
      "Epoch 003 | Train Loss: 0.5402 | Val Loss: 0.5118 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5118)\r\n",
      "Epoch 004 | Train Loss: 0.5318 | Val Loss: 0.5162 | Time: 3.82s\r\n",
      "Epoch 005 | Train Loss: 0.5264 | Val Loss: 0.4960 | Time: 3.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4960)\r\n",
      "Epoch 006 | Train Loss: 0.5273 | Val Loss: 0.5457 | Time: 3.82s\r\n",
      "Epoch 007 | Train Loss: 0.5350 | Val Loss: 0.4901 | Time: 3.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4901)\r\n",
      "Epoch 008 | Train Loss: 0.5219 | Val Loss: 0.5038 | Time: 3.80s\r\n",
      "Epoch 009 | Train Loss: 0.5247 | Val Loss: 0.4896 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4896)\r\n",
      "Epoch 010 | Train Loss: 0.5496 | Val Loss: 0.5874 | Time: 3.79s\r\n",
      "Epoch 011 | Train Loss: 0.5374 | Val Loss: 0.5000 | Time: 3.80s\r\n",
      "Epoch 012 | Train Loss: 0.5142 | Val Loss: 0.5198 | Time: 3.83s\r\n",
      "Epoch 013 | Train Loss: 0.5127 | Val Loss: 0.5464 | Time: 3.94s\r\n",
      "Epoch 014 | Train Loss: 0.5193 | Val Loss: 0.5068 | Time: 3.84s\r\n",
      "Epoch 015 | Train Loss: 0.5072 | Val Loss: 0.4936 | Time: 3.82s\r\n",
      "Epoch 016 | Train Loss: 0.5117 | Val Loss: 0.4965 | Time: 3.81s\r\n",
      "Epoch 017 | Train Loss: 0.5085 | Val Loss: 0.5361 | Time: 3.86s\r\n",
      "Epoch 018 | Train Loss: 0.5544 | Val Loss: 0.5626 | Time: 3.82s\r\n",
      "Epoch 019 | Train Loss: 0.5329 | Val Loss: 0.4651 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4651)\r\n",
      "Epoch 020 | Train Loss: 0.5066 | Val Loss: 0.4604 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4604)\r\n",
      "Epoch 021 | Train Loss: 0.5056 | Val Loss: 0.4617 | Time: 3.92s\r\n",
      "Epoch 022 | Train Loss: 0.5093 | Val Loss: 0.4879 | Time: 3.86s\r\n",
      "Epoch 023 | Train Loss: 0.5104 | Val Loss: 0.4693 | Time: 3.80s\r\n",
      "Epoch 024 | Train Loss: 0.4987 | Val Loss: 0.4827 | Time: 3.81s\r\n",
      "Epoch 025 | Train Loss: 0.5044 | Val Loss: 0.4692 | Time: 3.85s\r\n",
      "Epoch 026 | Train Loss: 0.5121 | Val Loss: 0.4834 | Time: 3.81s\r\n",
      "Epoch 027 | Train Loss: 0.5080 | Val Loss: 0.4694 | Time: 3.84s\r\n",
      "Epoch 028 | Train Loss: 0.5112 | Val Loss: 0.4592 | Time: 3.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4592)\r\n",
      "Epoch 029 | Train Loss: 0.5099 | Val Loss: 0.4785 | Time: 3.83s\r\n",
      "Epoch 030 | Train Loss: 0.5035 | Val Loss: 0.4603 | Time: 3.93s\r\n",
      "Epoch 031 | Train Loss: 0.4976 | Val Loss: 0.4668 | Time: 3.80s\r\n",
      "Epoch 032 | Train Loss: 0.4943 | Val Loss: 0.5032 | Time: 3.82s\r\n",
      "Epoch 033 | Train Loss: 0.4945 | Val Loss: 0.4803 | Time: 3.76s\r\n",
      "Epoch 034 | Train Loss: 0.4898 | Val Loss: 0.4508 | Time: 3.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4508)\r\n",
      "Epoch 035 | Train Loss: 0.5037 | Val Loss: 0.4674 | Time: 3.81s\r\n",
      "Epoch 036 | Train Loss: 0.4975 | Val Loss: 0.4602 | Time: 3.74s\r\n",
      "Epoch 037 | Train Loss: 0.4918 | Val Loss: 0.4481 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4481)\r\n",
      "Epoch 038 | Train Loss: 0.5078 | Val Loss: 0.5069 | Time: 3.92s\r\n",
      "Epoch 039 | Train Loss: 0.5052 | Val Loss: 0.4701 | Time: 3.77s\r\n",
      "Epoch 040 | Train Loss: 0.4919 | Val Loss: 0.4627 | Time: 3.82s\r\n",
      "Epoch 041 | Train Loss: 0.4966 | Val Loss: 0.4542 | Time: 3.79s\r\n",
      "Epoch 042 | Train Loss: 0.4928 | Val Loss: 0.4403 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4403)\r\n",
      "Epoch 043 | Train Loss: 0.4936 | Val Loss: 0.4569 | Time: 3.83s\r\n",
      "Epoch 044 | Train Loss: 0.5094 | Val Loss: 0.4729 | Time: 3.80s\r\n",
      "Epoch 045 | Train Loss: 0.4983 | Val Loss: 0.4621 | Time: 3.84s\r\n",
      "Epoch 046 | Train Loss: 0.5000 | Val Loss: 0.4438 | Time: 3.92s\r\n",
      "Epoch 047 | Train Loss: 0.4952 | Val Loss: 0.4563 | Time: 3.77s\r\n",
      "Epoch 048 | Train Loss: 0.4820 | Val Loss: 0.4368 | Time: 3.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 049 | Train Loss: 0.4910 | Val Loss: 0.4501 | Time: 3.78s\r\n",
      "✅ Meilleur modèle Random chargé (époque 48, val_loss: 0.4368)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.2988\r\n",
      "   2. gender               (CAT): 0.2661\r\n",
      "   3. OnlineBackup         (CAT): 0.1664\r\n",
      "   4. MonthlyCharges       (NUM): 0.0890\r\n",
      "   5. SeniorCitizen        (CAT): 0.0617\r\n",
      "   6. TotalCharges         (NUM): 0.0491\r\n",
      "   7. PaymentMethod        (CAT): 0.0294\r\n",
      "   8. TechSupport          (CAT): 0.0203\r\n",
      "   9. MultipleLines        (CAT): 0.0193\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.2988\r\n",
      "   2. gender              : 0.2661\r\n",
      "   3. OnlineBackup        : 0.1664\r\n",
      "   4. MonthlyCharges      : 0.0890\r\n",
      "   5. SeniorCitizen       : 0.0617\r\n",
      "   6. TotalCharges        : 0.0491\r\n",
      "   7. PaymentMethod       : 0.0294\r\n",
      "   8. TechSupport         : 0.0203\r\n",
      "   9. MultipleLines       : 0.0193\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_15/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 270.9s ===\r\n",
      "\u001b[32m[I 2025-07-19 23:20:52,745]\u001b[0m Trial 15 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 3, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 64, 'attention_dropout_stage1': 0.10839880225935454, 'ffn_dropout_stage1': 0.16321803982384786, 'residual_dropout_stage1': 0.15917123448499335, 'lr_stage1': 0.0009347463354382333, 'weight_decay_stage1': 0.00016912974449257055, 'd_token_stage2': 128, 'n_blocks_stage2': 5, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.16312298422867838, 'ffn_dropout_stage2': 0.1739125568141941, 'residual_dropout_stage2': 0.1112689491980845, 'lr_stage2': 0.0247871183285927, 'weight_decay_stage2': 0.001059099632448476, 'batch_size': 64, 'patience': 18, 'embedding_type': 'Q', 'M': 9, 'k': 4}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  64%|████▍  | 16/25 [5:07:55<3:02:29, 1216.62s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: LR\r\n",
      "Modèle FTT+ créé avec 120,097 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4880 | Val Loss: 0.4622 | Time: 6.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4622)\r\n",
      "Epoch 001 | Train Loss: 0.4445 | Val Loss: 0.4421 | Time: 6.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4421)\r\n",
      "Epoch 002 | Train Loss: 0.4377 | Val Loss: 0.4324 | Time: 6.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4324)\r\n",
      "Epoch 003 | Train Loss: 0.4332 | Val Loss: 0.4304 | Time: 6.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4304)\r\n",
      "Epoch 004 | Train Loss: 0.4282 | Val Loss: 0.4282 | Time: 6.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4282)\r\n",
      "Epoch 005 | Train Loss: 0.4226 | Val Loss: 0.4260 | Time: 6.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4260)\r\n",
      "Epoch 006 | Train Loss: 0.4221 | Val Loss: 0.4251 | Time: 6.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4251)\r\n",
      "Epoch 007 | Train Loss: 0.4214 | Val Loss: 0.4303 | Time: 6.49s\r\n",
      "Epoch 008 | Train Loss: 0.4202 | Val Loss: 0.4237 | Time: 6.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4237)\r\n",
      "Epoch 009 | Train Loss: 0.4181 | Val Loss: 0.4253 | Time: 6.37s\r\n",
      "Epoch 010 | Train Loss: 0.4179 | Val Loss: 0.4290 | Time: 6.39s\r\n",
      "Epoch 011 | Train Loss: 0.4173 | Val Loss: 0.4302 | Time: 6.45s\r\n",
      "Epoch 012 | Train Loss: 0.4165 | Val Loss: 0.4227 | Time: 6.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 013 | Train Loss: 0.4144 | Val Loss: 0.4248 | Time: 6.42s\r\n",
      "Epoch 014 | Train Loss: 0.4165 | Val Loss: 0.4273 | Time: 6.35s\r\n",
      "Epoch 015 | Train Loss: 0.4129 | Val Loss: 0.4355 | Time: 6.40s\r\n",
      "Epoch 016 | Train Loss: 0.4168 | Val Loss: 0.4180 | Time: 6.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 017 | Train Loss: 0.4112 | Val Loss: 0.4228 | Time: 6.62s\r\n",
      "Epoch 018 | Train Loss: 0.4125 | Val Loss: 0.4288 | Time: 6.44s\r\n",
      "Epoch 019 | Train Loss: 0.4069 | Val Loss: 0.4260 | Time: 6.44s\r\n",
      "Epoch 020 | Train Loss: 0.4115 | Val Loss: 0.4297 | Time: 6.47s\r\n",
      "Epoch 021 | Train Loss: 0.4105 | Val Loss: 0.4271 | Time: 6.34s\r\n",
      "Epoch 022 | Train Loss: 0.4106 | Val Loss: 0.4278 | Time: 6.52s\r\n",
      "Epoch 023 | Train Loss: 0.4065 | Val Loss: 0.4464 | Time: 6.47s\r\n",
      "Epoch 024 | Train Loss: 0.4096 | Val Loss: 0.4320 | Time: 6.42s\r\n",
      "Epoch 025 | Train Loss: 0.4084 | Val Loss: 0.4263 | Time: 6.50s\r\n",
      "Epoch 026 | Train Loss: 0.4071 | Val Loss: 0.4422 | Time: 6.46s\r\n",
      "Epoch 027 | Train Loss: 0.4044 | Val Loss: 0.4394 | Time: 6.49s\r\n",
      "Epoch 028 | Train Loss: 0.4051 | Val Loss: 0.4315 | Time: 6.47s\r\n",
      "Epoch 029 | Train Loss: 0.4051 | Val Loss: 0.4239 | Time: 6.46s\r\n",
      "Epoch 030 | Train Loss: 0.4032 | Val Loss: 0.4310 | Time: 6.38s\r\n",
      "Epoch 031 | Train Loss: 0.4050 | Val Loss: 0.4382 | Time: 6.42s\r\n",
      "Epoch 032 | Train Loss: 0.4048 | Val Loss: 0.4436 | Time: 6.74s\r\n",
      "Epoch 033 | Train Loss: 0.4020 | Val Loss: 0.4378 | Time: 6.41s\r\n",
      "Epoch 034 | Train Loss: 0.4005 | Val Loss: 0.4418 | Time: 6.50s\r\n",
      "Epoch 035 | Train Loss: 0.4010 | Val Loss: 0.4484 | Time: 6.37s\r\n",
      "Epoch 036 | Train Loss: 0.4007 | Val Loss: 0.4431 | Time: 6.47s\r\n",
      "Epoch 037 | Train Loss: 0.4033 | Val Loss: 0.4338 | Time: 6.47s\r\n",
      "Epoch 038 | Train Loss: 0.3990 | Val Loss: 0.4518 | Time: 6.36s\r\n",
      "\r\n",
      "Early stopping à l'époque 38 (patience: 22)\r\n",
      "✅ Meilleur modèle chargé (époque 16, val_loss: 0.4180)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0590\r\n",
      "   2. tenure              : 0.0588\r\n",
      "   3. StreamingTV         : 0.0567\r\n",
      "   4. TotalCharges        : 0.0558\r\n",
      "   5. MultipleLines       : 0.0548\r\n",
      "   6. PhoneService        : 0.0547\r\n",
      "   7. SeniorCitizen       : 0.0544\r\n",
      "   8. TechSupport         : 0.0538\r\n",
      "   9. PaperlessBilling    : 0.0530\r\n",
      "  10. Dependents          : 0.0528\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 18 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0590\r\n",
      "   2. tenure               (NUM): 0.0588\r\n",
      "   3. StreamingTV          (CAT): 0.0567\r\n",
      "   4. TotalCharges         (NUM): 0.0558\r\n",
      "   5. MultipleLines        (CAT): 0.0548\r\n",
      "   6. PhoneService         (CAT): 0.0547\r\n",
      "   7. SeniorCitizen        (CAT): 0.0544\r\n",
      "   8. TechSupport          (CAT): 0.0538\r\n",
      "   9. PaperlessBilling     (CAT): 0.0530\r\n",
      "  10. Dependents           (CAT): 0.0528\r\n",
      "  11. Contract             (CAT): 0.0508\r\n",
      "  12. DeviceProtection     (CAT): 0.0508\r\n",
      "  13. InternetService      (CAT): 0.0504\r\n",
      "  14. Partner              (CAT): 0.0500\r\n",
      "  15. MonthlyCharges       (NUM): 0.0498\r\n",
      "  16. StreamingMovies      (CAT): 0.0497\r\n",
      "  17. OnlineSecurity       (CAT): 0.0496\r\n",
      "  18. gender               (CAT): 0.0492\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'StreamingTV', 'MultipleLines', 'PhoneService', 'SeniorCitizen', 'TechSupport', 'PaperlessBilling', 'Dependents', 'Contract', 'DeviceProtection', 'InternetService', 'Partner', 'StreamingMovies', 'OnlineSecurity', 'gender'] → indices [15, 11, 5, 4, 1, 10, 14, 3, 13, 9, 6, 2, 12, 7, 0]\r\n",
      "📊 Features sélectionnées: 3 numériques, 15 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 137,409 paramètres\r\n",
      "🔗 Sparsité d'attention: 87.26%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6199 | Val Loss: 0.5461 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5461)\r\n",
      "Epoch 001 | Train Loss: 0.5420 | Val Loss: 0.5015 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5015)\r\n",
      "Epoch 002 | Train Loss: 0.5088 | Val Loss: 0.4700 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4700)\r\n",
      "Epoch 003 | Train Loss: 0.4878 | Val Loss: 0.4542 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4542)\r\n",
      "Epoch 004 | Train Loss: 0.4769 | Val Loss: 0.4422 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4422)\r\n",
      "Epoch 005 | Train Loss: 0.4700 | Val Loss: 0.4345 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 006 | Train Loss: 0.4616 | Val Loss: 0.4299 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4299)\r\n",
      "Epoch 007 | Train Loss: 0.4534 | Val Loss: 0.4282 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4282)\r\n",
      "Epoch 008 | Train Loss: 0.4518 | Val Loss: 0.4254 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 009 | Train Loss: 0.4503 | Val Loss: 0.4238 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4238)\r\n",
      "Epoch 010 | Train Loss: 0.4494 | Val Loss: 0.4234 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4234)\r\n",
      "Epoch 011 | Train Loss: 0.4504 | Val Loss: 0.4215 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4215)\r\n",
      "Epoch 012 | Train Loss: 0.4429 | Val Loss: 0.4214 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 013 | Train Loss: 0.4454 | Val Loss: 0.4212 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4212)\r\n",
      "Epoch 014 | Train Loss: 0.4446 | Val Loss: 0.4237 | Time: 3.25s\r\n",
      "Epoch 015 | Train Loss: 0.4428 | Val Loss: 0.4200 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 016 | Train Loss: 0.4380 | Val Loss: 0.4218 | Time: 3.24s\r\n",
      "Epoch 017 | Train Loss: 0.4391 | Val Loss: 0.4190 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 018 | Train Loss: 0.4386 | Val Loss: 0.4188 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 019 | Train Loss: 0.4406 | Val Loss: 0.4216 | Time: 3.26s\r\n",
      "Epoch 020 | Train Loss: 0.4389 | Val Loss: 0.4185 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4185)\r\n",
      "Epoch 021 | Train Loss: 0.4432 | Val Loss: 0.4164 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4164)\r\n",
      "Epoch 022 | Train Loss: 0.4403 | Val Loss: 0.4158 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4158)\r\n",
      "Epoch 023 | Train Loss: 0.4359 | Val Loss: 0.4166 | Time: 3.18s\r\n",
      "Epoch 024 | Train Loss: 0.4361 | Val Loss: 0.4153 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4153)\r\n",
      "Epoch 025 | Train Loss: 0.4344 | Val Loss: 0.4194 | Time: 3.29s\r\n",
      "Epoch 026 | Train Loss: 0.4310 | Val Loss: 0.4194 | Time: 3.19s\r\n",
      "Epoch 027 | Train Loss: 0.4348 | Val Loss: 0.4176 | Time: 3.25s\r\n",
      "Epoch 028 | Train Loss: 0.4362 | Val Loss: 0.4175 | Time: 3.25s\r\n",
      "Epoch 029 | Train Loss: 0.4365 | Val Loss: 0.4145 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4145)\r\n",
      "Epoch 030 | Train Loss: 0.4356 | Val Loss: 0.4150 | Time: 3.24s\r\n",
      "Epoch 031 | Train Loss: 0.4336 | Val Loss: 0.4165 | Time: 3.21s\r\n",
      "Epoch 032 | Train Loss: 0.4347 | Val Loss: 0.4156 | Time: 3.22s\r\n",
      "Epoch 033 | Train Loss: 0.4356 | Val Loss: 0.4134 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4134)\r\n",
      "Epoch 034 | Train Loss: 0.4324 | Val Loss: 0.4129 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 035 | Train Loss: 0.4337 | Val Loss: 0.4131 | Time: 3.32s\r\n",
      "Epoch 036 | Train Loss: 0.4319 | Val Loss: 0.4151 | Time: 3.28s\r\n",
      "Epoch 037 | Train Loss: 0.4335 | Val Loss: 0.4142 | Time: 3.21s\r\n",
      "Epoch 038 | Train Loss: 0.4326 | Val Loss: 0.4144 | Time: 3.20s\r\n",
      "Epoch 039 | Train Loss: 0.4350 | Val Loss: 0.4136 | Time: 3.19s\r\n",
      "Epoch 040 | Train Loss: 0.4361 | Val Loss: 0.4137 | Time: 3.26s\r\n",
      "Epoch 041 | Train Loss: 0.4331 | Val Loss: 0.4144 | Time: 3.21s\r\n",
      "Epoch 042 | Train Loss: 0.4321 | Val Loss: 0.4131 | Time: 3.23s\r\n",
      "Epoch 043 | Train Loss: 0.4314 | Val Loss: 0.4112 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4112)\r\n",
      "Epoch 044 | Train Loss: 0.4263 | Val Loss: 0.4138 | Time: 3.31s\r\n",
      "Epoch 045 | Train Loss: 0.4282 | Val Loss: 0.4154 | Time: 3.22s\r\n",
      "Epoch 046 | Train Loss: 0.4268 | Val Loss: 0.4177 | Time: 3.29s\r\n",
      "Epoch 047 | Train Loss: 0.4288 | Val Loss: 0.4152 | Time: 3.25s\r\n",
      "Epoch 048 | Train Loss: 0.4291 | Val Loss: 0.4151 | Time: 3.25s\r\n",
      "Epoch 049 | Train Loss: 0.4283 | Val Loss: 0.4155 | Time: 3.28s\r\n",
      "✅ Meilleur modèle Random chargé (époque 43, val_loss: 0.4112)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. DeviceProtection     (CAT): 0.1118\r\n",
      "   2. tenure               (NUM): 0.1015\r\n",
      "   3. MultipleLines        (CAT): 0.0782\r\n",
      "   4. SeniorCitizen        (CAT): 0.0682\r\n",
      "   5. Contract             (CAT): 0.0599\r\n",
      "   6. InternetService      (CAT): 0.0549\r\n",
      "   7. TechSupport          (CAT): 0.0530\r\n",
      "   8. MonthlyCharges       (NUM): 0.0527\r\n",
      "   9. StreamingMovies      (CAT): 0.0458\r\n",
      "  10. OnlineSecurity       (CAT): 0.0436\r\n",
      "  11. PaperlessBilling     (CAT): 0.0421\r\n",
      "  12. gender               (CAT): 0.0420\r\n",
      "  13. Partner              (CAT): 0.0418\r\n",
      "  14. PaymentMethod        (CAT): 0.0414\r\n",
      "  15. PhoneService         (CAT): 0.0410\r\n",
      "  16. Dependents           (CAT): 0.0410\r\n",
      "  17. StreamingTV          (CAT): 0.0406\r\n",
      "  18. TotalCharges         (NUM): 0.0403\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.1118\r\n",
      "   2. tenure              : 0.1015\r\n",
      "   3. MultipleLines       : 0.0782\r\n",
      "   4. SeniorCitizen       : 0.0682\r\n",
      "   5. Contract            : 0.0599\r\n",
      "   6. InternetService     : 0.0549\r\n",
      "   7. TechSupport         : 0.0530\r\n",
      "   8. MonthlyCharges      : 0.0527\r\n",
      "   9. StreamingMovies     : 0.0458\r\n",
      "  10. OnlineSecurity      : 0.0436\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 417.6s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: LR\r\n",
      "Modèle FTT+ créé avec 120,097 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4980 | Val Loss: 0.4462 | Time: 6.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4462)\r\n",
      "Epoch 001 | Train Loss: 0.4463 | Val Loss: 0.4372 | Time: 6.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4372)\r\n",
      "Epoch 002 | Train Loss: 0.4320 | Val Loss: 0.4249 | Time: 6.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4249)\r\n",
      "Epoch 003 | Train Loss: 0.4268 | Val Loss: 0.4229 | Time: 6.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4229)\r\n",
      "Epoch 004 | Train Loss: 0.4186 | Val Loss: 0.4218 | Time: 6.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4218)\r\n",
      "Epoch 005 | Train Loss: 0.4183 | Val Loss: 0.4195 | Time: 6.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4195)\r\n",
      "Epoch 006 | Train Loss: 0.4132 | Val Loss: 0.4184 | Time: 6.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 007 | Train Loss: 0.4125 | Val Loss: 0.4174 | Time: 6.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "Epoch 008 | Train Loss: 0.4132 | Val Loss: 0.4135 | Time: 6.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4135)\r\n",
      "Epoch 009 | Train Loss: 0.4108 | Val Loss: 0.4137 | Time: 6.44s\r\n",
      "Epoch 010 | Train Loss: 0.4081 | Val Loss: 0.4141 | Time: 6.42s\r\n",
      "Epoch 011 | Train Loss: 0.4092 | Val Loss: 0.4148 | Time: 6.48s\r\n",
      "Epoch 012 | Train Loss: 0.4089 | Val Loss: 0.4133 | Time: 6.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4133)\r\n",
      "Epoch 013 | Train Loss: 0.4060 | Val Loss: 0.4128 | Time: 6.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4128)\r\n",
      "Epoch 014 | Train Loss: 0.4056 | Val Loss: 0.4149 | Time: 6.47s\r\n",
      "Epoch 015 | Train Loss: 0.4042 | Val Loss: 0.4145 | Time: 6.43s\r\n",
      "Epoch 016 | Train Loss: 0.4035 | Val Loss: 0.4137 | Time: 6.61s\r\n",
      "Epoch 017 | Train Loss: 0.4047 | Val Loss: 0.4152 | Time: 6.43s\r\n",
      "Epoch 018 | Train Loss: 0.4022 | Val Loss: 0.4164 | Time: 6.44s\r\n",
      "Epoch 019 | Train Loss: 0.4028 | Val Loss: 0.4145 | Time: 6.45s\r\n",
      "Epoch 020 | Train Loss: 0.4003 | Val Loss: 0.4130 | Time: 6.41s\r\n",
      "Epoch 021 | Train Loss: 0.3999 | Val Loss: 0.4145 | Time: 6.68s\r\n",
      "Epoch 022 | Train Loss: 0.3982 | Val Loss: 0.4137 | Time: 6.41s\r\n",
      "Epoch 023 | Train Loss: 0.3977 | Val Loss: 0.4145 | Time: 6.43s\r\n",
      "Epoch 024 | Train Loss: 0.3995 | Val Loss: 0.4133 | Time: 6.38s\r\n",
      "Epoch 025 | Train Loss: 0.3971 | Val Loss: 0.4136 | Time: 6.41s\r\n",
      "Epoch 026 | Train Loss: 0.3968 | Val Loss: 0.4150 | Time: 6.48s\r\n",
      "Epoch 027 | Train Loss: 0.3937 | Val Loss: 0.4136 | Time: 6.38s\r\n",
      "Epoch 028 | Train Loss: 0.3936 | Val Loss: 0.4173 | Time: 6.41s\r\n",
      "Epoch 029 | Train Loss: 0.3944 | Val Loss: 0.4170 | Time: 6.41s\r\n",
      "Epoch 030 | Train Loss: 0.3920 | Val Loss: 0.4172 | Time: 6.43s\r\n",
      "Epoch 031 | Train Loss: 0.3956 | Val Loss: 0.4216 | Time: 6.49s\r\n",
      "Epoch 032 | Train Loss: 0.3906 | Val Loss: 0.4262 | Time: 6.42s\r\n",
      "Epoch 033 | Train Loss: 0.3907 | Val Loss: 0.4233 | Time: 6.45s\r\n",
      "Epoch 034 | Train Loss: 0.3953 | Val Loss: 0.4190 | Time: 6.48s\r\n",
      "Epoch 035 | Train Loss: 0.3921 | Val Loss: 0.4225 | Time: 6.41s\r\n",
      "\r\n",
      "Early stopping à l'époque 35 (patience: 22)\r\n",
      "✅ Meilleur modèle chargé (époque 13, val_loss: 0.4128)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0801\r\n",
      "   2. InternetService     : 0.0647\r\n",
      "   3. gender              : 0.0542\r\n",
      "   4. PhoneService        : 0.0542\r\n",
      "   5. SeniorCitizen       : 0.0541\r\n",
      "   6. PaperlessBilling    : 0.0530\r\n",
      "   7. TotalCharges        : 0.0522\r\n",
      "   8. MonthlyCharges      : 0.0522\r\n",
      "   9. StreamingMovies     : 0.0520\r\n",
      "  10. StreamingTV         : 0.0520\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 18 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0801\r\n",
      "   2. InternetService      (CAT): 0.0647\r\n",
      "   3. gender               (CAT): 0.0542\r\n",
      "   4. PhoneService         (CAT): 0.0542\r\n",
      "   5. SeniorCitizen        (CAT): 0.0541\r\n",
      "   6. PaperlessBilling     (CAT): 0.0530\r\n",
      "   7. TotalCharges         (NUM): 0.0522\r\n",
      "   8. MonthlyCharges       (NUM): 0.0522\r\n",
      "   9. StreamingMovies      (CAT): 0.0520\r\n",
      "  10. StreamingTV          (CAT): 0.0520\r\n",
      "  11. TechSupport          (CAT): 0.0519\r\n",
      "  12. OnlineSecurity       (CAT): 0.0511\r\n",
      "  13. OnlineBackup         (CAT): 0.0495\r\n",
      "  14. MultipleLines        (CAT): 0.0476\r\n",
      "  15. Partner              (CAT): 0.0476\r\n",
      "  16. Contract             (CAT): 0.0465\r\n",
      "  17. tenure               (NUM): 0.0459\r\n",
      "  18. Dependents           (CAT): 0.0458\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'MonthlyCharges', 'tenure'] → indices [2, 1, 0]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'InternetService', 'gender', 'PhoneService', 'SeniorCitizen', 'PaperlessBilling', 'StreamingMovies', 'StreamingTV', 'TechSupport', 'OnlineSecurity', 'OnlineBackup', 'MultipleLines', 'Partner', 'Contract', 'Dependents'] → indices [15, 6, 0, 4, 1, 14, 12, 11, 10, 7, 8, 5, 2, 13, 3]\r\n",
      "📊 Features sélectionnées: 3 numériques, 15 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 137,409 paramètres\r\n",
      "🔗 Sparsité d'attention: 87.26%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5811 | Val Loss: 0.5045 | Time: 3.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5045)\r\n",
      "Epoch 001 | Train Loss: 0.5141 | Val Loss: 0.4663 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4663)\r\n",
      "Epoch 002 | Train Loss: 0.4843 | Val Loss: 0.4522 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4522)\r\n",
      "Epoch 003 | Train Loss: 0.4715 | Val Loss: 0.4432 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4432)\r\n",
      "Epoch 004 | Train Loss: 0.4671 | Val Loss: 0.4429 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4429)\r\n",
      "Epoch 005 | Train Loss: 0.4601 | Val Loss: 0.4365 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4365)\r\n",
      "Epoch 006 | Train Loss: 0.4560 | Val Loss: 0.4327 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 007 | Train Loss: 0.4508 | Val Loss: 0.4317 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 008 | Train Loss: 0.4500 | Val Loss: 0.4282 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4282)\r\n",
      "Epoch 009 | Train Loss: 0.4440 | Val Loss: 0.4250 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4250)\r\n",
      "Epoch 010 | Train Loss: 0.4464 | Val Loss: 0.4242 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4242)\r\n",
      "Epoch 011 | Train Loss: 0.4383 | Val Loss: 0.4229 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4229)\r\n",
      "Epoch 012 | Train Loss: 0.4457 | Val Loss: 0.4219 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4219)\r\n",
      "Epoch 013 | Train Loss: 0.4400 | Val Loss: 0.4205 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4205)\r\n",
      "Epoch 014 | Train Loss: 0.4419 | Val Loss: 0.4200 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 015 | Train Loss: 0.4422 | Val Loss: 0.4194 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 016 | Train Loss: 0.4406 | Val Loss: 0.4210 | Time: 3.26s\r\n",
      "Epoch 017 | Train Loss: 0.4421 | Val Loss: 0.4188 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 018 | Train Loss: 0.4327 | Val Loss: 0.4179 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4179)\r\n",
      "Epoch 019 | Train Loss: 0.4320 | Val Loss: 0.4187 | Time: 3.31s\r\n",
      "Epoch 020 | Train Loss: 0.4340 | Val Loss: 0.4161 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4161)\r\n",
      "Epoch 021 | Train Loss: 0.4329 | Val Loss: 0.4172 | Time: 3.29s\r\n",
      "Epoch 022 | Train Loss: 0.4309 | Val Loss: 0.4168 | Time: 3.22s\r\n",
      "Epoch 023 | Train Loss: 0.4345 | Val Loss: 0.4172 | Time: 3.26s\r\n",
      "Epoch 024 | Train Loss: 0.4367 | Val Loss: 0.4168 | Time: 3.26s\r\n",
      "Epoch 025 | Train Loss: 0.4324 | Val Loss: 0.4168 | Time: 3.25s\r\n",
      "Epoch 026 | Train Loss: 0.4291 | Val Loss: 0.4171 | Time: 3.25s\r\n",
      "Epoch 027 | Train Loss: 0.4312 | Val Loss: 0.4151 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4151)\r\n",
      "Epoch 028 | Train Loss: 0.4294 | Val Loss: 0.4164 | Time: 3.28s\r\n",
      "Epoch 029 | Train Loss: 0.4312 | Val Loss: 0.4151 | Time: 3.40s\r\n",
      "Epoch 030 | Train Loss: 0.4352 | Val Loss: 0.4134 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4134)\r\n",
      "Epoch 031 | Train Loss: 0.4275 | Val Loss: 0.4151 | Time: 3.25s\r\n",
      "Epoch 032 | Train Loss: 0.4283 | Val Loss: 0.4159 | Time: 3.25s\r\n",
      "Epoch 033 | Train Loss: 0.4265 | Val Loss: 0.4169 | Time: 3.20s\r\n",
      "Epoch 034 | Train Loss: 0.4273 | Val Loss: 0.4163 | Time: 3.30s\r\n",
      "Epoch 035 | Train Loss: 0.4307 | Val Loss: 0.4146 | Time: 3.23s\r\n",
      "Epoch 036 | Train Loss: 0.4280 | Val Loss: 0.4140 | Time: 3.24s\r\n",
      "Epoch 037 | Train Loss: 0.4272 | Val Loss: 0.4155 | Time: 3.28s\r\n",
      "Epoch 038 | Train Loss: 0.4265 | Val Loss: 0.4148 | Time: 3.30s\r\n",
      "Epoch 039 | Train Loss: 0.4231 | Val Loss: 0.4131 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4131)\r\n",
      "Epoch 040 | Train Loss: 0.4283 | Val Loss: 0.4119 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4119)\r\n",
      "Epoch 041 | Train Loss: 0.4244 | Val Loss: 0.4122 | Time: 3.24s\r\n",
      "Epoch 042 | Train Loss: 0.4280 | Val Loss: 0.4125 | Time: 3.24s\r\n",
      "Epoch 043 | Train Loss: 0.4244 | Val Loss: 0.4130 | Time: 3.25s\r\n",
      "Epoch 044 | Train Loss: 0.4226 | Val Loss: 0.4133 | Time: 3.23s\r\n",
      "Epoch 045 | Train Loss: 0.4267 | Val Loss: 0.4139 | Time: 3.25s\r\n",
      "Epoch 046 | Train Loss: 0.4222 | Val Loss: 0.4145 | Time: 3.31s\r\n",
      "Epoch 047 | Train Loss: 0.4210 | Val Loss: 0.4132 | Time: 3.27s\r\n",
      "Epoch 048 | Train Loss: 0.4237 | Val Loss: 0.4159 | Time: 3.50s\r\n",
      "Epoch 049 | Train Loss: 0.4262 | Val Loss: 0.4152 | Time: 3.35s\r\n",
      "✅ Meilleur modèle Random chargé (époque 40, val_loss: 0.4119)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.1042\r\n",
      "   2. MonthlyCharges       (NUM): 0.1015\r\n",
      "   3. SeniorCitizen        (CAT): 0.0901\r\n",
      "   4. TechSupport          (CAT): 0.0870\r\n",
      "   5. PaymentMethod        (CAT): 0.0850\r\n",
      "   6. tenure               (NUM): 0.0638\r\n",
      "   7. gender               (CAT): 0.0598\r\n",
      "   8. Dependents           (CAT): 0.0524\r\n",
      "   9. OnlineBackup         (CAT): 0.0377\r\n",
      "  10. Partner              (CAT): 0.0365\r\n",
      "  11. OnlineSecurity       (CAT): 0.0364\r\n",
      "  12. Contract             (CAT): 0.0358\r\n",
      "  13. PaperlessBilling     (CAT): 0.0358\r\n",
      "  14. StreamingMovies      (CAT): 0.0358\r\n",
      "  15. StreamingTV          (CAT): 0.0357\r\n",
      "  16. MultipleLines        (CAT): 0.0355\r\n",
      "  17. TotalCharges         (NUM): 0.0348\r\n",
      "  18. InternetService      (CAT): 0.0322\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.1042\r\n",
      "   2. MonthlyCharges      : 0.1015\r\n",
      "   3. SeniorCitizen       : 0.0901\r\n",
      "   4. TechSupport         : 0.0870\r\n",
      "   5. PaymentMethod       : 0.0850\r\n",
      "   6. tenure              : 0.0638\r\n",
      "   7. gender              : 0.0598\r\n",
      "   8. Dependents          : 0.0524\r\n",
      "   9. OnlineBackup        : 0.0377\r\n",
      "  10. Partner             : 0.0365\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 399.7s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: LR\r\n",
      "Modèle FTT+ créé avec 120,097 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4857 | Val Loss: 0.4563 | Time: 6.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4563)\r\n",
      "Epoch 001 | Train Loss: 0.4466 | Val Loss: 0.4461 | Time: 6.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4461)\r\n",
      "Epoch 002 | Train Loss: 0.4406 | Val Loss: 0.4328 | Time: 6.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4328)\r\n",
      "Epoch 003 | Train Loss: 0.4335 | Val Loss: 0.4280 | Time: 6.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4280)\r\n",
      "Epoch 004 | Train Loss: 0.4284 | Val Loss: 0.4256 | Time: 6.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4256)\r\n",
      "Epoch 005 | Train Loss: 0.4291 | Val Loss: 0.4245 | Time: 6.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4245)\r\n",
      "Epoch 006 | Train Loss: 0.4236 | Val Loss: 0.4184 | Time: 6.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 007 | Train Loss: 0.4181 | Val Loss: 0.4204 | Time: 6.47s\r\n",
      "Epoch 008 | Train Loss: 0.4198 | Val Loss: 0.4202 | Time: 6.54s\r\n",
      "Epoch 009 | Train Loss: 0.4167 | Val Loss: 0.4194 | Time: 6.41s\r\n",
      "Epoch 010 | Train Loss: 0.4172 | Val Loss: 0.4208 | Time: 6.45s\r\n",
      "Epoch 011 | Train Loss: 0.4164 | Val Loss: 0.4206 | Time: 6.43s\r\n",
      "Epoch 012 | Train Loss: 0.4151 | Val Loss: 0.4349 | Time: 6.42s\r\n",
      "Epoch 013 | Train Loss: 0.4113 | Val Loss: 0.4320 | Time: 6.55s\r\n",
      "Epoch 014 | Train Loss: 0.4105 | Val Loss: 0.4329 | Time: 6.44s\r\n",
      "Epoch 015 | Train Loss: 0.4119 | Val Loss: 0.4292 | Time: 6.38s\r\n",
      "Epoch 016 | Train Loss: 0.4095 | Val Loss: 0.4356 | Time: 6.46s\r\n",
      "Epoch 017 | Train Loss: 0.4103 | Val Loss: 0.4356 | Time: 6.45s\r\n",
      "Epoch 018 | Train Loss: 0.4108 | Val Loss: 0.4309 | Time: 6.51s\r\n",
      "Epoch 019 | Train Loss: 0.4076 | Val Loss: 0.4319 | Time: 6.52s\r\n",
      "Epoch 020 | Train Loss: 0.4070 | Val Loss: 0.4308 | Time: 6.47s\r\n",
      "Epoch 021 | Train Loss: 0.4073 | Val Loss: 0.4385 | Time: 6.36s\r\n",
      "Epoch 022 | Train Loss: 0.4062 | Val Loss: 0.4393 | Time: 6.46s\r\n",
      "Epoch 023 | Train Loss: 0.4031 | Val Loss: 0.4373 | Time: 6.49s\r\n",
      "Epoch 024 | Train Loss: 0.4080 | Val Loss: 0.4326 | Time: 6.35s\r\n",
      "Epoch 025 | Train Loss: 0.4023 | Val Loss: 0.4466 | Time: 6.43s\r\n",
      "Epoch 026 | Train Loss: 0.4027 | Val Loss: 0.4315 | Time: 6.42s\r\n",
      "Epoch 027 | Train Loss: 0.4001 | Val Loss: 0.4319 | Time: 6.43s\r\n",
      "Epoch 028 | Train Loss: 0.3998 | Val Loss: 0.4362 | Time: 6.50s\r\n",
      "\r\n",
      "Early stopping à l'époque 28 (patience: 22)\r\n",
      "✅ Meilleur modèle chargé (époque 6, val_loss: 0.4184)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.0572\r\n",
      "   2. OnlineBackup        : 0.0564\r\n",
      "   3. PaperlessBilling    : 0.0564\r\n",
      "   4. TotalCharges        : 0.0553\r\n",
      "   5. InternetService     : 0.0547\r\n",
      "   6. TechSupport         : 0.0545\r\n",
      "   7. Dependents          : 0.0543\r\n",
      "   8. PaymentMethod       : 0.0536\r\n",
      "   9. SeniorCitizen       : 0.0531\r\n",
      "  10. StreamingTV         : 0.0517\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 18 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. DeviceProtection     (CAT): 0.0572\r\n",
      "   2. OnlineBackup         (CAT): 0.0564\r\n",
      "   3. PaperlessBilling     (CAT): 0.0564\r\n",
      "   4. TotalCharges         (NUM): 0.0553\r\n",
      "   5. InternetService      (CAT): 0.0547\r\n",
      "   6. TechSupport          (CAT): 0.0545\r\n",
      "   7. Dependents           (CAT): 0.0543\r\n",
      "   8. PaymentMethod        (CAT): 0.0536\r\n",
      "   9. SeniorCitizen        (CAT): 0.0531\r\n",
      "  10. StreamingTV          (CAT): 0.0517\r\n",
      "  11. MultipleLines        (CAT): 0.0516\r\n",
      "  12. MonthlyCharges       (NUM): 0.0516\r\n",
      "  13. Contract             (CAT): 0.0514\r\n",
      "  14. OnlineSecurity       (CAT): 0.0510\r\n",
      "  15. tenure               (NUM): 0.0503\r\n",
      "  16. Partner              (CAT): 0.0499\r\n",
      "  17. StreamingMovies      (CAT): 0.0496\r\n",
      "  18. gender               (CAT): 0.0493\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'MonthlyCharges', 'tenure'] → indices [2, 1, 0]\r\n",
      "   - Catégorielles sélectionnées: ['DeviceProtection', 'OnlineBackup', 'PaperlessBilling', 'InternetService', 'TechSupport', 'Dependents', 'PaymentMethod', 'SeniorCitizen', 'StreamingTV', 'MultipleLines', 'Contract', 'OnlineSecurity', 'Partner', 'StreamingMovies', 'gender'] → indices [9, 8, 14, 6, 10, 3, 15, 1, 11, 5, 13, 7, 2, 12, 0]\r\n",
      "📊 Features sélectionnées: 3 numériques, 15 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 137,473 paramètres\r\n",
      "🔗 Sparsité d'attention: 87.26%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6479 | Val Loss: 0.5712 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5712)\r\n",
      "Epoch 001 | Train Loss: 0.5569 | Val Loss: 0.4740 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4740)\r\n",
      "Epoch 002 | Train Loss: 0.5085 | Val Loss: 0.4424 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4424)\r\n",
      "Epoch 003 | Train Loss: 0.4890 | Val Loss: 0.4390 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4390)\r\n",
      "Epoch 004 | Train Loss: 0.4748 | Val Loss: 0.4375 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4375)\r\n",
      "Epoch 005 | Train Loss: 0.4679 | Val Loss: 0.4345 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 006 | Train Loss: 0.4625 | Val Loss: 0.4319 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4319)\r\n",
      "Epoch 007 | Train Loss: 0.4592 | Val Loss: 0.4338 | Time: 3.29s\r\n",
      "Epoch 008 | Train Loss: 0.4580 | Val Loss: 0.4323 | Time: 3.32s\r\n",
      "Epoch 009 | Train Loss: 0.4558 | Val Loss: 0.4297 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 010 | Train Loss: 0.4580 | Val Loss: 0.4304 | Time: 3.26s\r\n",
      "Epoch 011 | Train Loss: 0.4520 | Val Loss: 0.4266 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4266)\r\n",
      "Epoch 012 | Train Loss: 0.4551 | Val Loss: 0.4266 | Time: 3.27s\r\n",
      "Epoch 013 | Train Loss: 0.4509 | Val Loss: 0.4304 | Time: 3.25s\r\n",
      "Epoch 014 | Train Loss: 0.4468 | Val Loss: 0.4255 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 015 | Train Loss: 0.4473 | Val Loss: 0.4240 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4240)\r\n",
      "Epoch 016 | Train Loss: 0.4457 | Val Loss: 0.4247 | Time: 3.25s\r\n",
      "Epoch 017 | Train Loss: 0.4453 | Val Loss: 0.4268 | Time: 3.32s\r\n",
      "Epoch 018 | Train Loss: 0.4442 | Val Loss: 0.4262 | Time: 3.23s\r\n",
      "Epoch 019 | Train Loss: 0.4423 | Val Loss: 0.4221 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4221)\r\n",
      "Epoch 020 | Train Loss: 0.4406 | Val Loss: 0.4259 | Time: 3.28s\r\n",
      "Epoch 021 | Train Loss: 0.4401 | Val Loss: 0.4204 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 022 | Train Loss: 0.4422 | Val Loss: 0.4202 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4202)\r\n",
      "Epoch 023 | Train Loss: 0.4357 | Val Loss: 0.4239 | Time: 3.23s\r\n",
      "Epoch 024 | Train Loss: 0.4421 | Val Loss: 0.4218 | Time: 3.25s\r\n",
      "Epoch 025 | Train Loss: 0.4345 | Val Loss: 0.4197 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 026 | Train Loss: 0.4378 | Val Loss: 0.4203 | Time: 3.28s\r\n",
      "Epoch 027 | Train Loss: 0.4414 | Val Loss: 0.4204 | Time: 3.33s\r\n",
      "Epoch 028 | Train Loss: 0.4359 | Val Loss: 0.4206 | Time: 3.21s\r\n",
      "Epoch 029 | Train Loss: 0.4353 | Val Loss: 0.4216 | Time: 3.30s\r\n",
      "Epoch 030 | Train Loss: 0.4332 | Val Loss: 0.4227 | Time: 3.29s\r\n",
      "Epoch 031 | Train Loss: 0.4306 | Val Loss: 0.4192 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 032 | Train Loss: 0.4341 | Val Loss: 0.4205 | Time: 3.28s\r\n",
      "Epoch 033 | Train Loss: 0.4362 | Val Loss: 0.4179 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4179)\r\n",
      "Epoch 034 | Train Loss: 0.4298 | Val Loss: 0.4176 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4176)\r\n",
      "Epoch 035 | Train Loss: 0.4322 | Val Loss: 0.4170 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 036 | Train Loss: 0.4342 | Val Loss: 0.4159 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 037 | Train Loss: 0.4338 | Val Loss: 0.4156 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4156)\r\n",
      "Epoch 038 | Train Loss: 0.4283 | Val Loss: 0.4168 | Time: 3.31s\r\n",
      "Epoch 039 | Train Loss: 0.4356 | Val Loss: 0.4173 | Time: 3.24s\r\n",
      "Epoch 040 | Train Loss: 0.4306 | Val Loss: 0.4182 | Time: 3.27s\r\n",
      "Epoch 041 | Train Loss: 0.4309 | Val Loss: 0.4184 | Time: 3.25s\r\n",
      "Epoch 042 | Train Loss: 0.4291 | Val Loss: 0.4184 | Time: 3.25s\r\n",
      "Epoch 043 | Train Loss: 0.4323 | Val Loss: 0.4181 | Time: 3.25s\r\n",
      "Epoch 044 | Train Loss: 0.4319 | Val Loss: 0.4188 | Time: 3.28s\r\n",
      "Epoch 045 | Train Loss: 0.4291 | Val Loss: 0.4193 | Time: 3.25s\r\n",
      "Epoch 046 | Train Loss: 0.4301 | Val Loss: 0.4184 | Time: 3.35s\r\n",
      "Epoch 047 | Train Loss: 0.4304 | Val Loss: 0.4174 | Time: 3.24s\r\n",
      "Epoch 048 | Train Loss: 0.4255 | Val Loss: 0.4184 | Time: 3.24s\r\n",
      "Epoch 049 | Train Loss: 0.4233 | Val Loss: 0.4214 | Time: 3.22s\r\n",
      "✅ Meilleur modèle Random chargé (époque 37, val_loss: 0.4156)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineBackup         (CAT): 0.1207\r\n",
      "   2. Partner              (CAT): 0.0957\r\n",
      "   3. Contract             (CAT): 0.0698\r\n",
      "   4. MultipleLines        (CAT): 0.0674\r\n",
      "   5. OnlineSecurity       (CAT): 0.0628\r\n",
      "   6. TechSupport          (CAT): 0.0628\r\n",
      "   7. PaperlessBilling     (CAT): 0.0578\r\n",
      "   8. MonthlyCharges       (NUM): 0.0440\r\n",
      "   9. StreamingTV          (CAT): 0.0438\r\n",
      "  10. TotalCharges         (NUM): 0.0436\r\n",
      "  11. gender               (CAT): 0.0429\r\n",
      "  12. PaymentMethod        (CAT): 0.0422\r\n",
      "  13. StreamingMovies      (CAT): 0.0418\r\n",
      "  14. SeniorCitizen        (CAT): 0.0415\r\n",
      "  15. tenure               (NUM): 0.0415\r\n",
      "  16. DeviceProtection     (CAT): 0.0413\r\n",
      "  17. Dependents           (CAT): 0.0409\r\n",
      "  18. InternetService      (CAT): 0.0396\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.1207\r\n",
      "   2. Partner             : 0.0957\r\n",
      "   3. Contract            : 0.0698\r\n",
      "   4. MultipleLines       : 0.0674\r\n",
      "   5. OnlineSecurity      : 0.0628\r\n",
      "   6. TechSupport         : 0.0628\r\n",
      "   7. PaperlessBilling    : 0.0578\r\n",
      "   8. MonthlyCharges      : 0.0440\r\n",
      "   9. StreamingTV         : 0.0438\r\n",
      "  10. TotalCharges        : 0.0436\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_16/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 354.8s ===\r\n",
      "\u001b[32m[I 2025-07-19 23:40:25,545]\u001b[0m Trial 16 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 4, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.1964276399164675, 'ffn_dropout_stage1': 0.21260730046359855, 'residual_dropout_stage1': 0.14121837459673708, 'lr_stage1': 0.0005356915080283581, 'weight_decay_stage1': 0.000427384986498085, 'd_token_stage2': 64, 'n_blocks_stage2': 2, 'n_heads_stage2': 4, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.12185715433659067, 'ffn_dropout_stage2': 0.14415776364054417, 'residual_dropout_stage2': 0.1307908216151681, 'lr_stage2': 3.67945572288103e-05, 'weight_decay_stage2': 0.01697046464588742, 'batch_size': 32, 'patience': 22, 'embedding_type': 'LR', 'M': 18, 'k': 5}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  68%|████▊  | 17/25 [5:27:28<2:40:27, 1203.44s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q-LR\r\n",
      "Modèle FTT+ créé avec 1,007,233 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4857 | Val Loss: 0.4217 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 001 | Train Loss: 0.4293 | Val Loss: 0.4129 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 002 | Train Loss: 0.4295 | Val Loss: 0.4082 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4082)\r\n",
      "Epoch 003 | Train Loss: 0.4245 | Val Loss: 0.4064 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4064)\r\n",
      "Epoch 004 | Train Loss: 0.4187 | Val Loss: 0.4066 | Time: 5.89s\r\n",
      "Epoch 005 | Train Loss: 0.4166 | Val Loss: 0.4060 | Time: 5.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4060)\r\n",
      "Epoch 006 | Train Loss: 0.4165 | Val Loss: 0.4062 | Time: 5.89s\r\n",
      "Epoch 007 | Train Loss: 0.4151 | Val Loss: 0.4072 | Time: 5.88s\r\n",
      "Epoch 008 | Train Loss: 0.4121 | Val Loss: 0.4055 | Time: 6.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4055)\r\n",
      "Epoch 009 | Train Loss: 0.4124 | Val Loss: 0.4057 | Time: 5.88s\r\n",
      "Epoch 010 | Train Loss: 0.4112 | Val Loss: 0.4062 | Time: 5.93s\r\n",
      "Epoch 011 | Train Loss: 0.4100 | Val Loss: 0.4056 | Time: 5.87s\r\n",
      "Epoch 012 | Train Loss: 0.4087 | Val Loss: 0.4070 | Time: 5.89s\r\n",
      "Epoch 013 | Train Loss: 0.4092 | Val Loss: 0.4063 | Time: 5.98s\r\n",
      "Epoch 014 | Train Loss: 0.4068 | Val Loss: 0.4086 | Time: 5.89s\r\n",
      "Epoch 015 | Train Loss: 0.4061 | Val Loss: 0.4100 | Time: 5.99s\r\n",
      "Epoch 016 | Train Loss: 0.4045 | Val Loss: 0.4118 | Time: 5.85s\r\n",
      "Epoch 017 | Train Loss: 0.4044 | Val Loss: 0.4096 | Time: 5.84s\r\n",
      "Epoch 018 | Train Loss: 0.4043 | Val Loss: 0.4154 | Time: 5.88s\r\n",
      "Epoch 019 | Train Loss: 0.3990 | Val Loss: 0.4173 | Time: 5.94s\r\n",
      "Epoch 020 | Train Loss: 0.4007 | Val Loss: 0.4127 | Time: 5.91s\r\n",
      "Epoch 021 | Train Loss: 0.3975 | Val Loss: 0.4215 | Time: 5.91s\r\n",
      "Epoch 022 | Train Loss: 0.4006 | Val Loss: 0.4246 | Time: 5.85s\r\n",
      "Epoch 023 | Train Loss: 0.3959 | Val Loss: 0.4277 | Time: 5.87s\r\n",
      "Epoch 024 | Train Loss: 0.3937 | Val Loss: 0.4257 | Time: 6.13s\r\n",
      "Epoch 025 | Train Loss: 0.3955 | Val Loss: 0.4254 | Time: 5.91s\r\n",
      "Epoch 026 | Train Loss: 0.3933 | Val Loss: 0.4283 | Time: 5.86s\r\n",
      "Epoch 027 | Train Loss: 0.3923 | Val Loss: 0.4315 | Time: 5.93s\r\n",
      "\r\n",
      "Early stopping à l'époque 27 (patience: 19)\r\n",
      "✅ Meilleur modèle chargé (époque 8, val_loss: 0.4055)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.0566\r\n",
      "   2. PhoneService        : 0.0565\r\n",
      "   3. SeniorCitizen       : 0.0558\r\n",
      "   4. StreamingMovies     : 0.0553\r\n",
      "   5. DeviceProtection    : 0.0551\r\n",
      "   6. OnlineSecurity      : 0.0544\r\n",
      "   7. InternetService     : 0.0528\r\n",
      "   8. StreamingTV         : 0.0528\r\n",
      "   9. Partner             : 0.0525\r\n",
      "  10. Contract            : 0.0523\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 14 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MonthlyCharges       (NUM): 0.0566\r\n",
      "   2. PhoneService         (CAT): 0.0565\r\n",
      "   3. SeniorCitizen        (CAT): 0.0558\r\n",
      "   4. StreamingMovies      (CAT): 0.0553\r\n",
      "   5. DeviceProtection     (CAT): 0.0551\r\n",
      "   6. OnlineSecurity       (CAT): 0.0544\r\n",
      "   7. InternetService      (CAT): 0.0528\r\n",
      "   8. StreamingTV          (CAT): 0.0528\r\n",
      "   9. Partner              (CAT): 0.0525\r\n",
      "  10. Contract             (CAT): 0.0523\r\n",
      "  11. gender               (CAT): 0.0519\r\n",
      "  12. OnlineBackup         (CAT): 0.0514\r\n",
      "  13. TechSupport          (CAT): 0.0514\r\n",
      "  14. Dependents           (CAT): 0.0512\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['PhoneService', 'SeniorCitizen', 'StreamingMovies', 'DeviceProtection', 'OnlineSecurity', 'InternetService', 'StreamingTV', 'Partner', 'Contract', 'gender', 'OnlineBackup', 'TechSupport', 'Dependents'] → indices [4, 1, 12, 9, 7, 6, 11, 2, 13, 0, 8, 10, 3]\r\n",
      "📊 Features sélectionnées: 1 numériques, 13 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 119,585 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.33%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5426 | Val Loss: 0.4951 | Time: 6.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4951)\r\n",
      "Epoch 001 | Train Loss: 0.4904 | Val Loss: 0.4761 | Time: 6.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4761)\r\n",
      "Epoch 002 | Train Loss: 0.4783 | Val Loss: 0.4634 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4634)\r\n",
      "Epoch 003 | Train Loss: 0.4717 | Val Loss: 0.4620 | Time: 6.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4620)\r\n",
      "Epoch 004 | Train Loss: 0.4628 | Val Loss: 0.4572 | Time: 6.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4572)\r\n",
      "Epoch 005 | Train Loss: 0.4605 | Val Loss: 0.4519 | Time: 6.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4519)\r\n",
      "Epoch 006 | Train Loss: 0.4566 | Val Loss: 0.4523 | Time: 6.24s\r\n",
      "Epoch 007 | Train Loss: 0.4518 | Val Loss: 0.4538 | Time: 6.04s\r\n",
      "Epoch 008 | Train Loss: 0.4488 | Val Loss: 0.4552 | Time: 6.12s\r\n",
      "Epoch 009 | Train Loss: 0.4500 | Val Loss: 0.4556 | Time: 6.07s\r\n",
      "Epoch 010 | Train Loss: 0.4508 | Val Loss: 0.4545 | Time: 6.15s\r\n",
      "Epoch 011 | Train Loss: 0.4488 | Val Loss: 0.4549 | Time: 6.30s\r\n",
      "Epoch 012 | Train Loss: 0.4522 | Val Loss: 0.4499 | Time: 6.15s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4499)\r\n",
      "Epoch 013 | Train Loss: 0.4475 | Val Loss: 0.4526 | Time: 6.20s\r\n",
      "Epoch 014 | Train Loss: 0.4465 | Val Loss: 0.4549 | Time: 6.14s\r\n",
      "Epoch 015 | Train Loss: 0.4454 | Val Loss: 0.4533 | Time: 6.17s\r\n",
      "Epoch 016 | Train Loss: 0.4454 | Val Loss: 0.4542 | Time: 6.19s\r\n",
      "Epoch 017 | Train Loss: 0.4454 | Val Loss: 0.4534 | Time: 6.16s\r\n",
      "Epoch 018 | Train Loss: 0.4456 | Val Loss: 0.4552 | Time: 6.11s\r\n",
      "Epoch 019 | Train Loss: 0.4448 | Val Loss: 0.4533 | Time: 6.14s\r\n",
      "Epoch 020 | Train Loss: 0.4423 | Val Loss: 0.4490 | Time: 6.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4490)\r\n",
      "Epoch 021 | Train Loss: 0.4421 | Val Loss: 0.4525 | Time: 6.16s\r\n",
      "Epoch 022 | Train Loss: 0.4456 | Val Loss: 0.4508 | Time: 6.31s\r\n",
      "Epoch 023 | Train Loss: 0.4420 | Val Loss: 0.4520 | Time: 6.18s\r\n",
      "Epoch 024 | Train Loss: 0.4424 | Val Loss: 0.4542 | Time: 6.16s\r\n",
      "Epoch 025 | Train Loss: 0.4406 | Val Loss: 0.4554 | Time: 6.12s\r\n",
      "Epoch 026 | Train Loss: 0.4422 | Val Loss: 0.4516 | Time: 6.13s\r\n",
      "Epoch 027 | Train Loss: 0.4392 | Val Loss: 0.4563 | Time: 6.17s\r\n",
      "Epoch 028 | Train Loss: 0.4411 | Val Loss: 0.4537 | Time: 6.15s\r\n",
      "Epoch 029 | Train Loss: 0.4376 | Val Loss: 0.4523 | Time: 6.15s\r\n",
      "Epoch 030 | Train Loss: 0.4414 | Val Loss: 0.4534 | Time: 6.09s\r\n",
      "Epoch 031 | Train Loss: 0.4355 | Val Loss: 0.4526 | Time: 6.11s\r\n",
      "Epoch 032 | Train Loss: 0.4408 | Val Loss: 0.4556 | Time: 6.15s\r\n",
      "Epoch 033 | Train Loss: 0.4412 | Val Loss: 0.4498 | Time: 6.15s\r\n",
      "Epoch 034 | Train Loss: 0.4383 | Val Loss: 0.4516 | Time: 6.12s\r\n",
      "Epoch 035 | Train Loss: 0.4395 | Val Loss: 0.4485 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4485)\r\n",
      "Epoch 036 | Train Loss: 0.4407 | Val Loss: 0.4518 | Time: 6.17s\r\n",
      "Epoch 037 | Train Loss: 0.4372 | Val Loss: 0.4508 | Time: 6.24s\r\n",
      "Epoch 038 | Train Loss: 0.4331 | Val Loss: 0.4558 | Time: 6.12s\r\n",
      "Epoch 039 | Train Loss: 0.4411 | Val Loss: 0.4509 | Time: 6.17s\r\n",
      "Epoch 040 | Train Loss: 0.4371 | Val Loss: 0.4539 | Time: 6.06s\r\n",
      "Epoch 041 | Train Loss: 0.4361 | Val Loss: 0.4568 | Time: 6.14s\r\n",
      "Epoch 042 | Train Loss: 0.4345 | Val Loss: 0.4570 | Time: 6.22s\r\n",
      "Epoch 043 | Train Loss: 0.4372 | Val Loss: 0.4576 | Time: 6.08s\r\n",
      "Epoch 044 | Train Loss: 0.4373 | Val Loss: 0.4586 | Time: 6.19s\r\n",
      "Epoch 045 | Train Loss: 0.4354 | Val Loss: 0.4555 | Time: 6.08s\r\n",
      "Epoch 046 | Train Loss: 0.4346 | Val Loss: 0.4594 | Time: 6.09s\r\n",
      "Epoch 047 | Train Loss: 0.4327 | Val Loss: 0.4566 | Time: 6.27s\r\n",
      "Epoch 048 | Train Loss: 0.4358 | Val Loss: 0.4582 | Time: 6.12s\r\n",
      "Epoch 049 | Train Loss: 0.4343 | Val Loss: 0.4556 | Time: 6.09s\r\n",
      "✅ Meilleur modèle Random chargé (époque 35, val_loss: 0.4485)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.1040\r\n",
      "   2. OnlineBackup         (CAT): 0.1002\r\n",
      "   3. OnlineSecurity       (CAT): 0.0752\r\n",
      "   4. SeniorCitizen        (CAT): 0.0731\r\n",
      "   5. MonthlyCharges       (NUM): 0.0729\r\n",
      "   6. gender               (CAT): 0.0723\r\n",
      "   7. DeviceProtection     (CAT): 0.0711\r\n",
      "   8. StreamingMovies      (CAT): 0.0687\r\n",
      "   9. StreamingTV          (CAT): 0.0659\r\n",
      "  10. Partner              (CAT): 0.0644\r\n",
      "  11. Contract             (CAT): 0.0606\r\n",
      "  12. TechSupport          (CAT): 0.0583\r\n",
      "  13. InternetService      (CAT): 0.0583\r\n",
      "  14. Dependents           (CAT): 0.0550\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.1040\r\n",
      "   2. OnlineBackup        : 0.1002\r\n",
      "   3. OnlineSecurity      : 0.0752\r\n",
      "   4. SeniorCitizen       : 0.0731\r\n",
      "   5. MonthlyCharges      : 0.0729\r\n",
      "   6. gender              : 0.0723\r\n",
      "   7. DeviceProtection    : 0.0711\r\n",
      "   8. StreamingMovies     : 0.0687\r\n",
      "   9. StreamingTV         : 0.0659\r\n",
      "  10. Partner             : 0.0644\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 477.0s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q-LR\r\n",
      "Modèle FTT+ créé avec 1,007,233 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4929 | Val Loss: 0.4318 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4318)\r\n",
      "Epoch 001 | Train Loss: 0.4275 | Val Loss: 0.4243 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4243)\r\n",
      "Epoch 002 | Train Loss: 0.4209 | Val Loss: 0.4235 | Time: 5.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 003 | Train Loss: 0.4182 | Val Loss: 0.4194 | Time: 5.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 004 | Train Loss: 0.4148 | Val Loss: 0.4193 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4193)\r\n",
      "Epoch 005 | Train Loss: 0.4113 | Val Loss: 0.4180 | Time: 6.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 006 | Train Loss: 0.4113 | Val Loss: 0.4137 | Time: 5.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 007 | Train Loss: 0.4081 | Val Loss: 0.4146 | Time: 5.99s\r\n",
      "Epoch 008 | Train Loss: 0.4077 | Val Loss: 0.4137 | Time: 6.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 009 | Train Loss: 0.4061 | Val Loss: 0.4153 | Time: 5.95s\r\n",
      "Epoch 010 | Train Loss: 0.4023 | Val Loss: 0.4150 | Time: 5.94s\r\n",
      "Epoch 011 | Train Loss: 0.4021 | Val Loss: 0.4146 | Time: 5.93s\r\n",
      "Epoch 012 | Train Loss: 0.3980 | Val Loss: 0.4136 | Time: 5.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4136)\r\n",
      "Epoch 013 | Train Loss: 0.3994 | Val Loss: 0.4149 | Time: 6.06s\r\n",
      "Epoch 014 | Train Loss: 0.3989 | Val Loss: 0.4145 | Time: 5.95s\r\n",
      "Epoch 015 | Train Loss: 0.3989 | Val Loss: 0.4139 | Time: 5.98s\r\n",
      "Epoch 016 | Train Loss: 0.3941 | Val Loss: 0.4143 | Time: 5.91s\r\n",
      "Epoch 017 | Train Loss: 0.3941 | Val Loss: 0.4147 | Time: 6.01s\r\n",
      "Epoch 018 | Train Loss: 0.3936 | Val Loss: 0.4141 | Time: 6.01s\r\n",
      "Epoch 019 | Train Loss: 0.3939 | Val Loss: 0.4154 | Time: 5.88s\r\n",
      "Epoch 020 | Train Loss: 0.3894 | Val Loss: 0.4181 | Time: 5.94s\r\n",
      "Epoch 021 | Train Loss: 0.3916 | Val Loss: 0.4157 | Time: 5.87s\r\n",
      "Epoch 022 | Train Loss: 0.3920 | Val Loss: 0.4189 | Time: 5.92s\r\n",
      "Epoch 023 | Train Loss: 0.3855 | Val Loss: 0.4196 | Time: 5.93s\r\n",
      "Epoch 024 | Train Loss: 0.3893 | Val Loss: 0.4181 | Time: 5.91s\r\n",
      "Epoch 025 | Train Loss: 0.3863 | Val Loss: 0.4203 | Time: 5.89s\r\n",
      "Epoch 026 | Train Loss: 0.3848 | Val Loss: 0.4204 | Time: 5.84s\r\n",
      "Epoch 027 | Train Loss: 0.3842 | Val Loss: 0.4213 | Time: 5.91s\r\n",
      "Epoch 028 | Train Loss: 0.3834 | Val Loss: 0.4225 | Time: 5.87s\r\n",
      "Epoch 029 | Train Loss: 0.3804 | Val Loss: 0.4215 | Time: 6.03s\r\n",
      "Epoch 030 | Train Loss: 0.3791 | Val Loss: 0.4235 | Time: 5.87s\r\n",
      "Epoch 031 | Train Loss: 0.3797 | Val Loss: 0.4255 | Time: 5.89s\r\n",
      "\r\n",
      "Early stopping à l'époque 31 (patience: 19)\r\n",
      "✅ Meilleur modèle chargé (époque 12, val_loss: 0.4136)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.0601\r\n",
      "   2. StreamingMovies     : 0.0589\r\n",
      "   3. OnlineSecurity      : 0.0562\r\n",
      "   4. TotalCharges        : 0.0545\r\n",
      "   5. Partner             : 0.0543\r\n",
      "   6. TechSupport         : 0.0530\r\n",
      "   7. StreamingTV         : 0.0528\r\n",
      "   8. Dependents          : 0.0523\r\n",
      "   9. tenure              : 0.0516\r\n",
      "  10. gender              : 0.0514\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 14 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PhoneService         (CAT): 0.0601\r\n",
      "   2. StreamingMovies      (CAT): 0.0589\r\n",
      "   3. OnlineSecurity       (CAT): 0.0562\r\n",
      "   4. TotalCharges         (NUM): 0.0545\r\n",
      "   5. Partner              (CAT): 0.0543\r\n",
      "   6. TechSupport          (CAT): 0.0530\r\n",
      "   7. StreamingTV          (CAT): 0.0528\r\n",
      "   8. Dependents           (CAT): 0.0523\r\n",
      "   9. tenure               (NUM): 0.0516\r\n",
      "  10. gender               (CAT): 0.0514\r\n",
      "  11. PaymentMethod        (CAT): 0.0513\r\n",
      "  12. PaperlessBilling     (CAT): 0.0513\r\n",
      "  13. DeviceProtection     (CAT): 0.0512\r\n",
      "  14. MonthlyCharges       (NUM): 0.0509\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure', 'MonthlyCharges'] → indices [2, 0, 1]\r\n",
      "   - Catégorielles sélectionnées: ['PhoneService', 'StreamingMovies', 'OnlineSecurity', 'Partner', 'TechSupport', 'StreamingTV', 'Dependents', 'gender', 'PaymentMethod', 'PaperlessBilling', 'DeviceProtection'] → indices [4, 12, 7, 2, 10, 11, 3, 0, 15, 14, 9]\r\n",
      "📊 Features sélectionnées: 3 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 119,489 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.33%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5570 | Val Loss: 0.4761 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4761)\r\n",
      "Epoch 001 | Train Loss: 0.4750 | Val Loss: 0.4678 | Time: 6.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4678)\r\n",
      "Epoch 002 | Train Loss: 0.4642 | Val Loss: 0.4573 | Time: 6.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4573)\r\n",
      "Epoch 003 | Train Loss: 0.4601 | Val Loss: 0.4539 | Time: 6.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4539)\r\n",
      "Epoch 004 | Train Loss: 0.4531 | Val Loss: 0.4499 | Time: 6.13s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4499)\r\n",
      "Epoch 005 | Train Loss: 0.4529 | Val Loss: 0.4434 | Time: 6.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4434)\r\n",
      "Epoch 006 | Train Loss: 0.4474 | Val Loss: 0.4444 | Time: 6.12s\r\n",
      "Epoch 007 | Train Loss: 0.4448 | Val Loss: 0.4393 | Time: 6.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4393)\r\n",
      "Epoch 008 | Train Loss: 0.4439 | Val Loss: 0.4388 | Time: 6.15s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4388)\r\n",
      "Epoch 009 | Train Loss: 0.4452 | Val Loss: 0.4371 | Time: 6.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4371)\r\n",
      "Epoch 010 | Train Loss: 0.4444 | Val Loss: 0.4335 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4335)\r\n",
      "Epoch 011 | Train Loss: 0.4380 | Val Loss: 0.4341 | Time: 6.24s\r\n",
      "Epoch 012 | Train Loss: 0.4399 | Val Loss: 0.4354 | Time: 6.15s\r\n",
      "Epoch 013 | Train Loss: 0.4362 | Val Loss: 0.4344 | Time: 6.13s\r\n",
      "Epoch 014 | Train Loss: 0.4363 | Val Loss: 0.4305 | Time: 6.16s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4305)\r\n",
      "Epoch 015 | Train Loss: 0.4327 | Val Loss: 0.4286 | Time: 6.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4286)\r\n",
      "Epoch 016 | Train Loss: 0.4370 | Val Loss: 0.4292 | Time: 6.19s\r\n",
      "Epoch 017 | Train Loss: 0.4312 | Val Loss: 0.4302 | Time: 6.23s\r\n",
      "Epoch 018 | Train Loss: 0.4341 | Val Loss: 0.4294 | Time: 6.16s\r\n",
      "Epoch 019 | Train Loss: 0.4308 | Val Loss: 0.4294 | Time: 6.14s\r\n",
      "Epoch 020 | Train Loss: 0.4342 | Val Loss: 0.4297 | Time: 6.08s\r\n",
      "Epoch 021 | Train Loss: 0.4324 | Val Loss: 0.4320 | Time: 6.08s\r\n",
      "Epoch 022 | Train Loss: 0.4293 | Val Loss: 0.4321 | Time: 6.26s\r\n",
      "Epoch 023 | Train Loss: 0.4318 | Val Loss: 0.4300 | Time: 6.28s\r\n",
      "Epoch 024 | Train Loss: 0.4271 | Val Loss: 0.4310 | Time: 6.16s\r\n",
      "Epoch 025 | Train Loss: 0.4280 | Val Loss: 0.4299 | Time: 6.06s\r\n",
      "Epoch 026 | Train Loss: 0.4256 | Val Loss: 0.4308 | Time: 6.10s\r\n",
      "Epoch 027 | Train Loss: 0.4272 | Val Loss: 0.4290 | Time: 6.21s\r\n",
      "Epoch 028 | Train Loss: 0.4259 | Val Loss: 0.4329 | Time: 6.21s\r\n",
      "Epoch 029 | Train Loss: 0.4241 | Val Loss: 0.4302 | Time: 6.23s\r\n",
      "Epoch 030 | Train Loss: 0.4213 | Val Loss: 0.4306 | Time: 6.18s\r\n",
      "Epoch 031 | Train Loss: 0.4238 | Val Loss: 0.4290 | Time: 6.17s\r\n",
      "Epoch 032 | Train Loss: 0.4256 | Val Loss: 0.4319 | Time: 6.17s\r\n",
      "Epoch 033 | Train Loss: 0.4216 | Val Loss: 0.4278 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4278)\r\n",
      "Epoch 034 | Train Loss: 0.4251 | Val Loss: 0.4329 | Time: 6.11s\r\n",
      "Epoch 035 | Train Loss: 0.4200 | Val Loss: 0.4329 | Time: 6.13s\r\n",
      "Epoch 036 | Train Loss: 0.4178 | Val Loss: 0.4324 | Time: 6.10s\r\n",
      "Epoch 037 | Train Loss: 0.4189 | Val Loss: 0.4330 | Time: 6.13s\r\n",
      "Epoch 038 | Train Loss: 0.4194 | Val Loss: 0.4347 | Time: 6.34s\r\n",
      "Epoch 039 | Train Loss: 0.4175 | Val Loss: 0.4314 | Time: 6.20s\r\n",
      "Epoch 040 | Train Loss: 0.4200 | Val Loss: 0.4289 | Time: 6.18s\r\n",
      "Epoch 041 | Train Loss: 0.4193 | Val Loss: 0.4321 | Time: 6.18s\r\n",
      "Epoch 042 | Train Loss: 0.4150 | Val Loss: 0.4297 | Time: 6.16s\r\n",
      "Epoch 043 | Train Loss: 0.4201 | Val Loss: 0.4295 | Time: 6.19s\r\n",
      "Epoch 044 | Train Loss: 0.4199 | Val Loss: 0.4279 | Time: 6.13s\r\n",
      "Epoch 045 | Train Loss: 0.4133 | Val Loss: 0.4301 | Time: 6.17s\r\n",
      "Epoch 046 | Train Loss: 0.4201 | Val Loss: 0.4281 | Time: 6.15s\r\n",
      "Epoch 047 | Train Loss: 0.4166 | Val Loss: 0.4279 | Time: 6.13s\r\n",
      "Epoch 048 | Train Loss: 0.4177 | Val Loss: 0.4294 | Time: 6.18s\r\n",
      "Epoch 049 | Train Loss: 0.4148 | Val Loss: 0.4281 | Time: 6.17s\r\n",
      "✅ Meilleur modèle Random chargé (époque 33, val_loss: 0.4278)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineSecurity       (CAT): 0.0993\r\n",
      "   2. Partner              (CAT): 0.0881\r\n",
      "   3. gender               (CAT): 0.0747\r\n",
      "   4. StreamingTV          (CAT): 0.0744\r\n",
      "   5. MonthlyCharges       (NUM): 0.0716\r\n",
      "   6. PaperlessBilling     (CAT): 0.0700\r\n",
      "   7. PaymentMethod        (CAT): 0.0692\r\n",
      "   8. StreamingMovies      (CAT): 0.0674\r\n",
      "   9. TotalCharges         (NUM): 0.0664\r\n",
      "  10. Dependents           (CAT): 0.0646\r\n",
      "  11. DeviceProtection     (CAT): 0.0645\r\n",
      "  12. PhoneService         (CAT): 0.0642\r\n",
      "  13. TechSupport          (CAT): 0.0631\r\n",
      "  14. tenure               (NUM): 0.0627\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.0993\r\n",
      "   2. Partner             : 0.0881\r\n",
      "   3. gender              : 0.0747\r\n",
      "   4. StreamingTV         : 0.0744\r\n",
      "   5. MonthlyCharges      : 0.0716\r\n",
      "   6. PaperlessBilling    : 0.0700\r\n",
      "   7. PaymentMethod       : 0.0692\r\n",
      "   8. StreamingMovies     : 0.0674\r\n",
      "   9. TotalCharges        : 0.0664\r\n",
      "  10. Dependents          : 0.0646\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 502.3s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q-LR\r\n",
      "Modèle FTT+ créé avec 1,007,233 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4951 | Val Loss: 0.4326 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4326)\r\n",
      "Epoch 001 | Train Loss: 0.4411 | Val Loss: 0.4237 | Time: 5.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4237)\r\n",
      "Epoch 002 | Train Loss: 0.4266 | Val Loss: 0.4142 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4142)\r\n",
      "Epoch 003 | Train Loss: 0.4258 | Val Loss: 0.4177 | Time: 5.97s\r\n",
      "Epoch 004 | Train Loss: 0.4210 | Val Loss: 0.4169 | Time: 5.94s\r\n",
      "Epoch 005 | Train Loss: 0.4173 | Val Loss: 0.4106 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4106)\r\n",
      "Epoch 006 | Train Loss: 0.4163 | Val Loss: 0.4148 | Time: 5.90s\r\n",
      "Epoch 007 | Train Loss: 0.4154 | Val Loss: 0.4143 | Time: 5.89s\r\n",
      "Epoch 008 | Train Loss: 0.4128 | Val Loss: 0.4141 | Time: 6.23s\r\n",
      "Epoch 009 | Train Loss: 0.4128 | Val Loss: 0.4140 | Time: 5.88s\r\n",
      "Epoch 010 | Train Loss: 0.4103 | Val Loss: 0.4110 | Time: 5.97s\r\n",
      "Epoch 011 | Train Loss: 0.4085 | Val Loss: 0.4157 | Time: 5.94s\r\n",
      "Epoch 012 | Train Loss: 0.4083 | Val Loss: 0.4149 | Time: 5.85s\r\n",
      "Epoch 013 | Train Loss: 0.4069 | Val Loss: 0.4176 | Time: 5.87s\r\n",
      "Epoch 014 | Train Loss: 0.4041 | Val Loss: 0.4195 | Time: 6.16s\r\n",
      "Epoch 015 | Train Loss: 0.4032 | Val Loss: 0.4196 | Time: 5.88s\r\n",
      "Epoch 016 | Train Loss: 0.4024 | Val Loss: 0.4211 | Time: 5.86s\r\n",
      "Epoch 017 | Train Loss: 0.4005 | Val Loss: 0.4239 | Time: 5.88s\r\n",
      "Epoch 018 | Train Loss: 0.4018 | Val Loss: 0.4251 | Time: 5.92s\r\n",
      "Epoch 019 | Train Loss: 0.4002 | Val Loss: 0.4252 | Time: 6.08s\r\n",
      "Epoch 020 | Train Loss: 0.3974 | Val Loss: 0.4329 | Time: 5.90s\r\n",
      "Epoch 021 | Train Loss: 0.3947 | Val Loss: 0.4332 | Time: 5.91s\r\n",
      "Epoch 022 | Train Loss: 0.3954 | Val Loss: 0.4352 | Time: 5.91s\r\n",
      "Epoch 023 | Train Loss: 0.3949 | Val Loss: 0.4312 | Time: 5.93s\r\n",
      "Epoch 024 | Train Loss: 0.3925 | Val Loss: 0.4370 | Time: 6.01s\r\n",
      "\r\n",
      "Early stopping à l'époque 24 (patience: 19)\r\n",
      "✅ Meilleur modèle chargé (époque 5, val_loss: 0.4106)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingTV         : 0.0563\r\n",
      "   2. SeniorCitizen       : 0.0553\r\n",
      "   3. tenure              : 0.0551\r\n",
      "   4. InternetService     : 0.0540\r\n",
      "   5. StreamingMovies     : 0.0534\r\n",
      "   6. OnlineSecurity      : 0.0533\r\n",
      "   7. TechSupport         : 0.0532\r\n",
      "   8. gender              : 0.0530\r\n",
      "   9. PaperlessBilling    : 0.0524\r\n",
      "  10. MonthlyCharges      : 0.0524\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 14 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. StreamingTV          (CAT): 0.0563\r\n",
      "   2. SeniorCitizen        (CAT): 0.0553\r\n",
      "   3. tenure               (NUM): 0.0551\r\n",
      "   4. InternetService      (CAT): 0.0540\r\n",
      "   5. StreamingMovies      (CAT): 0.0534\r\n",
      "   6. OnlineSecurity       (CAT): 0.0533\r\n",
      "   7. TechSupport          (CAT): 0.0532\r\n",
      "   8. gender               (CAT): 0.0530\r\n",
      "   9. PaperlessBilling     (CAT): 0.0524\r\n",
      "  10. MonthlyCharges       (NUM): 0.0524\r\n",
      "  11. PhoneService         (CAT): 0.0523\r\n",
      "  12. PaymentMethod        (CAT): 0.0522\r\n",
      "  13. TotalCharges         (NUM): 0.0521\r\n",
      "  14. Partner              (CAT): 0.0521\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'MonthlyCharges', 'TotalCharges'] → indices [0, 1, 2]\r\n",
      "   - Catégorielles sélectionnées: ['StreamingTV', 'SeniorCitizen', 'InternetService', 'StreamingMovies', 'OnlineSecurity', 'TechSupport', 'gender', 'PaperlessBilling', 'PhoneService', 'PaymentMethod', 'Partner'] → indices [11, 1, 6, 12, 7, 10, 0, 14, 4, 15, 2]\r\n",
      "📊 Features sélectionnées: 3 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 7 paires\r\n",
      "Modèle Random créé avec 119,489 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.33%\r\n",
      "   - Connexions feature-feature: 14\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5586 | Val Loss: 0.4397 | Time: 6.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4397)\r\n",
      "Epoch 001 | Train Loss: 0.4695 | Val Loss: 0.4321 | Time: 6.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4321)\r\n",
      "Epoch 002 | Train Loss: 0.4616 | Val Loss: 0.4308 | Time: 6.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4308)\r\n",
      "Epoch 003 | Train Loss: 0.4538 | Val Loss: 0.4266 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4266)\r\n",
      "Epoch 004 | Train Loss: 0.4500 | Val Loss: 0.4222 | Time: 6.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 005 | Train Loss: 0.4462 | Val Loss: 0.4253 | Time: 6.14s\r\n",
      "Epoch 006 | Train Loss: 0.4416 | Val Loss: 0.4261 | Time: 6.12s\r\n",
      "Epoch 007 | Train Loss: 0.4404 | Val Loss: 0.4239 | Time: 6.15s\r\n",
      "Epoch 008 | Train Loss: 0.4414 | Val Loss: 0.4237 | Time: 6.16s\r\n",
      "Epoch 009 | Train Loss: 0.4361 | Val Loss: 0.4238 | Time: 6.34s\r\n",
      "Epoch 010 | Train Loss: 0.4317 | Val Loss: 0.4252 | Time: 6.15s\r\n",
      "Epoch 011 | Train Loss: 0.4362 | Val Loss: 0.4247 | Time: 6.17s\r\n",
      "Epoch 012 | Train Loss: 0.4361 | Val Loss: 0.4227 | Time: 6.15s\r\n",
      "Epoch 013 | Train Loss: 0.4354 | Val Loss: 0.4228 | Time: 6.15s\r\n",
      "Epoch 014 | Train Loss: 0.4311 | Val Loss: 0.4253 | Time: 6.20s\r\n",
      "Epoch 015 | Train Loss: 0.4348 | Val Loss: 0.4255 | Time: 6.16s\r\n",
      "Epoch 016 | Train Loss: 0.4298 | Val Loss: 0.4243 | Time: 6.16s\r\n",
      "Epoch 017 | Train Loss: 0.4354 | Val Loss: 0.4222 | Time: 6.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 018 | Train Loss: 0.4295 | Val Loss: 0.4236 | Time: 6.14s\r\n",
      "Epoch 019 | Train Loss: 0.4277 | Val Loss: 0.4239 | Time: 6.16s\r\n",
      "Epoch 020 | Train Loss: 0.4250 | Val Loss: 0.4260 | Time: 6.16s\r\n",
      "Epoch 021 | Train Loss: 0.4291 | Val Loss: 0.4277 | Time: 6.15s\r\n",
      "Epoch 022 | Train Loss: 0.4294 | Val Loss: 0.4232 | Time: 6.12s\r\n",
      "Epoch 023 | Train Loss: 0.4325 | Val Loss: 0.4244 | Time: 6.15s\r\n",
      "Epoch 024 | Train Loss: 0.4273 | Val Loss: 0.4256 | Time: 6.18s\r\n",
      "Epoch 025 | Train Loss: 0.4271 | Val Loss: 0.4278 | Time: 6.22s\r\n",
      "Epoch 026 | Train Loss: 0.4234 | Val Loss: 0.4314 | Time: 6.12s\r\n",
      "Epoch 027 | Train Loss: 0.4249 | Val Loss: 0.4294 | Time: 6.15s\r\n",
      "Epoch 028 | Train Loss: 0.4255 | Val Loss: 0.4265 | Time: 6.11s\r\n",
      "Epoch 029 | Train Loss: 0.4260 | Val Loss: 0.4252 | Time: 6.11s\r\n",
      "Epoch 030 | Train Loss: 0.4248 | Val Loss: 0.4289 | Time: 6.22s\r\n",
      "Epoch 031 | Train Loss: 0.4257 | Val Loss: 0.4302 | Time: 6.12s\r\n",
      "Epoch 032 | Train Loss: 0.4271 | Val Loss: 0.4282 | Time: 6.20s\r\n",
      "Epoch 033 | Train Loss: 0.4230 | Val Loss: 0.4364 | Time: 6.17s\r\n",
      "Epoch 034 | Train Loss: 0.4249 | Val Loss: 0.4264 | Time: 6.19s\r\n",
      "Epoch 035 | Train Loss: 0.4216 | Val Loss: 0.4294 | Time: 6.37s\r\n",
      "Epoch 036 | Train Loss: 0.4219 | Val Loss: 0.4315 | Time: 6.16s\r\n",
      "\r\n",
      "Early stopping à l'époque 36 (patience: 19)\r\n",
      "✅ Meilleur modèle Random chargé (époque 17, val_loss: 0.4222)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Partner              (CAT): 0.0942\r\n",
      "   2. OnlineSecurity       (CAT): 0.0936\r\n",
      "   3. SeniorCitizen        (CAT): 0.0872\r\n",
      "   4. PaymentMethod        (CAT): 0.0819\r\n",
      "   5. tenure               (NUM): 0.0793\r\n",
      "   6. TechSupport          (CAT): 0.0741\r\n",
      "   7. InternetService      (CAT): 0.0685\r\n",
      "   8. StreamingTV          (CAT): 0.0656\r\n",
      "   9. StreamingMovies      (CAT): 0.0615\r\n",
      "  10. MonthlyCharges       (NUM): 0.0615\r\n",
      "  11. PaperlessBilling     (CAT): 0.0600\r\n",
      "  12. PhoneService         (CAT): 0.0590\r\n",
      "  13. gender               (CAT): 0.0570\r\n",
      "  14. TotalCharges         (NUM): 0.0568\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Partner             : 0.0942\r\n",
      "   2. OnlineSecurity      : 0.0936\r\n",
      "   3. SeniorCitizen       : 0.0872\r\n",
      "   4. PaymentMethod       : 0.0819\r\n",
      "   5. tenure              : 0.0793\r\n",
      "   6. TechSupport         : 0.0741\r\n",
      "   7. InternetService     : 0.0685\r\n",
      "   8. StreamingTV         : 0.0656\r\n",
      "   9. StreamingMovies     : 0.0615\r\n",
      "  10. MonthlyCharges      : 0.0615\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_17/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 380.8s ===\r\n",
      "\u001b[32m[I 2025-07-20 00:03:06,340]\u001b[0m Trial 17 finished with value: 0.0 and parameters: {'d_token_stage1': 128, 'n_blocks_stage1': 6, 'n_heads_stage1': 4, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.1401074725837409, 'ffn_dropout_stage1': 0.23614859118182197, 'residual_dropout_stage1': 0.18793385893471112, 'lr_stage1': 6.250037743071343e-05, 'weight_decay_stage1': 1.9644783512187015e-05, 'd_token_stage2': 32, 'n_blocks_stage2': 4, 'n_heads_stage2': 8, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.13986426462611498, 'ffn_dropout_stage2': 0.1065024192305305, 'residual_dropout_stage2': 0.11105098867618089, 'lr_stage2': 0.00022491677241458607, 'weight_decay_stage2': 1.1265285296945426e-06, 'batch_size': 32, 'patience': 19, 'embedding_type': 'Q-LR', 'M': 14, 'k': 7}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  72%|█████  | 18/25 [5:50:09<2:25:55, 1250.73s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 214,337 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6568 | Val Loss: 0.6130 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6130)\r\n",
      "Epoch 001 | Train Loss: 0.5937 | Val Loss: 0.5676 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5676)\r\n",
      "Epoch 002 | Train Loss: 0.5614 | Val Loss: 0.5359 | Time: 7.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5359)\r\n",
      "Epoch 003 | Train Loss: 0.5394 | Val Loss: 0.5041 | Time: 7.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5041)\r\n",
      "Epoch 004 | Train Loss: 0.5152 | Val Loss: 0.4779 | Time: 7.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4779)\r\n",
      "Epoch 005 | Train Loss: 0.4944 | Val Loss: 0.4617 | Time: 6.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4617)\r\n",
      "Epoch 006 | Train Loss: 0.4814 | Val Loss: 0.4525 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4525)\r\n",
      "Epoch 007 | Train Loss: 0.4724 | Val Loss: 0.4468 | Time: 7.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4468)\r\n",
      "Epoch 008 | Train Loss: 0.4648 | Val Loss: 0.4435 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4435)\r\n",
      "Epoch 009 | Train Loss: 0.4589 | Val Loss: 0.4406 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4406)\r\n",
      "Epoch 010 | Train Loss: 0.4541 | Val Loss: 0.4386 | Time: 6.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4386)\r\n",
      "Epoch 011 | Train Loss: 0.4525 | Val Loss: 0.4368 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 012 | Train Loss: 0.4492 | Val Loss: 0.4359 | Time: 7.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4359)\r\n",
      "Epoch 013 | Train Loss: 0.4474 | Val Loss: 0.4342 | Time: 7.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 014 | Train Loss: 0.4434 | Val Loss: 0.4330 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4330)\r\n",
      "Epoch 015 | Train Loss: 0.4434 | Val Loss: 0.4315 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4315)\r\n",
      "Epoch 016 | Train Loss: 0.4424 | Val Loss: 0.4308 | Time: 7.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4308)\r\n",
      "Epoch 017 | Train Loss: 0.4401 | Val Loss: 0.4297 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 018 | Train Loss: 0.4416 | Val Loss: 0.4284 | Time: 7.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4284)\r\n",
      "Epoch 019 | Train Loss: 0.4395 | Val Loss: 0.4279 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4279)\r\n",
      "Epoch 020 | Train Loss: 0.4361 | Val Loss: 0.4265 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 021 | Train Loss: 0.4343 | Val Loss: 0.4258 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 022 | Train Loss: 0.4333 | Val Loss: 0.4250 | Time: 7.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4250)\r\n",
      "Epoch 023 | Train Loss: 0.4341 | Val Loss: 0.4243 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4243)\r\n",
      "Epoch 024 | Train Loss: 0.4368 | Val Loss: 0.4236 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4236)\r\n",
      "Epoch 025 | Train Loss: 0.4316 | Val Loss: 0.4224 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4224)\r\n",
      "Epoch 026 | Train Loss: 0.4334 | Val Loss: 0.4224 | Time: 6.97s\r\n",
      "Epoch 027 | Train Loss: 0.4313 | Val Loss: 0.4216 | Time: 6.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 028 | Train Loss: 0.4280 | Val Loss: 0.4215 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4215)\r\n",
      "Epoch 029 | Train Loss: 0.4296 | Val Loss: 0.4215 | Time: 7.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4215)\r\n",
      "Epoch 030 | Train Loss: 0.4262 | Val Loss: 0.4214 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "Epoch 031 | Train Loss: 0.4270 | Val Loss: 0.4213 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4213)\r\n",
      "Epoch 032 | Train Loss: 0.4297 | Val Loss: 0.4201 | Time: 6.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4201)\r\n",
      "Epoch 033 | Train Loss: 0.4290 | Val Loss: 0.4194 | Time: 6.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 034 | Train Loss: 0.4272 | Val Loss: 0.4195 | Time: 7.08s\r\n",
      "Epoch 035 | Train Loss: 0.4268 | Val Loss: 0.4191 | Time: 6.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4191)\r\n",
      "Epoch 036 | Train Loss: 0.4268 | Val Loss: 0.4188 | Time: 6.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 037 | Train Loss: 0.4280 | Val Loss: 0.4178 | Time: 6.87s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 038 | Train Loss: 0.4255 | Val Loss: 0.4170 | Time: 6.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 039 | Train Loss: 0.4258 | Val Loss: 0.4172 | Time: 7.09s\r\n",
      "Epoch 040 | Train Loss: 0.4259 | Val Loss: 0.4166 | Time: 6.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4166)\r\n",
      "Epoch 041 | Train Loss: 0.4223 | Val Loss: 0.4167 | Time: 6.98s\r\n",
      "Epoch 042 | Train Loss: 0.4249 | Val Loss: 0.4159 | Time: 6.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 043 | Train Loss: 0.4269 | Val Loss: 0.4160 | Time: 6.98s\r\n",
      "Epoch 044 | Train Loss: 0.4231 | Val Loss: 0.4153 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4153)\r\n",
      "Epoch 045 | Train Loss: 0.4241 | Val Loss: 0.4147 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "Epoch 046 | Train Loss: 0.4234 | Val Loss: 0.4140 | Time: 6.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4140)\r\n",
      "Epoch 047 | Train Loss: 0.4264 | Val Loss: 0.4141 | Time: 7.02s\r\n",
      "Epoch 048 | Train Loss: 0.4273 | Val Loss: 0.4130 | Time: 7.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4130)\r\n",
      "Epoch 049 | Train Loss: 0.4237 | Val Loss: 0.4144 | Time: 6.96s\r\n",
      "✅ Meilleur modèle chargé (époque 48, val_loss: 0.4130)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.0541\r\n",
      "   2. MonthlyCharges      : 0.0539\r\n",
      "   3. OnlineSecurity      : 0.0538\r\n",
      "   4. PaymentMethod       : 0.0533\r\n",
      "   5. Contract            : 0.0531\r\n",
      "   6. MultipleLines       : 0.0531\r\n",
      "   7. tenure              : 0.0531\r\n",
      "   8. Partner             : 0.0530\r\n",
      "   9. PhoneService        : 0.0530\r\n",
      "  10. StreamingMovies     : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 13 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. OnlineBackup         (CAT): 0.0541\r\n",
      "   2. MonthlyCharges       (NUM): 0.0539\r\n",
      "   3. OnlineSecurity       (CAT): 0.0538\r\n",
      "   4. PaymentMethod        (CAT): 0.0533\r\n",
      "   5. Contract             (CAT): 0.0531\r\n",
      "   6. MultipleLines        (CAT): 0.0531\r\n",
      "   7. tenure               (NUM): 0.0531\r\n",
      "   8. Partner              (CAT): 0.0530\r\n",
      "   9. PhoneService         (CAT): 0.0530\r\n",
      "  10. StreamingMovies      (CAT): 0.0525\r\n",
      "  11. SeniorCitizen        (CAT): 0.0524\r\n",
      "  12. TechSupport          (CAT): 0.0522\r\n",
      "  13. Dependents           (CAT): 0.0521\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure'] → indices [1, 0]\r\n",
      "   - Catégorielles sélectionnées: ['OnlineBackup', 'OnlineSecurity', 'PaymentMethod', 'Contract', 'MultipleLines', 'Partner', 'PhoneService', 'StreamingMovies', 'SeniorCitizen', 'TechSupport', 'Dependents'] → indices [8, 7, 15, 13, 5, 2, 4, 12, 1, 10, 3]\r\n",
      "📊 Features sélectionnées: 2 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 354,561 paramètres\r\n",
      "🔗 Sparsité d'attention: 82.65%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4865 | Val Loss: 0.4660 | Time: 2.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4660)\r\n",
      "Epoch 001 | Train Loss: 0.4532 | Val Loss: 0.4475 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4475)\r\n",
      "Epoch 002 | Train Loss: 0.4557 | Val Loss: 0.4506 | Time: 2.36s\r\n",
      "Epoch 003 | Train Loss: 0.4494 | Val Loss: 0.4566 | Time: 2.39s\r\n",
      "Epoch 004 | Train Loss: 0.4501 | Val Loss: 0.4458 | Time: 2.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4458)\r\n",
      "Epoch 005 | Train Loss: 0.4448 | Val Loss: 0.4475 | Time: 2.37s\r\n",
      "Epoch 006 | Train Loss: 0.4483 | Val Loss: 0.4346 | Time: 2.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4346)\r\n",
      "Epoch 007 | Train Loss: 0.4541 | Val Loss: 0.4386 | Time: 2.47s\r\n",
      "Epoch 008 | Train Loss: 0.4511 | Val Loss: 0.4548 | Time: 2.37s\r\n",
      "Epoch 009 | Train Loss: 0.4466 | Val Loss: 0.4494 | Time: 2.39s\r\n",
      "Epoch 010 | Train Loss: 0.4510 | Val Loss: 0.4543 | Time: 2.37s\r\n",
      "Epoch 011 | Train Loss: 0.4517 | Val Loss: 0.4495 | Time: 2.38s\r\n",
      "Epoch 012 | Train Loss: 0.4531 | Val Loss: 0.4642 | Time: 2.36s\r\n",
      "Epoch 013 | Train Loss: 0.4540 | Val Loss: 0.4395 | Time: 2.36s\r\n",
      "Epoch 014 | Train Loss: 0.4496 | Val Loss: 0.4524 | Time: 2.38s\r\n",
      "Epoch 015 | Train Loss: 0.4495 | Val Loss: 0.4533 | Time: 2.40s\r\n",
      "Epoch 016 | Train Loss: 0.4482 | Val Loss: 0.4556 | Time: 2.37s\r\n",
      "Epoch 017 | Train Loss: 0.4499 | Val Loss: 0.4519 | Time: 2.39s\r\n",
      "Epoch 018 | Train Loss: 0.4498 | Val Loss: 0.4347 | Time: 2.35s\r\n",
      "Epoch 019 | Train Loss: 0.4461 | Val Loss: 0.4475 | Time: 2.36s\r\n",
      "Epoch 020 | Train Loss: 0.4523 | Val Loss: 0.4400 | Time: 2.53s\r\n",
      "Epoch 021 | Train Loss: 0.4506 | Val Loss: 0.4423 | Time: 2.34s\r\n",
      "\r\n",
      "Early stopping à l'époque 21 (patience: 15)\r\n",
      "✅ Meilleur modèle Random chargé (époque 6, val_loss: 0.4346)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Partner              (CAT): 0.4531\r\n",
      "   2. PhoneService         (CAT): 0.1451\r\n",
      "   3. TechSupport          (CAT): 0.1150\r\n",
      "   4. MonthlyCharges       (NUM): 0.0752\r\n",
      "   5. tenure               (NUM): 0.0425\r\n",
      "   6. OnlineBackup         (CAT): 0.0292\r\n",
      "   7. MultipleLines        (CAT): 0.0275\r\n",
      "   8. Dependents           (CAT): 0.0210\r\n",
      "   9. Contract             (CAT): 0.0196\r\n",
      "  10. PaymentMethod        (CAT): 0.0193\r\n",
      "  11. SeniorCitizen        (CAT): 0.0192\r\n",
      "  12. StreamingMovies      (CAT): 0.0175\r\n",
      "  13. OnlineSecurity       (CAT): 0.0157\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Partner             : 0.4531\r\n",
      "   2. PhoneService        : 0.1451\r\n",
      "   3. TechSupport         : 0.1150\r\n",
      "   4. MonthlyCharges      : 0.0752\r\n",
      "   5. tenure              : 0.0425\r\n",
      "   6. OnlineBackup        : 0.0292\r\n",
      "   7. MultipleLines       : 0.0275\r\n",
      "   8. Dependents          : 0.0210\r\n",
      "   9. Contract            : 0.0196\r\n",
      "  10. PaymentMethod       : 0.0193\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 406.0s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 214,337 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5991 | Val Loss: 0.5657 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5657)\r\n",
      "Epoch 001 | Train Loss: 0.5599 | Val Loss: 0.5329 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5329)\r\n",
      "Epoch 002 | Train Loss: 0.5342 | Val Loss: 0.4991 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4991)\r\n",
      "Epoch 003 | Train Loss: 0.5064 | Val Loss: 0.4745 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4745)\r\n",
      "Epoch 004 | Train Loss: 0.4854 | Val Loss: 0.4604 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4604)\r\n",
      "Epoch 005 | Train Loss: 0.4702 | Val Loss: 0.4537 | Time: 7.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4537)\r\n",
      "Epoch 006 | Train Loss: 0.4625 | Val Loss: 0.4500 | Time: 7.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4500)\r\n",
      "Epoch 007 | Train Loss: 0.4534 | Val Loss: 0.4476 | Time: 7.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4476)\r\n",
      "Epoch 008 | Train Loss: 0.4522 | Val Loss: 0.4456 | Time: 7.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4456)\r\n",
      "Epoch 009 | Train Loss: 0.4503 | Val Loss: 0.4445 | Time: 6.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4445)\r\n",
      "Epoch 010 | Train Loss: 0.4456 | Val Loss: 0.4434 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4434)\r\n",
      "Epoch 011 | Train Loss: 0.4450 | Val Loss: 0.4419 | Time: 6.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "Epoch 012 | Train Loss: 0.4386 | Val Loss: 0.4414 | Time: 7.15s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4414)\r\n",
      "Epoch 013 | Train Loss: 0.4406 | Val Loss: 0.4394 | Time: 6.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4394)\r\n",
      "Epoch 014 | Train Loss: 0.4381 | Val Loss: 0.4386 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4386)\r\n",
      "Epoch 015 | Train Loss: 0.4396 | Val Loss: 0.4373 | Time: 7.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4373)\r\n",
      "Epoch 016 | Train Loss: 0.4343 | Val Loss: 0.4364 | Time: 7.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 017 | Train Loss: 0.4343 | Val Loss: 0.4354 | Time: 7.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4354)\r\n",
      "Epoch 018 | Train Loss: 0.4351 | Val Loss: 0.4345 | Time: 7.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 019 | Train Loss: 0.4293 | Val Loss: 0.4340 | Time: 6.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4340)\r\n",
      "Epoch 020 | Train Loss: 0.4298 | Val Loss: 0.4327 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4327)\r\n",
      "Epoch 021 | Train Loss: 0.4291 | Val Loss: 0.4323 | Time: 7.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4323)\r\n",
      "Epoch 022 | Train Loss: 0.4262 | Val Loss: 0.4317 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 023 | Train Loss: 0.4273 | Val Loss: 0.4308 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4308)\r\n",
      "Epoch 024 | Train Loss: 0.4247 | Val Loss: 0.4308 | Time: 7.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4308)\r\n",
      "Epoch 025 | Train Loss: 0.4237 | Val Loss: 0.4295 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4295)\r\n",
      "Epoch 026 | Train Loss: 0.4245 | Val Loss: 0.4293 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 027 | Train Loss: 0.4236 | Val Loss: 0.4279 | Time: 7.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4279)\r\n",
      "Epoch 028 | Train Loss: 0.4234 | Val Loss: 0.4273 | Time: 6.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4273)\r\n",
      "Epoch 029 | Train Loss: 0.4202 | Val Loss: 0.4276 | Time: 6.98s\r\n",
      "Epoch 030 | Train Loss: 0.4221 | Val Loss: 0.4271 | Time: 7.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4271)\r\n",
      "Epoch 031 | Train Loss: 0.4215 | Val Loss: 0.4265 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 032 | Train Loss: 0.4184 | Val Loss: 0.4265 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 033 | Train Loss: 0.4200 | Val Loss: 0.4265 | Time: 6.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 034 | Train Loss: 0.4200 | Val Loss: 0.4258 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 035 | Train Loss: 0.4186 | Val Loss: 0.4260 | Time: 7.02s\r\n",
      "Epoch 036 | Train Loss: 0.4195 | Val Loss: 0.4250 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4250)\r\n",
      "Epoch 037 | Train Loss: 0.4188 | Val Loss: 0.4246 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4246)\r\n",
      "Epoch 038 | Train Loss: 0.4173 | Val Loss: 0.4250 | Time: 7.02s\r\n",
      "Epoch 039 | Train Loss: 0.4191 | Val Loss: 0.4254 | Time: 7.10s\r\n",
      "Epoch 040 | Train Loss: 0.4213 | Val Loss: 0.4239 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "Epoch 041 | Train Loss: 0.4189 | Val Loss: 0.4235 | Time: 7.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 042 | Train Loss: 0.4156 | Val Loss: 0.4235 | Time: 6.95s\r\n",
      "Epoch 043 | Train Loss: 0.4164 | Val Loss: 0.4230 | Time: 7.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4230)\r\n",
      "Epoch 044 | Train Loss: 0.4170 | Val Loss: 0.4223 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4223)\r\n",
      "Epoch 045 | Train Loss: 0.4136 | Val Loss: 0.4229 | Time: 6.94s\r\n",
      "Epoch 046 | Train Loss: 0.4131 | Val Loss: 0.4231 | Time: 7.05s\r\n",
      "Epoch 047 | Train Loss: 0.4141 | Val Loss: 0.4222 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 048 | Train Loss: 0.4145 | Val Loss: 0.4220 | Time: 7.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4220)\r\n",
      "Epoch 049 | Train Loss: 0.4142 | Val Loss: 0.4214 | Time: 7.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4214)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4214)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.0550\r\n",
      "   2. StreamingMovies     : 0.0545\r\n",
      "   3. Dependents          : 0.0540\r\n",
      "   4. PaperlessBilling    : 0.0540\r\n",
      "   5. Contract            : 0.0539\r\n",
      "   6. InternetService     : 0.0537\r\n",
      "   7. PhoneService        : 0.0534\r\n",
      "   8. PaymentMethod       : 0.0527\r\n",
      "   9. DeviceProtection    : 0.0527\r\n",
      "  10. StreamingTV         : 0.0523\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 13 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. tenure               (NUM): 0.0550\r\n",
      "   2. StreamingMovies      (CAT): 0.0545\r\n",
      "   3. Dependents           (CAT): 0.0540\r\n",
      "   4. PaperlessBilling     (CAT): 0.0540\r\n",
      "   5. Contract             (CAT): 0.0539\r\n",
      "   6. InternetService      (CAT): 0.0537\r\n",
      "   7. PhoneService         (CAT): 0.0534\r\n",
      "   8. PaymentMethod        (CAT): 0.0527\r\n",
      "   9. DeviceProtection     (CAT): 0.0527\r\n",
      "  10. StreamingTV          (CAT): 0.0523\r\n",
      "  11. MultipleLines        (CAT): 0.0522\r\n",
      "  12. Partner              (CAT): 0.0521\r\n",
      "  13. gender               (CAT): 0.0520\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure'] → indices [0]\r\n",
      "   - Catégorielles sélectionnées: ['StreamingMovies', 'Dependents', 'PaperlessBilling', 'Contract', 'InternetService', 'PhoneService', 'PaymentMethod', 'DeviceProtection', 'StreamingTV', 'MultipleLines', 'Partner', 'gender'] → indices [12, 3, 14, 13, 6, 4, 15, 9, 11, 5, 2, 0]\r\n",
      "📊 Features sélectionnées: 1 numériques, 12 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 354,689 paramètres\r\n",
      "🔗 Sparsité d'attention: 82.65%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4858 | Val Loss: 0.4381 | Time: 2.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4381)\r\n",
      "Epoch 001 | Train Loss: 0.4468 | Val Loss: 0.4360 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4360)\r\n",
      "Epoch 002 | Train Loss: 0.4432 | Val Loss: 0.4357 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4357)\r\n",
      "Epoch 003 | Train Loss: 0.4422 | Val Loss: 0.4346 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4346)\r\n",
      "Epoch 004 | Train Loss: 0.4388 | Val Loss: 0.4369 | Time: 2.40s\r\n",
      "Epoch 005 | Train Loss: 0.4484 | Val Loss: 0.4371 | Time: 2.38s\r\n",
      "Epoch 006 | Train Loss: 0.4465 | Val Loss: 0.4376 | Time: 2.37s\r\n",
      "Epoch 007 | Train Loss: 0.4379 | Val Loss: 0.4232 | Time: 2.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4232)\r\n",
      "Epoch 008 | Train Loss: 0.4345 | Val Loss: 0.4261 | Time: 2.40s\r\n",
      "Epoch 009 | Train Loss: 0.4369 | Val Loss: 0.4283 | Time: 2.54s\r\n",
      "Epoch 010 | Train Loss: 0.4353 | Val Loss: 0.4259 | Time: 2.35s\r\n",
      "Epoch 011 | Train Loss: 0.4343 | Val Loss: 0.4258 | Time: 2.37s\r\n",
      "Epoch 012 | Train Loss: 0.4346 | Val Loss: 0.4282 | Time: 2.42s\r\n",
      "Epoch 013 | Train Loss: 0.4364 | Val Loss: 0.4336 | Time: 2.35s\r\n",
      "Epoch 014 | Train Loss: 0.4353 | Val Loss: 0.4333 | Time: 2.36s\r\n",
      "Epoch 015 | Train Loss: 0.4359 | Val Loss: 0.4499 | Time: 2.39s\r\n",
      "Epoch 016 | Train Loss: 0.4465 | Val Loss: 0.4372 | Time: 2.36s\r\n",
      "Epoch 017 | Train Loss: 0.4413 | Val Loss: 0.4311 | Time: 2.40s\r\n",
      "Epoch 018 | Train Loss: 0.4434 | Val Loss: 0.4340 | Time: 2.36s\r\n",
      "Epoch 019 | Train Loss: 0.4448 | Val Loss: 0.4372 | Time: 2.36s\r\n",
      "Epoch 020 | Train Loss: 0.4384 | Val Loss: 0.4206 | Time: 2.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4206)\r\n",
      "Epoch 021 | Train Loss: 0.4361 | Val Loss: 0.4265 | Time: 2.40s\r\n",
      "Epoch 022 | Train Loss: 0.4389 | Val Loss: 0.4506 | Time: 2.49s\r\n",
      "Epoch 023 | Train Loss: 0.4436 | Val Loss: 0.4368 | Time: 2.59s\r\n",
      "Epoch 024 | Train Loss: 0.4350 | Val Loss: 0.4380 | Time: 2.38s\r\n",
      "Epoch 025 | Train Loss: 0.4399 | Val Loss: 0.4352 | Time: 2.41s\r\n",
      "Epoch 026 | Train Loss: 0.4326 | Val Loss: 0.4362 | Time: 2.35s\r\n",
      "Epoch 027 | Train Loss: 0.4341 | Val Loss: 0.4313 | Time: 2.34s\r\n",
      "Epoch 028 | Train Loss: 0.4396 | Val Loss: 0.4308 | Time: 2.38s\r\n",
      "Epoch 029 | Train Loss: 0.4357 | Val Loss: 0.4274 | Time: 2.40s\r\n",
      "Epoch 030 | Train Loss: 0.4334 | Val Loss: 0.4313 | Time: 2.36s\r\n",
      "Epoch 031 | Train Loss: 0.4277 | Val Loss: 0.4304 | Time: 2.36s\r\n",
      "Epoch 032 | Train Loss: 0.4311 | Val Loss: 0.4389 | Time: 2.36s\r\n",
      "Epoch 033 | Train Loss: 0.4428 | Val Loss: 0.4383 | Time: 2.39s\r\n",
      "Epoch 034 | Train Loss: 0.4329 | Val Loss: 0.4347 | Time: 2.42s\r\n",
      "Epoch 035 | Train Loss: 0.4356 | Val Loss: 0.4371 | Time: 2.34s\r\n",
      "\r\n",
      "Early stopping à l'époque 35 (patience: 15)\r\n",
      "✅ Meilleur modèle Random chargé (époque 20, val_loss: 0.4206)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. gender               (CAT): 0.2956\r\n",
      "   2. Dependents           (CAT): 0.2902\r\n",
      "   3. StreamingMovies      (CAT): 0.1442\r\n",
      "   4. InternetService      (CAT): 0.1207\r\n",
      "   5. Partner              (CAT): 0.0653\r\n",
      "   6. PhoneService         (CAT): 0.0231\r\n",
      "   7. StreamingTV          (CAT): 0.0162\r\n",
      "   8. PaymentMethod        (CAT): 0.0151\r\n",
      "   9. MultipleLines        (CAT): 0.0100\r\n",
      "  10. PaperlessBilling     (CAT): 0.0098\r\n",
      "  11. Contract             (CAT): 0.0033\r\n",
      "  12. DeviceProtection     (CAT): 0.0033\r\n",
      "  13. tenure               (NUM): 0.0032\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.2956\r\n",
      "   2. Dependents          : 0.2902\r\n",
      "   3. StreamingMovies     : 0.1442\r\n",
      "   4. InternetService     : 0.1207\r\n",
      "   5. Partner             : 0.0653\r\n",
      "   6. PhoneService        : 0.0231\r\n",
      "   7. StreamingTV         : 0.0162\r\n",
      "   8. PaymentMethod       : 0.0151\r\n",
      "   9. MultipleLines       : 0.0100\r\n",
      "  10. PaperlessBilling    : 0.0098\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 440.6s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 214,337 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6953 | Val Loss: 0.6140 | Time: 7.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6140)\r\n",
      "Epoch 001 | Train Loss: 0.5888 | Val Loss: 0.5447 | Time: 7.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5447)\r\n",
      "Epoch 002 | Train Loss: 0.5486 | Val Loss: 0.5085 | Time: 6.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5085)\r\n",
      "Epoch 003 | Train Loss: 0.5232 | Val Loss: 0.4824 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4824)\r\n",
      "Epoch 004 | Train Loss: 0.5017 | Val Loss: 0.4652 | Time: 7.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4652)\r\n",
      "Epoch 005 | Train Loss: 0.4895 | Val Loss: 0.4544 | Time: 7.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4544)\r\n",
      "Epoch 006 | Train Loss: 0.4798 | Val Loss: 0.4467 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4467)\r\n",
      "Epoch 007 | Train Loss: 0.4696 | Val Loss: 0.4417 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 008 | Train Loss: 0.4648 | Val Loss: 0.4388 | Time: 7.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4388)\r\n",
      "Epoch 009 | Train Loss: 0.4605 | Val Loss: 0.4367 | Time: 6.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4367)\r\n",
      "Epoch 010 | Train Loss: 0.4554 | Val Loss: 0.4346 | Time: 7.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4346)\r\n",
      "Epoch 011 | Train Loss: 0.4510 | Val Loss: 0.4328 | Time: 7.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4328)\r\n",
      "Epoch 012 | Train Loss: 0.4516 | Val Loss: 0.4314 | Time: 7.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4314)\r\n",
      "Epoch 013 | Train Loss: 0.4510 | Val Loss: 0.4305 | Time: 7.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4305)\r\n",
      "Epoch 014 | Train Loss: 0.4461 | Val Loss: 0.4297 | Time: 7.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 015 | Train Loss: 0.4447 | Val Loss: 0.4305 | Time: 6.95s\r\n",
      "Epoch 016 | Train Loss: 0.4417 | Val Loss: 0.4294 | Time: 6.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4294)\r\n",
      "Epoch 017 | Train Loss: 0.4447 | Val Loss: 0.4288 | Time: 7.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4288)\r\n",
      "Epoch 018 | Train Loss: 0.4416 | Val Loss: 0.4280 | Time: 7.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4280)\r\n",
      "Epoch 019 | Train Loss: 0.4395 | Val Loss: 0.4278 | Time: 6.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4278)\r\n",
      "Epoch 020 | Train Loss: 0.4413 | Val Loss: 0.4285 | Time: 6.99s\r\n",
      "Epoch 021 | Train Loss: 0.4399 | Val Loss: 0.4277 | Time: 7.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4277)\r\n",
      "Epoch 022 | Train Loss: 0.4394 | Val Loss: 0.4271 | Time: 7.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4271)\r\n",
      "Epoch 023 | Train Loss: 0.4382 | Val Loss: 0.4265 | Time: 7.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 024 | Train Loss: 0.4379 | Val Loss: 0.4270 | Time: 7.05s\r\n",
      "Epoch 025 | Train Loss: 0.4377 | Val Loss: 0.4269 | Time: 7.00s\r\n",
      "Epoch 026 | Train Loss: 0.4346 | Val Loss: 0.4261 | Time: 7.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4261)\r\n",
      "Epoch 027 | Train Loss: 0.4349 | Val Loss: 0.4259 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4259)\r\n",
      "Epoch 028 | Train Loss: 0.4346 | Val Loss: 0.4255 | Time: 7.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 029 | Train Loss: 0.4321 | Val Loss: 0.4248 | Time: 7.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4248)\r\n",
      "Epoch 030 | Train Loss: 0.4327 | Val Loss: 0.4244 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4244)\r\n",
      "Epoch 031 | Train Loss: 0.4318 | Val Loss: 0.4235 | Time: 7.13s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 032 | Train Loss: 0.4317 | Val Loss: 0.4231 | Time: 6.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4231)\r\n",
      "Epoch 033 | Train Loss: 0.4320 | Val Loss: 0.4229 | Time: 6.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4229)\r\n",
      "Epoch 034 | Train Loss: 0.4286 | Val Loss: 0.4224 | Time: 7.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4224)\r\n",
      "Epoch 035 | Train Loss: 0.4313 | Val Loss: 0.4222 | Time: 7.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 036 | Train Loss: 0.4263 | Val Loss: 0.4211 | Time: 6.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4211)\r\n",
      "Epoch 037 | Train Loss: 0.4283 | Val Loss: 0.4211 | Time: 7.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4211)\r\n",
      "Epoch 038 | Train Loss: 0.4278 | Val Loss: 0.4202 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4202)\r\n",
      "Epoch 039 | Train Loss: 0.4284 | Val Loss: 0.4203 | Time: 7.01s\r\n",
      "Epoch 040 | Train Loss: 0.4266 | Val Loss: 0.4204 | Time: 7.13s\r\n",
      "Epoch 041 | Train Loss: 0.4246 | Val Loss: 0.4197 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 042 | Train Loss: 0.4287 | Val Loss: 0.4190 | Time: 7.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 043 | Train Loss: 0.4270 | Val Loss: 0.4186 | Time: 6.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 044 | Train Loss: 0.4248 | Val Loss: 0.4184 | Time: 7.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 045 | Train Loss: 0.4258 | Val Loss: 0.4185 | Time: 7.05s\r\n",
      "Epoch 046 | Train Loss: 0.4245 | Val Loss: 0.4189 | Time: 6.96s\r\n",
      "Epoch 047 | Train Loss: 0.4234 | Val Loss: 0.4180 | Time: 7.01s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 048 | Train Loss: 0.4282 | Val Loss: 0.4182 | Time: 7.03s\r\n",
      "Epoch 049 | Train Loss: 0.4255 | Val Loss: 0.4187 | Time: 7.10s\r\n",
      "✅ Meilleur modèle chargé (époque 47, val_loss: 0.4180)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.0561\r\n",
      "   2. PaperlessBilling    : 0.0544\r\n",
      "   3. InternetService     : 0.0541\r\n",
      "   4. Partner             : 0.0535\r\n",
      "   5. Dependents          : 0.0535\r\n",
      "   6. DeviceProtection    : 0.0530\r\n",
      "   7. OnlineSecurity      : 0.0528\r\n",
      "   8. gender              : 0.0525\r\n",
      "   9. StreamingMovies     : 0.0525\r\n",
      "  10. TotalCharges        : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 13 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. tenure               (NUM): 0.0561\r\n",
      "   2. PaperlessBilling     (CAT): 0.0544\r\n",
      "   3. InternetService      (CAT): 0.0541\r\n",
      "   4. Partner              (CAT): 0.0535\r\n",
      "   5. Dependents           (CAT): 0.0535\r\n",
      "   6. DeviceProtection     (CAT): 0.0530\r\n",
      "   7. OnlineSecurity       (CAT): 0.0528\r\n",
      "   8. gender               (CAT): 0.0525\r\n",
      "   9. StreamingMovies      (CAT): 0.0525\r\n",
      "  10. TotalCharges         (NUM): 0.0525\r\n",
      "  11. StreamingTV          (CAT): 0.0524\r\n",
      "  12. SeniorCitizen        (CAT): 0.0521\r\n",
      "  13. MonthlyCharges       (NUM): 0.0520\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'InternetService', 'Partner', 'Dependents', 'DeviceProtection', 'OnlineSecurity', 'gender', 'StreamingMovies', 'StreamingTV', 'SeniorCitizen'] → indices [14, 6, 2, 3, 9, 7, 0, 12, 11, 1]\r\n",
      "📊 Features sélectionnées: 3 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 354,049 paramètres\r\n",
      "🔗 Sparsité d'attention: 82.65%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.4796 | Val Loss: 0.4916 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4916)\r\n",
      "Epoch 001 | Train Loss: 0.4689 | Val Loss: 0.4791 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4791)\r\n",
      "Epoch 002 | Train Loss: 0.4602 | Val Loss: 0.4821 | Time: 2.37s\r\n",
      "Epoch 003 | Train Loss: 0.4634 | Val Loss: 0.4944 | Time: 2.41s\r\n",
      "Epoch 004 | Train Loss: 0.4643 | Val Loss: 0.4821 | Time: 2.40s\r\n",
      "Epoch 005 | Train Loss: 0.4621 | Val Loss: 0.4661 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4661)\r\n",
      "Epoch 006 | Train Loss: 0.4551 | Val Loss: 0.4734 | Time: 2.38s\r\n",
      "Epoch 007 | Train Loss: 0.4662 | Val Loss: 0.4764 | Time: 2.38s\r\n",
      "Epoch 008 | Train Loss: 0.4574 | Val Loss: 0.4553 | Time: 2.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4553)\r\n",
      "Epoch 009 | Train Loss: 0.4686 | Val Loss: 0.4592 | Time: 2.37s\r\n",
      "Epoch 010 | Train Loss: 0.4827 | Val Loss: 0.4693 | Time: 2.40s\r\n",
      "Epoch 011 | Train Loss: 0.4687 | Val Loss: 0.4557 | Time: 2.34s\r\n",
      "Epoch 012 | Train Loss: 0.4608 | Val Loss: 0.4433 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4433)\r\n",
      "Epoch 013 | Train Loss: 0.4650 | Val Loss: 0.4436 | Time: 2.34s\r\n",
      "Epoch 014 | Train Loss: 0.4637 | Val Loss: 0.4458 | Time: 2.39s\r\n",
      "Epoch 015 | Train Loss: 0.4590 | Val Loss: 0.4354 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4354)\r\n",
      "Epoch 016 | Train Loss: 0.4593 | Val Loss: 0.4372 | Time: 2.41s\r\n",
      "Epoch 017 | Train Loss: 0.4566 | Val Loss: 0.4505 | Time: 2.36s\r\n",
      "Epoch 018 | Train Loss: 0.4582 | Val Loss: 0.4364 | Time: 2.37s\r\n",
      "Epoch 019 | Train Loss: 0.4537 | Val Loss: 0.4488 | Time: 2.46s\r\n",
      "Epoch 020 | Train Loss: 0.4560 | Val Loss: 0.4441 | Time: 2.41s\r\n",
      "Epoch 021 | Train Loss: 0.4545 | Val Loss: 0.4349 | Time: 2.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4349)\r\n",
      "Epoch 022 | Train Loss: 0.4564 | Val Loss: 0.4371 | Time: 2.37s\r\n",
      "Epoch 023 | Train Loss: 0.4522 | Val Loss: 0.4348 | Time: 2.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4348)\r\n",
      "Epoch 024 | Train Loss: 0.4542 | Val Loss: 0.4496 | Time: 2.39s\r\n",
      "Epoch 025 | Train Loss: 0.4536 | Val Loss: 0.4359 | Time: 2.38s\r\n",
      "Epoch 026 | Train Loss: 0.4562 | Val Loss: 0.4405 | Time: 2.37s\r\n",
      "Epoch 027 | Train Loss: 0.4569 | Val Loss: 0.4370 | Time: 2.41s\r\n",
      "Epoch 028 | Train Loss: 0.4570 | Val Loss: 0.4294 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4294)\r\n",
      "Epoch 029 | Train Loss: 0.4514 | Val Loss: 0.4350 | Time: 2.39s\r\n",
      "Epoch 030 | Train Loss: 0.4569 | Val Loss: 0.4377 | Time: 2.38s\r\n",
      "Epoch 031 | Train Loss: 0.4542 | Val Loss: 0.4390 | Time: 2.38s\r\n",
      "Epoch 032 | Train Loss: 0.4573 | Val Loss: 0.4475 | Time: 2.39s\r\n",
      "Epoch 033 | Train Loss: 0.4582 | Val Loss: 0.4263 | Time: 2.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4263)\r\n",
      "Epoch 034 | Train Loss: 0.4562 | Val Loss: 0.4393 | Time: 2.36s\r\n",
      "Epoch 035 | Train Loss: 0.4612 | Val Loss: 0.4370 | Time: 2.35s\r\n",
      "Epoch 036 | Train Loss: 0.4572 | Val Loss: 0.4424 | Time: 2.40s\r\n",
      "Epoch 037 | Train Loss: 0.4661 | Val Loss: 0.4312 | Time: 2.44s\r\n",
      "Epoch 038 | Train Loss: 0.4576 | Val Loss: 0.4358 | Time: 2.37s\r\n",
      "Epoch 039 | Train Loss: 0.4594 | Val Loss: 0.4295 | Time: 2.37s\r\n",
      "Epoch 040 | Train Loss: 0.4561 | Val Loss: 0.4405 | Time: 2.39s\r\n",
      "Epoch 041 | Train Loss: 0.4555 | Val Loss: 0.4323 | Time: 2.40s\r\n",
      "Epoch 042 | Train Loss: 0.4509 | Val Loss: 0.4424 | Time: 2.38s\r\n",
      "Epoch 043 | Train Loss: 0.4528 | Val Loss: 0.4383 | Time: 2.35s\r\n",
      "Epoch 044 | Train Loss: 0.4472 | Val Loss: 0.4404 | Time: 2.35s\r\n",
      "Epoch 045 | Train Loss: 0.4461 | Val Loss: 0.4241 | Time: 2.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 046 | Train Loss: 0.4572 | Val Loss: 0.4282 | Time: 2.39s\r\n",
      "Epoch 047 | Train Loss: 0.4544 | Val Loss: 0.4338 | Time: 2.36s\r\n",
      "Epoch 048 | Train Loss: 0.4532 | Val Loss: 0.4284 | Time: 2.37s\r\n",
      "Epoch 049 | Train Loss: 0.4531 | Val Loss: 0.4313 | Time: 2.38s\r\n",
      "✅ Meilleur modèle Random chargé (époque 45, val_loss: 0.4241)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. InternetService      (CAT): 0.0778\r\n",
      "   2. PaperlessBilling     (CAT): 0.0778\r\n",
      "   3. DeviceProtection     (CAT): 0.0777\r\n",
      "   4. tenure               (NUM): 0.0776\r\n",
      "   5. SeniorCitizen        (CAT): 0.0774\r\n",
      "   6. TotalCharges         (NUM): 0.0770\r\n",
      "   7. StreamingMovies      (CAT): 0.0767\r\n",
      "   8. OnlineSecurity       (CAT): 0.0767\r\n",
      "   9. Dependents           (CAT): 0.0767\r\n",
      "  10. gender               (CAT): 0.0766\r\n",
      "  11. StreamingTV          (CAT): 0.0764\r\n",
      "  12. Partner              (CAT): 0.0761\r\n",
      "  13. MonthlyCharges       (NUM): 0.0753\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. InternetService     : 0.0778\r\n",
      "   2. PaperlessBilling    : 0.0778\r\n",
      "   3. DeviceProtection    : 0.0777\r\n",
      "   4. tenure              : 0.0776\r\n",
      "   5. SeniorCitizen       : 0.0774\r\n",
      "   6. TotalCharges        : 0.0770\r\n",
      "   7. StreamingMovies     : 0.0767\r\n",
      "   8. OnlineSecurity      : 0.0767\r\n",
      "   9. Dependents          : 0.0767\r\n",
      "  10. gender              : 0.0766\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_18/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 474.5s ===\r\n",
      "\u001b[32m[I 2025-07-20 00:25:08,057]\u001b[0m Trial 18 finished with value: 0.0 and parameters: {'d_token_stage1': 64, 'n_blocks_stage1': 5, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 128, 'attention_dropout_stage1': 0.29026028643181784, 'ffn_dropout_stage1': 0.1798609540189901, 'residual_dropout_stage1': 0.1641296586201532, 'lr_stage1': 1.2630670152118646e-05, 'weight_decay_stage1': 1.5793595601656016e-06, 'd_token_stage2': 128, 'n_blocks_stage2': 3, 'n_heads_stage2': 2, 'ffn_hidden_stage2': 128, 'attention_dropout_stage2': 0.10122311859497442, 'ffn_dropout_stage2': 0.201809380177114, 'residual_dropout_stage2': 0.15244813937254395, 'lr_stage2': 0.0029369779338536323, 'weight_decay_stage2': 0.08400736191386067, 'batch_size': 64, 'patience': 15, 'embedding_type': 'T', 'M': 13, 'k': 4}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  76%|█████▎ | 19/25 [6:12:10<2:07:12, 1272.05s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR\r\n",
      "Modèle FTT+ créé avec 38,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5952 | Val Loss: 0.6327 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6327)\r\n",
      "Epoch 001 | Train Loss: 0.5831 | Val Loss: 0.6225 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6225)\r\n",
      "Epoch 002 | Train Loss: 0.5822 | Val Loss: 0.5841 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5841)\r\n",
      "Epoch 003 | Train Loss: 0.5792 | Val Loss: 0.5840 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5840)\r\n",
      "Epoch 004 | Train Loss: 0.5788 | Val Loss: 0.5839 | Time: 1.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5839)\r\n",
      "Epoch 005 | Train Loss: 0.5795 | Val Loss: 0.5841 | Time: 1.31s\r\n",
      "Epoch 006 | Train Loss: 0.5796 | Val Loss: 0.5841 | Time: 1.26s\r\n",
      "Epoch 007 | Train Loss: 0.5797 | Val Loss: 0.5841 | Time: 1.32s\r\n",
      "Epoch 008 | Train Loss: 0.5797 | Val Loss: 0.5841 | Time: 1.28s\r\n",
      "Epoch 009 | Train Loss: 0.5797 | Val Loss: 0.5841 | Time: 1.28s\r\n",
      "Epoch 010 | Train Loss: 0.5797 | Val Loss: 0.5840 | Time: 1.28s\r\n",
      "Epoch 011 | Train Loss: 0.5797 | Val Loss: 0.5840 | Time: 1.27s\r\n",
      "Epoch 012 | Train Loss: 0.5797 | Val Loss: 0.5840 | Time: 1.28s\r\n",
      "Epoch 013 | Train Loss: 0.5793 | Val Loss: 0.5841 | Time: 1.31s\r\n",
      "Epoch 014 | Train Loss: 0.5795 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 015 | Train Loss: 0.5784 | Val Loss: 0.5953 | Time: 1.27s\r\n",
      "Epoch 016 | Train Loss: 0.5784 | Val Loss: 0.5840 | Time: 1.26s\r\n",
      "Epoch 017 | Train Loss: 0.5791 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5839)\r\n",
      "Epoch 018 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 019 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 020 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.36s\r\n",
      "Epoch 021 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.30s\r\n",
      "Epoch 022 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 023 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.32s\r\n",
      "Epoch 024 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 025 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.26s\r\n",
      "Epoch 026 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.31s\r\n",
      "Epoch 027 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 028 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 029 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.31s\r\n",
      "Epoch 030 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 031 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 032 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 033 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 034 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 035 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 036 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.31s\r\n",
      "Epoch 037 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 038 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.27s\r\n",
      "Epoch 039 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "Epoch 040 | Train Loss: 0.5797 | Val Loss: 0.5839 | Time: 1.28s\r\n",
      "\r\n",
      "Early stopping à l'époque 40 (patience: 23)\r\n",
      "✅ Meilleur modèle chargé (époque 17, val_loss: 0.5839)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.0593\r\n",
      "   2. tenure              : 0.0571\r\n",
      "   3. PaymentMethod       : 0.0521\r\n",
      "   4. StreamingMovies     : 0.0520\r\n",
      "   5. Contract            : 0.0520\r\n",
      "   6. DeviceProtection    : 0.0520\r\n",
      "   7. InternetService     : 0.0520\r\n",
      "   8. PhoneService        : 0.0520\r\n",
      "   9. StreamingTV         : 0.0520\r\n",
      "  10. TotalCharges        : 0.0520\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 10 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MonthlyCharges       (NUM): 0.0593\r\n",
      "   2. tenure               (NUM): 0.0571\r\n",
      "   3. PaymentMethod        (CAT): 0.0521\r\n",
      "   4. StreamingMovies      (CAT): 0.0520\r\n",
      "   5. Contract             (CAT): 0.0520\r\n",
      "   6. DeviceProtection     (CAT): 0.0520\r\n",
      "   7. InternetService      (CAT): 0.0520\r\n",
      "   8. PhoneService         (CAT): 0.0520\r\n",
      "   9. StreamingTV          (CAT): 0.0520\r\n",
      "  10. TotalCharges         (NUM): 0.0520\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure', 'TotalCharges'] → indices [1, 0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'StreamingMovies', 'Contract', 'DeviceProtection', 'InternetService', 'PhoneService', 'StreamingTV'] → indices [15, 12, 13, 9, 6, 4, 11]\r\n",
      "📊 Features sélectionnées: 3 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 149,313 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.25%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5656 | Val Loss: 0.4685 | Time: 1.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4685)\r\n",
      "Epoch 001 | Train Loss: 0.4865 | Val Loss: 0.4532 | Time: 1.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4532)\r\n",
      "Epoch 002 | Train Loss: 0.4686 | Val Loss: 0.4457 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4457)\r\n",
      "Epoch 003 | Train Loss: 0.4573 | Val Loss: 0.4417 | Time: 1.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 004 | Train Loss: 0.4500 | Val Loss: 0.4392 | Time: 1.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4392)\r\n",
      "Epoch 005 | Train Loss: 0.4506 | Val Loss: 0.4358 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4358)\r\n",
      "Epoch 006 | Train Loss: 0.4497 | Val Loss: 0.4344 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4344)\r\n",
      "Epoch 007 | Train Loss: 0.4453 | Val Loss: 0.4326 | Time: 1.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4326)\r\n",
      "Epoch 008 | Train Loss: 0.4441 | Val Loss: 0.4311 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4311)\r\n",
      "Epoch 009 | Train Loss: 0.4404 | Val Loss: 0.4341 | Time: 1.92s\r\n",
      "Epoch 010 | Train Loss: 0.4431 | Val Loss: 0.4303 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4303)\r\n",
      "Epoch 011 | Train Loss: 0.4423 | Val Loss: 0.4314 | Time: 1.91s\r\n",
      "Epoch 012 | Train Loss: 0.4363 | Val Loss: 0.4323 | Time: 1.93s\r\n",
      "Epoch 013 | Train Loss: 0.4376 | Val Loss: 0.4302 | Time: 1.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4302)\r\n",
      "Epoch 014 | Train Loss: 0.4358 | Val Loss: 0.4323 | Time: 1.90s\r\n",
      "Epoch 015 | Train Loss: 0.4356 | Val Loss: 0.4301 | Time: 1.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4301)\r\n",
      "Epoch 016 | Train Loss: 0.4392 | Val Loss: 0.4314 | Time: 1.98s\r\n",
      "Epoch 017 | Train Loss: 0.4364 | Val Loss: 0.4317 | Time: 1.95s\r\n",
      "Epoch 018 | Train Loss: 0.4356 | Val Loss: 0.4273 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4273)\r\n",
      "Epoch 019 | Train Loss: 0.4364 | Val Loss: 0.4276 | Time: 1.93s\r\n",
      "Epoch 020 | Train Loss: 0.4319 | Val Loss: 0.4308 | Time: 2.04s\r\n",
      "Epoch 021 | Train Loss: 0.4345 | Val Loss: 0.4289 | Time: 1.92s\r\n",
      "Epoch 022 | Train Loss: 0.4331 | Val Loss: 0.4274 | Time: 1.95s\r\n",
      "Epoch 023 | Train Loss: 0.4337 | Val Loss: 0.4253 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4253)\r\n",
      "Epoch 024 | Train Loss: 0.4282 | Val Loss: 0.4264 | Time: 1.92s\r\n",
      "Epoch 025 | Train Loss: 0.4300 | Val Loss: 0.4245 | Time: 1.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4245)\r\n",
      "Epoch 026 | Train Loss: 0.4327 | Val Loss: 0.4239 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "Epoch 027 | Train Loss: 0.4313 | Val Loss: 0.4283 | Time: 1.97s\r\n",
      "Epoch 028 | Train Loss: 0.4286 | Val Loss: 0.4291 | Time: 1.92s\r\n",
      "Epoch 029 | Train Loss: 0.4333 | Val Loss: 0.4295 | Time: 1.92s\r\n",
      "Epoch 030 | Train Loss: 0.4303 | Val Loss: 0.4254 | Time: 1.91s\r\n",
      "Epoch 031 | Train Loss: 0.4289 | Val Loss: 0.4264 | Time: 1.90s\r\n",
      "Epoch 032 | Train Loss: 0.4326 | Val Loss: 0.4235 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 033 | Train Loss: 0.4301 | Val Loss: 0.4275 | Time: 1.95s\r\n",
      "Epoch 034 | Train Loss: 0.4302 | Val Loss: 0.4243 | Time: 1.92s\r\n",
      "Epoch 035 | Train Loss: 0.4278 | Val Loss: 0.4276 | Time: 1.89s\r\n",
      "Epoch 036 | Train Loss: 0.4302 | Val Loss: 0.4277 | Time: 1.97s\r\n",
      "Epoch 037 | Train Loss: 0.4313 | Val Loss: 0.4273 | Time: 1.93s\r\n",
      "Epoch 038 | Train Loss: 0.4276 | Val Loss: 0.4264 | Time: 1.93s\r\n",
      "Epoch 039 | Train Loss: 0.4248 | Val Loss: 0.4254 | Time: 1.93s\r\n",
      "Epoch 040 | Train Loss: 0.4260 | Val Loss: 0.4272 | Time: 1.91s\r\n",
      "Epoch 041 | Train Loss: 0.4269 | Val Loss: 0.4265 | Time: 1.90s\r\n",
      "Epoch 042 | Train Loss: 0.4266 | Val Loss: 0.4274 | Time: 1.91s\r\n",
      "Epoch 043 | Train Loss: 0.4283 | Val Loss: 0.4247 | Time: 1.94s\r\n",
      "Epoch 044 | Train Loss: 0.4273 | Val Loss: 0.4257 | Time: 1.91s\r\n",
      "Epoch 045 | Train Loss: 0.4259 | Val Loss: 0.4294 | Time: 1.93s\r\n",
      "Epoch 046 | Train Loss: 0.4269 | Val Loss: 0.4257 | Time: 1.92s\r\n",
      "Epoch 047 | Train Loss: 0.4259 | Val Loss: 0.4242 | Time: 1.93s\r\n",
      "Epoch 048 | Train Loss: 0.4305 | Val Loss: 0.4265 | Time: 1.94s\r\n",
      "Epoch 049 | Train Loss: 0.4264 | Val Loss: 0.4235 | Time: 1.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4235)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.1093\r\n",
      "   2. TotalCharges         (NUM): 0.1084\r\n",
      "   3. InternetService      (CAT): 0.1071\r\n",
      "   4. StreamingMovies      (CAT): 0.1060\r\n",
      "   5. tenure               (NUM): 0.1017\r\n",
      "   6. MonthlyCharges       (NUM): 0.0983\r\n",
      "   7. StreamingTV          (CAT): 0.0969\r\n",
      "   8. PaymentMethod        (CAT): 0.0946\r\n",
      "   9. DeviceProtection     (CAT): 0.0932\r\n",
      "  10. Contract             (CAT): 0.0846\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.1093\r\n",
      "   2. TotalCharges        : 0.1084\r\n",
      "   3. InternetService     : 0.1071\r\n",
      "   4. StreamingMovies     : 0.1060\r\n",
      "   5. tenure              : 0.1017\r\n",
      "   6. MonthlyCharges      : 0.0983\r\n",
      "   7. StreamingTV         : 0.0969\r\n",
      "   8. PaymentMethod       : 0.0946\r\n",
      "   9. DeviceProtection    : 0.0932\r\n",
      "  10. Contract            : 0.0846\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 152.1s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR\r\n",
      "Modèle FTT+ créé avec 38,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5919 | Val Loss: 0.5325 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5325)\r\n",
      "Epoch 001 | Train Loss: 0.5361 | Val Loss: 0.5700 | Time: 1.27s\r\n",
      "Epoch 002 | Train Loss: 0.5509 | Val Loss: 0.5568 | Time: 1.26s\r\n",
      "Epoch 003 | Train Loss: 0.5457 | Val Loss: 0.5438 | Time: 1.38s\r\n",
      "Epoch 004 | Train Loss: 0.5798 | Val Loss: 0.5732 | Time: 1.29s\r\n",
      "Epoch 005 | Train Loss: 0.5656 | Val Loss: 0.5902 | Time: 1.27s\r\n",
      "Epoch 006 | Train Loss: 0.5514 | Val Loss: 0.5165 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5165)\r\n",
      "Epoch 007 | Train Loss: 0.5412 | Val Loss: 0.5118 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5118)\r\n",
      "Epoch 008 | Train Loss: 0.5322 | Val Loss: 0.5055 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5055)\r\n",
      "Epoch 009 | Train Loss: 0.5261 | Val Loss: 0.4987 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4987)\r\n",
      "Epoch 010 | Train Loss: 0.5201 | Val Loss: 0.4942 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4942)\r\n",
      "Epoch 011 | Train Loss: 0.5087 | Val Loss: 0.4926 | Time: 1.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4926)\r\n",
      "Epoch 012 | Train Loss: 0.5067 | Val Loss: 0.5139 | Time: 1.28s\r\n",
      "Epoch 013 | Train Loss: 0.5148 | Val Loss: 0.4907 | Time: 1.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4907)\r\n",
      "Epoch 014 | Train Loss: 0.5097 | Val Loss: 0.4857 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4857)\r\n",
      "Epoch 015 | Train Loss: 0.4966 | Val Loss: 0.5102 | Time: 1.27s\r\n",
      "Epoch 016 | Train Loss: 0.5246 | Val Loss: 0.5110 | Time: 1.28s\r\n",
      "Epoch 017 | Train Loss: 0.5202 | Val Loss: 0.5044 | Time: 1.28s\r\n",
      "Epoch 018 | Train Loss: 0.5122 | Val Loss: 0.4903 | Time: 1.27s\r\n",
      "Epoch 019 | Train Loss: 0.4897 | Val Loss: 0.4898 | Time: 1.29s\r\n",
      "Epoch 020 | Train Loss: 0.4964 | Val Loss: 0.4912 | Time: 1.27s\r\n",
      "Epoch 021 | Train Loss: 0.5446 | Val Loss: 0.5738 | Time: 1.27s\r\n",
      "Epoch 022 | Train Loss: 0.5331 | Val Loss: 0.4801 | Time: 1.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4801)\r\n",
      "Epoch 023 | Train Loss: 0.4994 | Val Loss: 0.4845 | Time: 1.26s\r\n",
      "Epoch 024 | Train Loss: 0.4976 | Val Loss: 0.4891 | Time: 1.26s\r\n",
      "Epoch 025 | Train Loss: 0.4948 | Val Loss: 0.4822 | Time: 1.26s\r\n",
      "Epoch 026 | Train Loss: 0.4896 | Val Loss: 0.4837 | Time: 1.29s\r\n",
      "Epoch 027 | Train Loss: 0.4987 | Val Loss: 0.4863 | Time: 1.41s\r\n",
      "Epoch 028 | Train Loss: 0.4826 | Val Loss: 0.4830 | Time: 1.30s\r\n",
      "Epoch 029 | Train Loss: 0.4773 | Val Loss: 0.4778 | Time: 1.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4778)\r\n",
      "Epoch 030 | Train Loss: 0.4788 | Val Loss: 0.4790 | Time: 1.26s\r\n",
      "Epoch 031 | Train Loss: 0.4856 | Val Loss: 0.4824 | Time: 1.26s\r\n",
      "Epoch 032 | Train Loss: 0.4930 | Val Loss: 0.4836 | Time: 1.25s\r\n",
      "Epoch 033 | Train Loss: 0.4885 | Val Loss: 0.4828 | Time: 1.25s\r\n",
      "Epoch 034 | Train Loss: 0.4871 | Val Loss: 0.4836 | Time: 1.25s\r\n",
      "Epoch 035 | Train Loss: 0.4942 | Val Loss: 0.4823 | Time: 1.30s\r\n",
      "Epoch 036 | Train Loss: 0.4920 | Val Loss: 0.4848 | Time: 1.28s\r\n",
      "Epoch 037 | Train Loss: 0.4829 | Val Loss: 0.4773 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4773)\r\n",
      "Epoch 038 | Train Loss: 0.4883 | Val Loss: 0.4814 | Time: 1.27s\r\n",
      "Epoch 039 | Train Loss: 0.4784 | Val Loss: 0.4800 | Time: 1.27s\r\n",
      "Epoch 040 | Train Loss: 0.4851 | Val Loss: 0.4757 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4757)\r\n",
      "Epoch 041 | Train Loss: 0.4733 | Val Loss: 0.4727 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4727)\r\n",
      "Epoch 042 | Train Loss: 0.4855 | Val Loss: 0.4658 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4658)\r\n",
      "Epoch 043 | Train Loss: 0.4957 | Val Loss: 0.4765 | Time: 1.29s\r\n",
      "Epoch 044 | Train Loss: 0.4997 | Val Loss: 0.4922 | Time: 1.26s\r\n",
      "Epoch 045 | Train Loss: 0.5058 | Val Loss: 0.4769 | Time: 1.26s\r\n",
      "Epoch 046 | Train Loss: 0.4943 | Val Loss: 0.4727 | Time: 1.27s\r\n",
      "Epoch 047 | Train Loss: 0.4849 | Val Loss: 0.4701 | Time: 1.26s\r\n",
      "Epoch 048 | Train Loss: 0.4840 | Val Loss: 0.4837 | Time: 1.28s\r\n",
      "Epoch 049 | Train Loss: 0.4890 | Val Loss: 0.4741 | Time: 1.26s\r\n",
      "✅ Meilleur modèle chargé (époque 42, val_loss: 0.4658)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MultipleLines       : 0.1102\r\n",
      "   2. Contract            : 0.0993\r\n",
      "   3. StreamingMovies     : 0.0733\r\n",
      "   4. tenure              : 0.0648\r\n",
      "   5. gender              : 0.0640\r\n",
      "   6. OnlineSecurity      : 0.0601\r\n",
      "   7. TotalCharges        : 0.0597\r\n",
      "   8. TechSupport         : 0.0573\r\n",
      "   9. Dependents          : 0.0513\r\n",
      "  10. SeniorCitizen       : 0.0490\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 10 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MultipleLines        (CAT): 0.1102\r\n",
      "   2. Contract             (CAT): 0.0993\r\n",
      "   3. StreamingMovies      (CAT): 0.0733\r\n",
      "   4. tenure               (NUM): 0.0648\r\n",
      "   5. gender               (CAT): 0.0640\r\n",
      "   6. OnlineSecurity       (CAT): 0.0601\r\n",
      "   7. TotalCharges         (NUM): 0.0597\r\n",
      "   8. TechSupport          (CAT): 0.0573\r\n",
      "   9. Dependents           (CAT): 0.0513\r\n",
      "  10. SeniorCitizen        (CAT): 0.0490\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges'] → indices [0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['MultipleLines', 'Contract', 'StreamingMovies', 'gender', 'OnlineSecurity', 'TechSupport', 'Dependents', 'SeniorCitizen'] → indices [5, 13, 12, 0, 7, 10, 3, 1]\r\n",
      "📊 Features sélectionnées: 2 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 149,249 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.25%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5098 | Val Loss: 0.4774 | Time: 1.96s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4774)\r\n",
      "Epoch 001 | Train Loss: 0.4702 | Val Loss: 0.4628 | Time: 2.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4628)\r\n",
      "Epoch 002 | Train Loss: 0.4548 | Val Loss: 0.4559 | Time: 1.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4559)\r\n",
      "Epoch 003 | Train Loss: 0.4550 | Val Loss: 0.4514 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4514)\r\n",
      "Epoch 004 | Train Loss: 0.4506 | Val Loss: 0.4477 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4477)\r\n",
      "Epoch 005 | Train Loss: 0.4502 | Val Loss: 0.4442 | Time: 1.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4442)\r\n",
      "Epoch 006 | Train Loss: 0.4422 | Val Loss: 0.4443 | Time: 1.89s\r\n",
      "Epoch 007 | Train Loss: 0.4407 | Val Loss: 0.4425 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 008 | Train Loss: 0.4387 | Val Loss: 0.4411 | Time: 1.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4411)\r\n",
      "Epoch 009 | Train Loss: 0.4411 | Val Loss: 0.4380 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4380)\r\n",
      "Epoch 010 | Train Loss: 0.4400 | Val Loss: 0.4393 | Time: 1.96s\r\n",
      "Epoch 011 | Train Loss: 0.4355 | Val Loss: 0.4372 | Time: 1.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4372)\r\n",
      "Epoch 012 | Train Loss: 0.4385 | Val Loss: 0.4347 | Time: 1.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4347)\r\n",
      "Epoch 013 | Train Loss: 0.4389 | Val Loss: 0.4345 | Time: 1.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 014 | Train Loss: 0.4336 | Val Loss: 0.4329 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4329)\r\n",
      "Epoch 015 | Train Loss: 0.4309 | Val Loss: 0.4337 | Time: 1.96s\r\n",
      "Epoch 016 | Train Loss: 0.4354 | Val Loss: 0.4330 | Time: 1.95s\r\n",
      "Epoch 017 | Train Loss: 0.4333 | Val Loss: 0.4347 | Time: 2.03s\r\n",
      "Epoch 018 | Train Loss: 0.4316 | Val Loss: 0.4338 | Time: 1.95s\r\n",
      "Epoch 019 | Train Loss: 0.4362 | Val Loss: 0.4325 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4325)\r\n",
      "Epoch 020 | Train Loss: 0.4324 | Val Loss: 0.4323 | Time: 1.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4323)\r\n",
      "Epoch 021 | Train Loss: 0.4270 | Val Loss: 0.4320 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4320)\r\n",
      "Epoch 022 | Train Loss: 0.4327 | Val Loss: 0.4333 | Time: 1.92s\r\n",
      "Epoch 023 | Train Loss: 0.4309 | Val Loss: 0.4313 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4313)\r\n",
      "Epoch 024 | Train Loss: 0.4322 | Val Loss: 0.4316 | Time: 1.91s\r\n",
      "Epoch 025 | Train Loss: 0.4287 | Val Loss: 0.4322 | Time: 1.95s\r\n",
      "Epoch 026 | Train Loss: 0.4262 | Val Loss: 0.4321 | Time: 1.94s\r\n",
      "Epoch 027 | Train Loss: 0.4306 | Val Loss: 0.4327 | Time: 1.92s\r\n",
      "Epoch 028 | Train Loss: 0.4255 | Val Loss: 0.4319 | Time: 1.93s\r\n",
      "Epoch 029 | Train Loss: 0.4271 | Val Loss: 0.4296 | Time: 1.99s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 030 | Train Loss: 0.4311 | Val Loss: 0.4295 | Time: 1.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4295)\r\n",
      "Epoch 031 | Train Loss: 0.4248 | Val Loss: 0.4306 | Time: 1.96s\r\n",
      "Epoch 032 | Train Loss: 0.4302 | Val Loss: 0.4317 | Time: 1.91s\r\n",
      "Epoch 033 | Train Loss: 0.4265 | Val Loss: 0.4306 | Time: 1.94s\r\n",
      "Epoch 034 | Train Loss: 0.4284 | Val Loss: 0.4311 | Time: 1.99s\r\n",
      "Epoch 035 | Train Loss: 0.4239 | Val Loss: 0.4321 | Time: 1.92s\r\n",
      "Epoch 036 | Train Loss: 0.4281 | Val Loss: 0.4313 | Time: 1.96s\r\n",
      "Epoch 037 | Train Loss: 0.4293 | Val Loss: 0.4310 | Time: 1.91s\r\n",
      "Epoch 038 | Train Loss: 0.4247 | Val Loss: 0.4311 | Time: 1.91s\r\n",
      "Epoch 039 | Train Loss: 0.4296 | Val Loss: 0.4323 | Time: 1.91s\r\n",
      "Epoch 040 | Train Loss: 0.4261 | Val Loss: 0.4316 | Time: 1.92s\r\n",
      "Epoch 041 | Train Loss: 0.4267 | Val Loss: 0.4335 | Time: 1.94s\r\n",
      "Epoch 042 | Train Loss: 0.4299 | Val Loss: 0.4334 | Time: 1.91s\r\n",
      "Epoch 043 | Train Loss: 0.4253 | Val Loss: 0.4330 | Time: 1.91s\r\n",
      "Epoch 044 | Train Loss: 0.4224 | Val Loss: 0.4327 | Time: 1.91s\r\n",
      "Epoch 045 | Train Loss: 0.4200 | Val Loss: 0.4322 | Time: 2.00s\r\n",
      "Epoch 046 | Train Loss: 0.4256 | Val Loss: 0.4309 | Time: 1.95s\r\n",
      "Epoch 047 | Train Loss: 0.4246 | Val Loss: 0.4306 | Time: 1.91s\r\n",
      "Epoch 048 | Train Loss: 0.4232 | Val Loss: 0.4319 | Time: 1.92s\r\n",
      "Epoch 049 | Train Loss: 0.4220 | Val Loss: 0.4313 | Time: 1.93s\r\n",
      "✅ Meilleur modèle Random chargé (époque 30, val_loss: 0.4295)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.1282\r\n",
      "   2. TechSupport          (CAT): 0.1236\r\n",
      "   3. Dependents           (CAT): 0.1161\r\n",
      "   4. TotalCharges         (NUM): 0.1097\r\n",
      "   5. Contract             (CAT): 0.1075\r\n",
      "   6. MultipleLines        (CAT): 0.0979\r\n",
      "   7. tenure               (NUM): 0.0839\r\n",
      "   8. StreamingMovies      (CAT): 0.0790\r\n",
      "   9. OnlineSecurity       (CAT): 0.0776\r\n",
      "  10. gender               (CAT): 0.0763\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.1282\r\n",
      "   2. TechSupport         : 0.1236\r\n",
      "   3. Dependents          : 0.1161\r\n",
      "   4. TotalCharges        : 0.1097\r\n",
      "   5. Contract            : 0.1075\r\n",
      "   6. MultipleLines       : 0.0979\r\n",
      "   7. tenure              : 0.0839\r\n",
      "   8. StreamingMovies     : 0.0790\r\n",
      "   9. OnlineSecurity      : 0.0776\r\n",
      "  10. gender              : 0.0763\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 163.8s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR\r\n",
      "Modèle FTT+ créé avec 38,289 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6029 | Val Loss: 0.5759 | Time: 1.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5759)\r\n",
      "Epoch 001 | Train Loss: 0.5519 | Val Loss: 0.5020 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5020)\r\n",
      "Epoch 002 | Train Loss: 0.5132 | Val Loss: 0.5179 | Time: 1.28s\r\n",
      "Epoch 003 | Train Loss: 0.5232 | Val Loss: 0.5107 | Time: 1.27s\r\n",
      "Epoch 004 | Train Loss: 0.5101 | Val Loss: 0.4790 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4790)\r\n",
      "Epoch 005 | Train Loss: 0.5016 | Val Loss: 0.4786 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4786)\r\n",
      "Epoch 006 | Train Loss: 0.5058 | Val Loss: 0.4826 | Time: 1.28s\r\n",
      "Epoch 007 | Train Loss: 0.5016 | Val Loss: 0.4810 | Time: 1.28s\r\n",
      "Epoch 008 | Train Loss: 0.5128 | Val Loss: 0.5107 | Time: 1.30s\r\n",
      "Epoch 009 | Train Loss: 0.5222 | Val Loss: 0.5289 | Time: 1.27s\r\n",
      "Epoch 010 | Train Loss: 0.5281 | Val Loss: 0.4749 | Time: 1.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4749)\r\n",
      "Epoch 011 | Train Loss: 0.5149 | Val Loss: 0.4827 | Time: 1.28s\r\n",
      "Epoch 012 | Train Loss: 0.5155 | Val Loss: 0.4859 | Time: 1.28s\r\n",
      "Epoch 013 | Train Loss: 0.5173 | Val Loss: 0.4930 | Time: 1.28s\r\n",
      "Epoch 014 | Train Loss: 0.5048 | Val Loss: 0.4764 | Time: 1.31s\r\n",
      "Epoch 015 | Train Loss: 0.4995 | Val Loss: 0.4790 | Time: 1.27s\r\n",
      "Epoch 016 | Train Loss: 0.5007 | Val Loss: 0.4755 | Time: 1.31s\r\n",
      "Epoch 017 | Train Loss: 0.4939 | Val Loss: 0.4710 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4710)\r\n",
      "Epoch 018 | Train Loss: 0.4920 | Val Loss: 0.4861 | Time: 1.26s\r\n",
      "Epoch 019 | Train Loss: 0.5056 | Val Loss: 0.4738 | Time: 1.26s\r\n",
      "Epoch 020 | Train Loss: 0.4989 | Val Loss: 0.4682 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4682)\r\n",
      "Epoch 021 | Train Loss: 0.5044 | Val Loss: 0.4716 | Time: 1.27s\r\n",
      "Epoch 022 | Train Loss: 0.4894 | Val Loss: 0.4693 | Time: 1.28s\r\n",
      "Epoch 023 | Train Loss: 0.4859 | Val Loss: 0.4701 | Time: 1.40s\r\n",
      "Epoch 024 | Train Loss: 0.4953 | Val Loss: 0.4627 | Time: 1.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4627)\r\n",
      "Epoch 025 | Train Loss: 0.4789 | Val Loss: 0.4545 | Time: 1.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4545)\r\n",
      "Epoch 026 | Train Loss: 0.4821 | Val Loss: 0.4648 | Time: 1.31s\r\n",
      "Epoch 027 | Train Loss: 0.4853 | Val Loss: 0.4531 | Time: 1.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4531)\r\n",
      "Epoch 028 | Train Loss: 0.4795 | Val Loss: 0.4604 | Time: 1.28s\r\n",
      "Epoch 029 | Train Loss: 0.4862 | Val Loss: 0.4609 | Time: 1.28s\r\n",
      "Epoch 030 | Train Loss: 0.4842 | Val Loss: 0.4561 | Time: 1.28s\r\n",
      "Epoch 031 | Train Loss: 0.4783 | Val Loss: 0.4649 | Time: 1.29s\r\n",
      "Epoch 032 | Train Loss: 0.4795 | Val Loss: 0.4660 | Time: 1.30s\r\n",
      "Epoch 033 | Train Loss: 0.4798 | Val Loss: 0.4646 | Time: 1.27s\r\n",
      "Epoch 034 | Train Loss: 0.4841 | Val Loss: 0.4725 | Time: 1.28s\r\n",
      "Epoch 035 | Train Loss: 0.4821 | Val Loss: 0.4662 | Time: 1.29s\r\n",
      "Epoch 036 | Train Loss: 0.4777 | Val Loss: 0.4595 | Time: 1.28s\r\n",
      "Epoch 037 | Train Loss: 0.4776 | Val Loss: 0.4595 | Time: 1.27s\r\n",
      "Epoch 038 | Train Loss: 0.4804 | Val Loss: 0.4587 | Time: 1.27s\r\n",
      "Epoch 039 | Train Loss: 0.4774 | Val Loss: 0.4576 | Time: 1.41s\r\n",
      "Epoch 040 | Train Loss: 0.4781 | Val Loss: 0.4638 | Time: 1.26s\r\n",
      "Epoch 041 | Train Loss: 0.4829 | Val Loss: 0.4937 | Time: 1.29s\r\n",
      "Epoch 042 | Train Loss: 0.5080 | Val Loss: 0.5139 | Time: 1.26s\r\n",
      "Epoch 043 | Train Loss: 0.5171 | Val Loss: 0.4941 | Time: 1.27s\r\n",
      "Epoch 044 | Train Loss: 0.5280 | Val Loss: 0.5107 | Time: 1.26s\r\n",
      "Epoch 045 | Train Loss: 0.5219 | Val Loss: 0.5078 | Time: 1.26s\r\n",
      "Epoch 046 | Train Loss: 0.5244 | Val Loss: 0.5543 | Time: 1.26s\r\n",
      "Epoch 047 | Train Loss: 0.5288 | Val Loss: 0.5309 | Time: 1.29s\r\n",
      "Epoch 048 | Train Loss: 0.5326 | Val Loss: 0.5122 | Time: 1.36s\r\n",
      "Epoch 049 | Train Loss: 0.5466 | Val Loss: 0.5652 | Time: 1.26s\r\n",
      "✅ Meilleur modèle chargé (époque 27, val_loss: 0.4531)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Partner             : 0.2884\r\n",
      "   2. MonthlyCharges      : 0.1218\r\n",
      "   3. InternetService     : 0.1027\r\n",
      "   4. tenure              : 0.0804\r\n",
      "   5. SeniorCitizen       : 0.0685\r\n",
      "   6. OnlineSecurity      : 0.0553\r\n",
      "   7. PaperlessBilling    : 0.0363\r\n",
      "   8. PhoneService        : 0.0343\r\n",
      "   9. DeviceProtection    : 0.0304\r\n",
      "  10. TotalCharges        : 0.0295\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 10 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Partner              (CAT): 0.2884\r\n",
      "   2. MonthlyCharges       (NUM): 0.1218\r\n",
      "   3. InternetService      (CAT): 0.1027\r\n",
      "   4. tenure               (NUM): 0.0804\r\n",
      "   5. SeniorCitizen        (CAT): 0.0685\r\n",
      "   6. OnlineSecurity       (CAT): 0.0553\r\n",
      "   7. PaperlessBilling     (CAT): 0.0363\r\n",
      "   8. PhoneService         (CAT): 0.0343\r\n",
      "   9. DeviceProtection     (CAT): 0.0304\r\n",
      "  10. TotalCharges         (NUM): 0.0295\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure', 'TotalCharges'] → indices [1, 0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['Partner', 'InternetService', 'SeniorCitizen', 'OnlineSecurity', 'PaperlessBilling', 'PhoneService', 'DeviceProtection'] → indices [2, 6, 1, 7, 14, 4, 9]\r\n",
      "📊 Features sélectionnées: 3 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 8 paires\r\n",
      "Modèle Random créé avec 149,057 paramètres\r\n",
      "🔗 Sparsité d'attention: 70.25%\r\n",
      "   - Connexions feature-feature: 16\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5383 | Val Loss: 0.4458 | Time: 1.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4458)\r\n",
      "Epoch 001 | Train Loss: 0.4819 | Val Loss: 0.4384 | Time: 1.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4384)\r\n",
      "Epoch 002 | Train Loss: 0.4661 | Val Loss: 0.4336 | Time: 1.91s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4336)\r\n",
      "Epoch 003 | Train Loss: 0.4636 | Val Loss: 0.4342 | Time: 1.95s\r\n",
      "Epoch 004 | Train Loss: 0.4571 | Val Loss: 0.4298 | Time: 1.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4298)\r\n",
      "Epoch 005 | Train Loss: 0.4576 | Val Loss: 0.4313 | Time: 1.93s\r\n",
      "Epoch 006 | Train Loss: 0.4523 | Val Loss: 0.4327 | Time: 1.96s\r\n",
      "Epoch 007 | Train Loss: 0.4472 | Val Loss: 0.4293 | Time: 1.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 008 | Train Loss: 0.4515 | Val Loss: 0.4304 | Time: 2.02s\r\n",
      "Epoch 009 | Train Loss: 0.4467 | Val Loss: 0.4267 | Time: 1.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4267)\r\n",
      "Epoch 010 | Train Loss: 0.4504 | Val Loss: 0.4288 | Time: 1.95s\r\n",
      "Epoch 011 | Train Loss: 0.4472 | Val Loss: 0.4246 | Time: 1.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4246)\r\n",
      "Epoch 012 | Train Loss: 0.4444 | Val Loss: 0.4252 | Time: 1.95s\r\n",
      "Epoch 013 | Train Loss: 0.4462 | Val Loss: 0.4241 | Time: 2.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 014 | Train Loss: 0.4464 | Val Loss: 0.4235 | Time: 2.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 015 | Train Loss: 0.4403 | Val Loss: 0.4256 | Time: 1.94s\r\n",
      "Epoch 016 | Train Loss: 0.4425 | Val Loss: 0.4255 | Time: 1.92s\r\n",
      "Epoch 017 | Train Loss: 0.4403 | Val Loss: 0.4243 | Time: 1.95s\r\n",
      "Epoch 018 | Train Loss: 0.4408 | Val Loss: 0.4239 | Time: 1.98s\r\n",
      "Epoch 019 | Train Loss: 0.4393 | Val Loss: 0.4251 | Time: 1.92s\r\n",
      "Epoch 020 | Train Loss: 0.4436 | Val Loss: 0.4216 | Time: 1.94s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 021 | Train Loss: 0.4434 | Val Loss: 0.4243 | Time: 1.92s\r\n",
      "Epoch 022 | Train Loss: 0.4408 | Val Loss: 0.4234 | Time: 1.92s\r\n",
      "Epoch 023 | Train Loss: 0.4366 | Val Loss: 0.4263 | Time: 1.94s\r\n",
      "Epoch 024 | Train Loss: 0.4404 | Val Loss: 0.4266 | Time: 1.97s\r\n",
      "Epoch 025 | Train Loss: 0.4374 | Val Loss: 0.4276 | Time: 1.94s\r\n",
      "Epoch 026 | Train Loss: 0.4388 | Val Loss: 0.4273 | Time: 1.91s\r\n",
      "Epoch 027 | Train Loss: 0.4346 | Val Loss: 0.4267 | Time: 1.93s\r\n",
      "Epoch 028 | Train Loss: 0.4365 | Val Loss: 0.4254 | Time: 1.94s\r\n",
      "Epoch 029 | Train Loss: 0.4359 | Val Loss: 0.4247 | Time: 1.91s\r\n",
      "Epoch 030 | Train Loss: 0.4372 | Val Loss: 0.4240 | Time: 1.97s\r\n",
      "Epoch 031 | Train Loss: 0.4381 | Val Loss: 0.4239 | Time: 1.91s\r\n",
      "Epoch 032 | Train Loss: 0.4372 | Val Loss: 0.4241 | Time: 1.90s\r\n",
      "Epoch 033 | Train Loss: 0.4328 | Val Loss: 0.4267 | Time: 1.94s\r\n",
      "Epoch 034 | Train Loss: 0.4298 | Val Loss: 0.4243 | Time: 1.91s\r\n",
      "Epoch 035 | Train Loss: 0.4366 | Val Loss: 0.4269 | Time: 1.93s\r\n",
      "Epoch 036 | Train Loss: 0.4340 | Val Loss: 0.4270 | Time: 1.92s\r\n",
      "Epoch 037 | Train Loss: 0.4357 | Val Loss: 0.4244 | Time: 1.94s\r\n",
      "Epoch 038 | Train Loss: 0.4346 | Val Loss: 0.4272 | Time: 1.95s\r\n",
      "Epoch 039 | Train Loss: 0.4342 | Val Loss: 0.4248 | Time: 1.96s\r\n",
      "Epoch 040 | Train Loss: 0.4380 | Val Loss: 0.4262 | Time: 1.97s\r\n",
      "Epoch 041 | Train Loss: 0.4377 | Val Loss: 0.4255 | Time: 1.90s\r\n",
      "Epoch 042 | Train Loss: 0.4337 | Val Loss: 0.4277 | Time: 1.91s\r\n",
      "Epoch 043 | Train Loss: 0.4327 | Val Loss: 0.4274 | Time: 1.92s\r\n",
      "\r\n",
      "Early stopping à l'époque 43 (patience: 23)\r\n",
      "✅ Meilleur modèle Random chargé (époque 20, val_loss: 0.4216)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. tenure               (NUM): 0.1101\r\n",
      "   2. DeviceProtection     (CAT): 0.1067\r\n",
      "   3. OnlineSecurity       (CAT): 0.1056\r\n",
      "   4. SeniorCitizen        (CAT): 0.1006\r\n",
      "   5. TotalCharges         (NUM): 0.1005\r\n",
      "   6. Partner              (CAT): 0.0979\r\n",
      "   7. PaperlessBilling     (CAT): 0.0973\r\n",
      "   8. InternetService      (CAT): 0.0973\r\n",
      "   9. MonthlyCharges       (NUM): 0.0962\r\n",
      "  10. PhoneService         (CAT): 0.0876\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.1101\r\n",
      "   2. DeviceProtection    : 0.1067\r\n",
      "   3. OnlineSecurity      : 0.1056\r\n",
      "   4. SeniorCitizen       : 0.1006\r\n",
      "   5. TotalCharges        : 0.1005\r\n",
      "   6. Partner             : 0.0979\r\n",
      "   7. PaperlessBilling    : 0.0973\r\n",
      "   8. InternetService     : 0.0973\r\n",
      "   9. MonthlyCharges      : 0.0962\r\n",
      "  10. PhoneService        : 0.0876\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_19/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 153.0s ===\r\n",
      "\u001b[32m[I 2025-07-20 00:32:57,436]\u001b[0m Trial 19 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 3, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 64, 'attention_dropout_stage1': 0.16832250894361128, 'ffn_dropout_stage1': 0.13021107646291183, 'residual_dropout_stage1': 0.13285470346028844, 'lr_stage1': 0.07701331994228075, 'weight_decay_stage1': 3.241330514797143e-05, 'd_token_stage2': 64, 'n_blocks_stage2': 5, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.28111918055891394, 'ffn_dropout_stage2': 0.29839129307264967, 'residual_dropout_stage2': 0.14527405821238623, 'lr_stage2': 0.0003629011243631176, 'weight_decay_stage2': 1.0709226141890311e-05, 'batch_size': 128, 'patience': 23, 'embedding_type': 'P-LR', 'M': 10, 'k': 8}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  80%|█████▌ | 20/25 [6:20:00<1:25:55, 1031.05s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR-LR\r\n",
      "Modèle FTT+ créé avec 125,265 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5551 | Val Loss: 0.4887 | Time: 6.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4887)\r\n",
      "Epoch 001 | Train Loss: 0.4845 | Val Loss: 0.4525 | Time: 6.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4525)\r\n",
      "Epoch 002 | Train Loss: 0.4585 | Val Loss: 0.4407 | Time: 6.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4407)\r\n",
      "Epoch 003 | Train Loss: 0.4476 | Val Loss: 0.4342 | Time: 6.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 004 | Train Loss: 0.4406 | Val Loss: 0.4306 | Time: 6.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4306)\r\n",
      "Epoch 005 | Train Loss: 0.4392 | Val Loss: 0.4276 | Time: 6.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4276)\r\n",
      "Epoch 006 | Train Loss: 0.4322 | Val Loss: 0.4253 | Time: 6.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4253)\r\n",
      "Epoch 007 | Train Loss: 0.4317 | Val Loss: 0.4208 | Time: 6.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4208)\r\n",
      "Epoch 008 | Train Loss: 0.4310 | Val Loss: 0.4216 | Time: 6.60s\r\n",
      "Epoch 009 | Train Loss: 0.4300 | Val Loss: 0.4194 | Time: 6.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 010 | Train Loss: 0.4266 | Val Loss: 0.4212 | Time: 6.67s\r\n",
      "Epoch 011 | Train Loss: 0.4263 | Val Loss: 0.4208 | Time: 6.57s\r\n",
      "Epoch 012 | Train Loss: 0.4242 | Val Loss: 0.4201 | Time: 6.54s\r\n",
      "Epoch 013 | Train Loss: 0.4238 | Val Loss: 0.4176 | Time: 6.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4176)\r\n",
      "Epoch 014 | Train Loss: 0.4217 | Val Loss: 0.4180 | Time: 6.66s\r\n",
      "Epoch 015 | Train Loss: 0.4206 | Val Loss: 0.4181 | Time: 6.56s\r\n",
      "Epoch 016 | Train Loss: 0.4183 | Val Loss: 0.4194 | Time: 6.54s\r\n",
      "Epoch 017 | Train Loss: 0.4173 | Val Loss: 0.4159 | Time: 6.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 018 | Train Loss: 0.4202 | Val Loss: 0.4148 | Time: 6.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4148)\r\n",
      "Epoch 019 | Train Loss: 0.4188 | Val Loss: 0.4150 | Time: 6.67s\r\n",
      "Epoch 020 | Train Loss: 0.4132 | Val Loss: 0.4176 | Time: 6.58s\r\n",
      "Epoch 021 | Train Loss: 0.4155 | Val Loss: 0.4159 | Time: 6.55s\r\n",
      "Epoch 022 | Train Loss: 0.4156 | Val Loss: 0.4151 | Time: 6.55s\r\n",
      "Epoch 023 | Train Loss: 0.4169 | Val Loss: 0.4123 | Time: 6.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4123)\r\n",
      "Epoch 024 | Train Loss: 0.4146 | Val Loss: 0.4158 | Time: 6.58s\r\n",
      "Epoch 025 | Train Loss: 0.4138 | Val Loss: 0.4147 | Time: 6.60s\r\n",
      "Epoch 026 | Train Loss: 0.4132 | Val Loss: 0.4141 | Time: 6.55s\r\n",
      "Epoch 027 | Train Loss: 0.4132 | Val Loss: 0.4158 | Time: 6.60s\r\n",
      "Epoch 028 | Train Loss: 0.4093 | Val Loss: 0.4154 | Time: 6.58s\r\n",
      "Epoch 029 | Train Loss: 0.4116 | Val Loss: 0.4165 | Time: 6.62s\r\n",
      "Epoch 030 | Train Loss: 0.4115 | Val Loss: 0.4151 | Time: 6.64s\r\n",
      "Epoch 031 | Train Loss: 0.4126 | Val Loss: 0.4127 | Time: 6.64s\r\n",
      "Epoch 032 | Train Loss: 0.4070 | Val Loss: 0.4145 | Time: 6.56s\r\n",
      "Epoch 033 | Train Loss: 0.4113 | Val Loss: 0.4141 | Time: 6.58s\r\n",
      "Epoch 034 | Train Loss: 0.4103 | Val Loss: 0.4121 | Time: 6.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4121)\r\n",
      "Epoch 035 | Train Loss: 0.4098 | Val Loss: 0.4144 | Time: 6.55s\r\n",
      "Epoch 036 | Train Loss: 0.4083 | Val Loss: 0.4150 | Time: 6.60s\r\n",
      "Epoch 037 | Train Loss: 0.4090 | Val Loss: 0.4175 | Time: 6.60s\r\n",
      "Epoch 038 | Train Loss: 0.4107 | Val Loss: 0.4160 | Time: 6.61s\r\n",
      "Epoch 039 | Train Loss: 0.4081 | Val Loss: 0.4171 | Time: 6.62s\r\n",
      "Epoch 040 | Train Loss: 0.4058 | Val Loss: 0.4177 | Time: 6.63s\r\n",
      "Epoch 041 | Train Loss: 0.4046 | Val Loss: 0.4153 | Time: 6.57s\r\n",
      "Epoch 042 | Train Loss: 0.4059 | Val Loss: 0.4165 | Time: 6.60s\r\n",
      "Epoch 043 | Train Loss: 0.4066 | Val Loss: 0.4229 | Time: 6.70s\r\n",
      "Epoch 044 | Train Loss: 0.4038 | Val Loss: 0.4213 | Time: 6.46s\r\n",
      "Epoch 045 | Train Loss: 0.4062 | Val Loss: 0.4212 | Time: 6.61s\r\n",
      "Epoch 046 | Train Loss: 0.4075 | Val Loss: 0.4209 | Time: 6.62s\r\n",
      "Epoch 047 | Train Loss: 0.4056 | Val Loss: 0.4223 | Time: 6.59s\r\n",
      "Epoch 048 | Train Loss: 0.4032 | Val Loss: 0.4219 | Time: 6.85s\r\n",
      "Epoch 049 | Train Loss: 0.4033 | Val Loss: 0.4234 | Time: 6.56s\r\n",
      "✅ Meilleur modèle chargé (époque 34, val_loss: 0.4121)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.0559\r\n",
      "   2. Partner             : 0.0549\r\n",
      "   3. Contract            : 0.0548\r\n",
      "   4. OnlineSecurity      : 0.0541\r\n",
      "   5. PaymentMethod       : 0.0537\r\n",
      "   6. PaperlessBilling    : 0.0533\r\n",
      "   7. StreamingTV         : 0.0531\r\n",
      "   8. TotalCharges        : 0.0527\r\n",
      "   9. MonthlyCharges      : 0.0526\r\n",
      "  10. SeniorCitizen       : 0.0525\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 8 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. DeviceProtection     (CAT): 0.0559\r\n",
      "   2. Partner              (CAT): 0.0549\r\n",
      "   3. Contract             (CAT): 0.0548\r\n",
      "   4. OnlineSecurity       (CAT): 0.0541\r\n",
      "   5. PaymentMethod        (CAT): 0.0537\r\n",
      "   6. PaperlessBilling     (CAT): 0.0533\r\n",
      "   7. StreamingTV          (CAT): 0.0531\r\n",
      "   8. TotalCharges         (NUM): 0.0527\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['DeviceProtection', 'Partner', 'Contract', 'OnlineSecurity', 'PaymentMethod', 'PaperlessBilling', 'StreamingTV'] → indices [9, 2, 13, 7, 15, 14, 11]\r\n",
      "📊 Features sélectionnées: 1 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 135,361 paramètres\r\n",
      "🔗 Sparsité d'attention: 65.43%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5935 | Val Loss: 0.5923 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5923)\r\n",
      "Epoch 001 | Train Loss: 0.5790 | Val Loss: 0.5898 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5898)\r\n",
      "Epoch 002 | Train Loss: 0.5793 | Val Loss: 0.5900 | Time: 3.28s\r\n",
      "Epoch 003 | Train Loss: 0.5793 | Val Loss: 0.5902 | Time: 3.25s\r\n",
      "Epoch 004 | Train Loss: 0.5793 | Val Loss: 0.5903 | Time: 3.32s\r\n",
      "Epoch 005 | Train Loss: 0.5793 | Val Loss: 0.5904 | Time: 3.29s\r\n",
      "Epoch 006 | Train Loss: 0.5793 | Val Loss: 0.5904 | Time: 3.31s\r\n",
      "Epoch 007 | Train Loss: 0.5793 | Val Loss: 0.5905 | Time: 3.24s\r\n",
      "Epoch 008 | Train Loss: 0.5793 | Val Loss: 0.5905 | Time: 3.27s\r\n",
      "Epoch 009 | Train Loss: 0.5793 | Val Loss: 0.5905 | Time: 3.22s\r\n",
      "Epoch 010 | Train Loss: 0.5793 | Val Loss: 0.5905 | Time: 3.25s\r\n",
      "Epoch 011 | Train Loss: 0.5793 | Val Loss: 0.5905 | Time: 3.32s\r\n",
      "Epoch 012 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.22s\r\n",
      "Epoch 013 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.22s\r\n",
      "Epoch 014 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.28s\r\n",
      "Epoch 015 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.21s\r\n",
      "Epoch 016 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.38s\r\n",
      "Epoch 017 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.29s\r\n",
      "Epoch 018 | Train Loss: 0.5793 | Val Loss: 0.5906 | Time: 3.25s\r\n",
      "\r\n",
      "Early stopping à l'époque 18 (patience: 17)\r\n",
      "✅ Meilleur modèle Random chargé (époque 1, val_loss: 0.5898)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineSecurity       (CAT): 0.1375\r\n",
      "   2. TotalCharges         (NUM): 0.1263\r\n",
      "   3. DeviceProtection     (CAT): 0.1254\r\n",
      "   4. Contract             (CAT): 0.1253\r\n",
      "   5. PaymentMethod        (CAT): 0.1246\r\n",
      "   6. StreamingTV          (CAT): 0.1239\r\n",
      "   7. Partner              (CAT): 0.1188\r\n",
      "   8. PaperlessBilling     (CAT): 0.1182\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.1375\r\n",
      "   2. TotalCharges        : 0.1263\r\n",
      "   3. DeviceProtection    : 0.1254\r\n",
      "   4. Contract            : 0.1253\r\n",
      "   5. PaymentMethod       : 0.1246\r\n",
      "   6. StreamingTV         : 0.1239\r\n",
      "   7. Partner             : 0.1188\r\n",
      "   8. PaperlessBilling    : 0.1182\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 395.0s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR-LR\r\n",
      "Modèle FTT+ créé avec 125,265 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5631 | Val Loss: 0.4938 | Time: 6.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4938)\r\n",
      "Epoch 001 | Train Loss: 0.4780 | Val Loss: 0.4512 | Time: 6.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4512)\r\n",
      "Epoch 002 | Train Loss: 0.4555 | Val Loss: 0.4435 | Time: 6.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4435)\r\n",
      "Epoch 003 | Train Loss: 0.4436 | Val Loss: 0.4343 | Time: 6.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4343)\r\n",
      "Epoch 004 | Train Loss: 0.4377 | Val Loss: 0.4304 | Time: 6.57s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4304)\r\n",
      "Epoch 005 | Train Loss: 0.4322 | Val Loss: 0.4261 | Time: 6.57s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4261)\r\n",
      "Epoch 006 | Train Loss: 0.4308 | Val Loss: 0.4224 | Time: 6.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4224)\r\n",
      "Epoch 007 | Train Loss: 0.4301 | Val Loss: 0.4216 | Time: 6.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 008 | Train Loss: 0.4258 | Val Loss: 0.4197 | Time: 6.57s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 009 | Train Loss: 0.4227 | Val Loss: 0.4181 | Time: 6.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4181)\r\n",
      "Epoch 010 | Train Loss: 0.4178 | Val Loss: 0.4188 | Time: 6.51s\r\n",
      "Epoch 011 | Train Loss: 0.4210 | Val Loss: 0.4172 | Time: 6.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4172)\r\n",
      "Epoch 012 | Train Loss: 0.4166 | Val Loss: 0.4157 | Time: 6.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4157)\r\n",
      "Epoch 013 | Train Loss: 0.4162 | Val Loss: 0.4152 | Time: 6.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4152)\r\n",
      "Epoch 014 | Train Loss: 0.4153 | Val Loss: 0.4168 | Time: 6.61s\r\n",
      "Epoch 015 | Train Loss: 0.4133 | Val Loss: 0.4142 | Time: 6.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4142)\r\n",
      "Epoch 016 | Train Loss: 0.4124 | Val Loss: 0.4135 | Time: 6.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4135)\r\n",
      "Epoch 017 | Train Loss: 0.4119 | Val Loss: 0.4144 | Time: 6.68s\r\n",
      "Epoch 018 | Train Loss: 0.4098 | Val Loss: 0.4147 | Time: 6.61s\r\n",
      "Epoch 019 | Train Loss: 0.4096 | Val Loss: 0.4148 | Time: 6.64s\r\n",
      "Epoch 020 | Train Loss: 0.4098 | Val Loss: 0.4155 | Time: 6.60s\r\n",
      "Epoch 021 | Train Loss: 0.4100 | Val Loss: 0.4151 | Time: 6.57s\r\n",
      "Epoch 022 | Train Loss: 0.4080 | Val Loss: 0.4146 | Time: 6.62s\r\n",
      "Epoch 023 | Train Loss: 0.4064 | Val Loss: 0.4148 | Time: 6.56s\r\n",
      "Epoch 024 | Train Loss: 0.4063 | Val Loss: 0.4151 | Time: 6.56s\r\n",
      "Epoch 025 | Train Loss: 0.4087 | Val Loss: 0.4156 | Time: 6.57s\r\n",
      "Epoch 026 | Train Loss: 0.4062 | Val Loss: 0.4151 | Time: 6.56s\r\n",
      "Epoch 027 | Train Loss: 0.4047 | Val Loss: 0.4158 | Time: 6.63s\r\n",
      "Epoch 028 | Train Loss: 0.4065 | Val Loss: 0.4174 | Time: 6.56s\r\n",
      "Epoch 029 | Train Loss: 0.4050 | Val Loss: 0.4168 | Time: 6.70s\r\n",
      "Epoch 030 | Train Loss: 0.4034 | Val Loss: 0.4172 | Time: 6.61s\r\n",
      "Epoch 031 | Train Loss: 0.4043 | Val Loss: 0.4201 | Time: 6.71s\r\n",
      "Epoch 032 | Train Loss: 0.4026 | Val Loss: 0.4177 | Time: 6.63s\r\n",
      "Epoch 033 | Train Loss: 0.4059 | Val Loss: 0.4166 | Time: 6.59s\r\n",
      "\r\n",
      "Early stopping à l'époque 33 (patience: 17)\r\n",
      "✅ Meilleur modèle chargé (époque 16, val_loss: 0.4135)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0548\r\n",
      "   2. MultipleLines       : 0.0542\r\n",
      "   3. TotalCharges        : 0.0537\r\n",
      "   4. gender              : 0.0536\r\n",
      "   5. PhoneService        : 0.0535\r\n",
      "   6. PaperlessBilling    : 0.0534\r\n",
      "   7. OnlineBackup        : 0.0531\r\n",
      "   8. Contract            : 0.0529\r\n",
      "   9. Dependents          : 0.0529\r\n",
      "  10. MonthlyCharges      : 0.0528\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 8 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0548\r\n",
      "   2. MultipleLines        (CAT): 0.0542\r\n",
      "   3. TotalCharges         (NUM): 0.0537\r\n",
      "   4. gender               (CAT): 0.0536\r\n",
      "   5. PhoneService         (CAT): 0.0535\r\n",
      "   6. PaperlessBilling     (CAT): 0.0534\r\n",
      "   7. OnlineBackup         (CAT): 0.0531\r\n",
      "   8. Contract             (CAT): 0.0529\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'MultipleLines', 'gender', 'PhoneService', 'PaperlessBilling', 'OnlineBackup', 'Contract'] → indices [15, 5, 0, 4, 14, 8, 13]\r\n",
      "📊 Features sélectionnées: 1 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 135,297 paramètres\r\n",
      "🔗 Sparsité d'attention: 65.43%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5967 | Val Loss: 0.5792 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5792)\r\n",
      "Epoch 001 | Train Loss: 0.5799 | Val Loss: 0.5827 | Time: 3.21s\r\n",
      "Epoch 002 | Train Loss: 0.5803 | Val Loss: 0.5830 | Time: 3.27s\r\n",
      "Epoch 003 | Train Loss: 0.5803 | Val Loss: 0.5831 | Time: 3.26s\r\n",
      "Epoch 004 | Train Loss: 0.5804 | Val Loss: 0.5832 | Time: 3.27s\r\n",
      "Epoch 005 | Train Loss: 0.5804 | Val Loss: 0.5833 | Time: 3.48s\r\n",
      "Epoch 006 | Train Loss: 0.5804 | Val Loss: 0.5833 | Time: 3.25s\r\n",
      "Epoch 007 | Train Loss: 0.5804 | Val Loss: 0.5833 | Time: 3.20s\r\n",
      "Epoch 008 | Train Loss: 0.5804 | Val Loss: 0.5833 | Time: 3.26s\r\n",
      "Epoch 009 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.24s\r\n",
      "Epoch 010 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.26s\r\n",
      "Epoch 011 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.29s\r\n",
      "Epoch 012 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.27s\r\n",
      "Epoch 013 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.28s\r\n",
      "Epoch 014 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.37s\r\n",
      "Epoch 015 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.37s\r\n",
      "Epoch 016 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.25s\r\n",
      "Epoch 017 | Train Loss: 0.5804 | Val Loss: 0.5834 | Time: 3.26s\r\n",
      "\r\n",
      "Early stopping à l'époque 17 (patience: 17)\r\n",
      "✅ Meilleur modèle Random chargé (époque 0, val_loss: 0.5792)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PhoneService         (CAT): 0.1535\r\n",
      "   2. MultipleLines        (CAT): 0.1483\r\n",
      "   3. Contract             (CAT): 0.1430\r\n",
      "   4. gender               (CAT): 0.1389\r\n",
      "   5. TotalCharges         (NUM): 0.1238\r\n",
      "   6. PaymentMethod        (CAT): 0.1122\r\n",
      "   7. PaperlessBilling     (CAT): 0.0970\r\n",
      "   8. OnlineBackup         (CAT): 0.0834\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.1535\r\n",
      "   2. MultipleLines       : 0.1483\r\n",
      "   3. Contract            : 0.1430\r\n",
      "   4. gender              : 0.1389\r\n",
      "   5. TotalCharges        : 0.1238\r\n",
      "   6. PaymentMethod       : 0.1122\r\n",
      "   7. PaperlessBilling    : 0.0970\r\n",
      "   8. OnlineBackup        : 0.0834\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 286.3s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: P-LR-LR\r\n",
      "Modèle FTT+ créé avec 125,265 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5560 | Val Loss: 0.4854 | Time: 6.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4854)\r\n",
      "Epoch 001 | Train Loss: 0.4802 | Val Loss: 0.4540 | Time: 6.56s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4540)\r\n",
      "Epoch 002 | Train Loss: 0.4599 | Val Loss: 0.4442 | Time: 6.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4442)\r\n",
      "Epoch 003 | Train Loss: 0.4532 | Val Loss: 0.4412 | Time: 6.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4412)\r\n",
      "Epoch 004 | Train Loss: 0.4457 | Val Loss: 0.4361 | Time: 6.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4361)\r\n",
      "Epoch 005 | Train Loss: 0.4388 | Val Loss: 0.4286 | Time: 6.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4286)\r\n",
      "Epoch 006 | Train Loss: 0.4366 | Val Loss: 0.4255 | Time: 6.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 007 | Train Loss: 0.4339 | Val Loss: 0.4355 | Time: 6.84s\r\n",
      "Epoch 008 | Train Loss: 0.4329 | Val Loss: 0.4315 | Time: 6.64s\r\n",
      "Epoch 009 | Train Loss: 0.4322 | Val Loss: 0.4277 | Time: 6.58s\r\n",
      "Epoch 010 | Train Loss: 0.4329 | Val Loss: 0.4267 | Time: 6.76s\r\n",
      "Epoch 011 | Train Loss: 0.4306 | Val Loss: 0.4265 | Time: 6.59s\r\n",
      "Epoch 012 | Train Loss: 0.4283 | Val Loss: 0.4206 | Time: 6.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4206)\r\n",
      "Epoch 013 | Train Loss: 0.4282 | Val Loss: 0.4258 | Time: 6.61s\r\n",
      "Epoch 014 | Train Loss: 0.4226 | Val Loss: 0.4302 | Time: 6.58s\r\n",
      "Epoch 015 | Train Loss: 0.4243 | Val Loss: 0.4215 | Time: 6.67s\r\n",
      "Epoch 016 | Train Loss: 0.4203 | Val Loss: 0.4247 | Time: 6.57s\r\n",
      "Epoch 017 | Train Loss: 0.4207 | Val Loss: 0.4265 | Time: 6.71s\r\n",
      "Epoch 018 | Train Loss: 0.4210 | Val Loss: 0.4232 | Time: 6.57s\r\n",
      "Epoch 019 | Train Loss: 0.4235 | Val Loss: 0.4228 | Time: 6.59s\r\n",
      "Epoch 020 | Train Loss: 0.4182 | Val Loss: 0.4254 | Time: 6.56s\r\n",
      "Epoch 021 | Train Loss: 0.4176 | Val Loss: 0.4273 | Time: 6.63s\r\n",
      "Epoch 022 | Train Loss: 0.4146 | Val Loss: 0.4240 | Time: 6.75s\r\n",
      "Epoch 023 | Train Loss: 0.4197 | Val Loss: 0.4263 | Time: 6.62s\r\n",
      "Epoch 024 | Train Loss: 0.4172 | Val Loss: 0.4279 | Time: 6.53s\r\n",
      "Epoch 025 | Train Loss: 0.4163 | Val Loss: 0.4266 | Time: 6.56s\r\n",
      "Epoch 026 | Train Loss: 0.4141 | Val Loss: 0.4265 | Time: 6.70s\r\n",
      "Epoch 027 | Train Loss: 0.4136 | Val Loss: 0.4291 | Time: 6.57s\r\n",
      "Epoch 028 | Train Loss: 0.4133 | Val Loss: 0.4272 | Time: 6.61s\r\n",
      "Epoch 029 | Train Loss: 0.4115 | Val Loss: 0.4271 | Time: 6.57s\r\n",
      "\r\n",
      "Early stopping à l'époque 29 (patience: 17)\r\n",
      "✅ Meilleur modèle chargé (époque 12, val_loss: 0.4206)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MultipleLines       : 0.0579\r\n",
      "   2. PaperlessBilling    : 0.0555\r\n",
      "   3. PaymentMethod       : 0.0548\r\n",
      "   4. OnlineSecurity      : 0.0545\r\n",
      "   5. InternetService     : 0.0544\r\n",
      "   6. DeviceProtection    : 0.0534\r\n",
      "   7. gender              : 0.0532\r\n",
      "   8. TotalCharges        : 0.0528\r\n",
      "   9. MonthlyCharges      : 0.0525\r\n",
      "  10. tenure              : 0.0524\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 8 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MultipleLines        (CAT): 0.0579\r\n",
      "   2. PaperlessBilling     (CAT): 0.0555\r\n",
      "   3. PaymentMethod        (CAT): 0.0548\r\n",
      "   4. OnlineSecurity       (CAT): 0.0545\r\n",
      "   5. InternetService      (CAT): 0.0544\r\n",
      "   6. DeviceProtection     (CAT): 0.0534\r\n",
      "   7. gender               (CAT): 0.0532\r\n",
      "   8. TotalCharges         (NUM): 0.0528\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['MultipleLines', 'PaperlessBilling', 'PaymentMethod', 'OnlineSecurity', 'InternetService', 'DeviceProtection', 'gender'] → indices [5, 14, 15, 7, 6, 9, 0]\r\n",
      "📊 Features sélectionnées: 1 numériques, 7 catégorielles\r\n",
      "🎲 Interactions aléatoires: 6 paires\r\n",
      "Modèle Random créé avec 135,361 paramètres\r\n",
      "🔗 Sparsité d'attention: 65.43%\r\n",
      "   - Connexions feature-feature: 12\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5989 | Val Loss: 0.5977 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5977)\r\n",
      "Epoch 001 | Train Loss: 0.5822 | Val Loss: 0.5816 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5816)\r\n",
      "Epoch 002 | Train Loss: 0.5813 | Val Loss: 0.5822 | Time: 3.31s\r\n",
      "Epoch 003 | Train Loss: 0.5815 | Val Loss: 0.5828 | Time: 3.24s\r\n",
      "Epoch 004 | Train Loss: 0.5816 | Val Loss: 0.5828 | Time: 3.25s\r\n",
      "Epoch 005 | Train Loss: 0.5816 | Val Loss: 0.5829 | Time: 3.28s\r\n",
      "Epoch 006 | Train Loss: 0.5816 | Val Loss: 0.5830 | Time: 3.24s\r\n",
      "Epoch 007 | Train Loss: 0.5816 | Val Loss: 0.5831 | Time: 3.26s\r\n",
      "Epoch 008 | Train Loss: 0.5817 | Val Loss: 0.5832 | Time: 3.29s\r\n",
      "Epoch 009 | Train Loss: 0.5817 | Val Loss: 0.5832 | Time: 3.33s\r\n",
      "Epoch 010 | Train Loss: 0.5817 | Val Loss: 0.5832 | Time: 3.26s\r\n",
      "Epoch 011 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.30s\r\n",
      "Epoch 012 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.34s\r\n",
      "Epoch 013 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.26s\r\n",
      "Epoch 014 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.28s\r\n",
      "Epoch 015 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.27s\r\n",
      "Epoch 016 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.27s\r\n",
      "Epoch 017 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.26s\r\n",
      "Epoch 018 | Train Loss: 0.5817 | Val Loss: 0.5833 | Time: 3.26s\r\n",
      "\r\n",
      "Early stopping à l'époque 18 (patience: 17)\r\n",
      "✅ Meilleur modèle Random chargé (époque 1, val_loss: 0.5816)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. PaymentMethod        (CAT): 0.1549\r\n",
      "   2. TotalCharges         (NUM): 0.1529\r\n",
      "   3. PaperlessBilling     (CAT): 0.1415\r\n",
      "   4. MultipleLines        (CAT): 0.1406\r\n",
      "   5. DeviceProtection     (CAT): 0.1381\r\n",
      "   6. OnlineSecurity       (CAT): 0.1212\r\n",
      "   7. InternetService      (CAT): 0.0793\r\n",
      "   8. gender               (CAT): 0.0716\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.1549\r\n",
      "   2. TotalCharges        : 0.1529\r\n",
      "   3. PaperlessBilling    : 0.1415\r\n",
      "   4. MultipleLines       : 0.1406\r\n",
      "   5. DeviceProtection    : 0.1381\r\n",
      "   6. OnlineSecurity      : 0.1212\r\n",
      "   7. InternetService     : 0.0793\r\n",
      "   8. gender              : 0.0716\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_20/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 264.1s ===\r\n",
      "\u001b[32m[I 2025-07-20 00:48:43,453]\u001b[0m Trial 20 finished with value: 0.0 and parameters: {'d_token_stage1': 32, 'n_blocks_stage1': 4, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.25070943631911097, 'ffn_dropout_stage1': 0.15908472259748507, 'residual_dropout_stage1': 0.16531866470360995, 'lr_stage1': 0.00010949828210884793, 'weight_decay_stage1': 6.334790824251028e-06, 'd_token_stage2': 64, 'n_blocks_stage2': 2, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.18847292212321876, 'ffn_dropout_stage2': 0.20177483484349104, 'residual_dropout_stage2': 0.1718625425306742, 'lr_stage2': 0.08360603301436347, 'weight_decay_stage2': 0.0008273679056137971, 'batch_size': 32, 'patience': 17, 'embedding_type': 'P-LR-LR', 'M': 8, 'k': 6}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  84%|█████▉ | 21/25 [6:35:46<1:07:02, 1005.53s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5821 | Val Loss: 0.5463 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5463)\r\n",
      "Epoch 001 | Train Loss: 0.5407 | Val Loss: 0.5091 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5091)\r\n",
      "Epoch 002 | Train Loss: 0.5093 | Val Loss: 0.4782 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4782)\r\n",
      "Epoch 003 | Train Loss: 0.4845 | Val Loss: 0.4590 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4590)\r\n",
      "Epoch 004 | Train Loss: 0.4743 | Val Loss: 0.4500 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4500)\r\n",
      "Epoch 005 | Train Loss: 0.4617 | Val Loss: 0.4447 | Time: 5.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4447)\r\n",
      "Epoch 006 | Train Loss: 0.4554 | Val Loss: 0.4405 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4405)\r\n",
      "Epoch 007 | Train Loss: 0.4560 | Val Loss: 0.4378 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4378)\r\n",
      "Epoch 008 | Train Loss: 0.4521 | Val Loss: 0.4344 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4344)\r\n",
      "Epoch 009 | Train Loss: 0.4479 | Val Loss: 0.4330 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4330)\r\n",
      "Epoch 010 | Train Loss: 0.4454 | Val Loss: 0.4307 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 011 | Train Loss: 0.4427 | Val Loss: 0.4293 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 012 | Train Loss: 0.4408 | Val Loss: 0.4270 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4270)\r\n",
      "Epoch 013 | Train Loss: 0.4425 | Val Loss: 0.4268 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4268)\r\n",
      "Epoch 014 | Train Loss: 0.4408 | Val Loss: 0.4260 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4260)\r\n",
      "Epoch 015 | Train Loss: 0.4369 | Val Loss: 0.4241 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 016 | Train Loss: 0.4361 | Val Loss: 0.4230 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4230)\r\n",
      "Epoch 017 | Train Loss: 0.4351 | Val Loss: 0.4227 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 018 | Train Loss: 0.4312 | Val Loss: 0.4215 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4215)\r\n",
      "Epoch 019 | Train Loss: 0.4339 | Val Loss: 0.4208 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4208)\r\n",
      "Epoch 020 | Train Loss: 0.4298 | Val Loss: 0.4202 | Time: 5.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4202)\r\n",
      "Epoch 021 | Train Loss: 0.4305 | Val Loss: 0.4203 | Time: 5.75s\r\n",
      "Epoch 022 | Train Loss: 0.4303 | Val Loss: 0.4201 | Time: 5.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4201)\r\n",
      "Epoch 023 | Train Loss: 0.4277 | Val Loss: 0.4190 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 024 | Train Loss: 0.4314 | Val Loss: 0.4184 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 025 | Train Loss: 0.4289 | Val Loss: 0.4170 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 026 | Train Loss: 0.4255 | Val Loss: 0.4175 | Time: 5.79s\r\n",
      "Epoch 027 | Train Loss: 0.4254 | Val Loss: 0.4181 | Time: 5.63s\r\n",
      "Epoch 028 | Train Loss: 0.4247 | Val Loss: 0.4171 | Time: 5.69s\r\n",
      "Epoch 029 | Train Loss: 0.4234 | Val Loss: 0.4165 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4165)\r\n",
      "Epoch 030 | Train Loss: 0.4248 | Val Loss: 0.4165 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4165)\r\n",
      "Epoch 031 | Train Loss: 0.4227 | Val Loss: 0.4156 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4156)\r\n",
      "Epoch 032 | Train Loss: 0.4228 | Val Loss: 0.4158 | Time: 5.68s\r\n",
      "Epoch 033 | Train Loss: 0.4232 | Val Loss: 0.4152 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4152)\r\n",
      "Epoch 034 | Train Loss: 0.4226 | Val Loss: 0.4153 | Time: 5.78s\r\n",
      "Epoch 035 | Train Loss: 0.4229 | Val Loss: 0.4150 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4150)\r\n",
      "Epoch 036 | Train Loss: 0.4206 | Val Loss: 0.4145 | Time: 5.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4145)\r\n",
      "Epoch 037 | Train Loss: 0.4213 | Val Loss: 0.4134 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4134)\r\n",
      "Epoch 038 | Train Loss: 0.4205 | Val Loss: 0.4140 | Time: 5.73s\r\n",
      "Epoch 039 | Train Loss: 0.4207 | Val Loss: 0.4142 | Time: 5.70s\r\n",
      "Epoch 040 | Train Loss: 0.4204 | Val Loss: 0.4145 | Time: 5.80s\r\n",
      "Epoch 041 | Train Loss: 0.4201 | Val Loss: 0.4132 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4132)\r\n",
      "Epoch 042 | Train Loss: 0.4229 | Val Loss: 0.4119 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4119)\r\n",
      "Epoch 043 | Train Loss: 0.4221 | Val Loss: 0.4120 | Time: 5.70s\r\n",
      "Epoch 044 | Train Loss: 0.4199 | Val Loss: 0.4120 | Time: 5.63s\r\n",
      "Epoch 045 | Train Loss: 0.4198 | Val Loss: 0.4108 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4108)\r\n",
      "Epoch 046 | Train Loss: 0.4218 | Val Loss: 0.4120 | Time: 5.68s\r\n",
      "Epoch 047 | Train Loss: 0.4171 | Val Loss: 0.4113 | Time: 5.86s\r\n",
      "Epoch 048 | Train Loss: 0.4169 | Val Loss: 0.4110 | Time: 5.70s\r\n",
      "Epoch 049 | Train Loss: 0.4185 | Val Loss: 0.4107 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4107)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4107)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. tenure              : 0.0678\r\n",
      "   2. StreamingTV         : 0.0677\r\n",
      "   3. DeviceProtection    : 0.0649\r\n",
      "   4. OnlineSecurity      : 0.0632\r\n",
      "   5. gender              : 0.0596\r\n",
      "   6. PaperlessBilling    : 0.0593\r\n",
      "   7. StreamingMovies     : 0.0555\r\n",
      "   8. Dependents          : 0.0527\r\n",
      "   9. PhoneService        : 0.0526\r\n",
      "  10. TechSupport         : 0.0517\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. tenure               (NUM): 0.0678\r\n",
      "   2. StreamingTV          (CAT): 0.0677\r\n",
      "   3. DeviceProtection     (CAT): 0.0649\r\n",
      "   4. OnlineSecurity       (CAT): 0.0632\r\n",
      "   5. gender               (CAT): 0.0596\r\n",
      "   6. PaperlessBilling     (CAT): 0.0593\r\n",
      "   7. StreamingMovies      (CAT): 0.0555\r\n",
      "   8. Dependents           (CAT): 0.0527\r\n",
      "   9. PhoneService         (CAT): 0.0526\r\n",
      "  10. TechSupport          (CAT): 0.0517\r\n",
      "  11. InternetService      (CAT): 0.0515\r\n",
      "  12. MonthlyCharges       (NUM): 0.0506\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'MonthlyCharges'] → indices [0, 1]\r\n",
      "   - Catégorielles sélectionnées: ['StreamingTV', 'DeviceProtection', 'OnlineSecurity', 'gender', 'PaperlessBilling', 'StreamingMovies', 'Dependents', 'PhoneService', 'TechSupport', 'InternetService'] → indices [11, 9, 7, 0, 14, 12, 3, 4, 10, 6]\r\n",
      "📊 Features sélectionnées: 2 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 61,569 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.07%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6762 | Val Loss: 0.5982 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5982)\r\n",
      "Epoch 001 | Train Loss: 0.6237 | Val Loss: 0.5543 | Time: 3.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5543)\r\n",
      "Epoch 002 | Train Loss: 0.5915 | Val Loss: 0.5322 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5322)\r\n",
      "Epoch 003 | Train Loss: 0.5675 | Val Loss: 0.5171 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5171)\r\n",
      "Epoch 004 | Train Loss: 0.5506 | Val Loss: 0.5037 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5037)\r\n",
      "Epoch 005 | Train Loss: 0.5396 | Val Loss: 0.4889 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4889)\r\n",
      "Epoch 006 | Train Loss: 0.5300 | Val Loss: 0.4781 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4781)\r\n",
      "Epoch 007 | Train Loss: 0.5198 | Val Loss: 0.4704 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4704)\r\n",
      "Epoch 008 | Train Loss: 0.5153 | Val Loss: 0.4657 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4657)\r\n",
      "Epoch 009 | Train Loss: 0.5031 | Val Loss: 0.4632 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4632)\r\n",
      "Epoch 010 | Train Loss: 0.5057 | Val Loss: 0.4618 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4618)\r\n",
      "Epoch 011 | Train Loss: 0.5010 | Val Loss: 0.4610 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4610)\r\n",
      "Epoch 012 | Train Loss: 0.4982 | Val Loss: 0.4603 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4603)\r\n",
      "Epoch 013 | Train Loss: 0.4997 | Val Loss: 0.4599 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4599)\r\n",
      "Epoch 014 | Train Loss: 0.4936 | Val Loss: 0.4591 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4591)\r\n",
      "Epoch 015 | Train Loss: 0.4928 | Val Loss: 0.4591 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4591)\r\n",
      "Epoch 016 | Train Loss: 0.4907 | Val Loss: 0.4587 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4587)\r\n",
      "Epoch 017 | Train Loss: 0.4890 | Val Loss: 0.4586 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4586)\r\n",
      "Epoch 018 | Train Loss: 0.4884 | Val Loss: 0.4578 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4578)\r\n",
      "Epoch 019 | Train Loss: 0.4865 | Val Loss: 0.4575 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4575)\r\n",
      "Epoch 020 | Train Loss: 0.4861 | Val Loss: 0.4571 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4571)\r\n",
      "Epoch 021 | Train Loss: 0.4848 | Val Loss: 0.4565 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4565)\r\n",
      "Epoch 022 | Train Loss: 0.4838 | Val Loss: 0.4554 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4554)\r\n",
      "Epoch 023 | Train Loss: 0.4819 | Val Loss: 0.4551 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4551)\r\n",
      "Epoch 024 | Train Loss: 0.4852 | Val Loss: 0.4542 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4542)\r\n",
      "Epoch 025 | Train Loss: 0.4836 | Val Loss: 0.4530 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4530)\r\n",
      "Epoch 026 | Train Loss: 0.4822 | Val Loss: 0.4526 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4526)\r\n",
      "Epoch 027 | Train Loss: 0.4867 | Val Loss: 0.4520 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4520)\r\n",
      "Epoch 028 | Train Loss: 0.4872 | Val Loss: 0.4510 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4510)\r\n",
      "Epoch 029 | Train Loss: 0.4797 | Val Loss: 0.4499 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4499)\r\n",
      "Epoch 030 | Train Loss: 0.4783 | Val Loss: 0.4493 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4493)\r\n",
      "Epoch 031 | Train Loss: 0.4759 | Val Loss: 0.4487 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4487)\r\n",
      "Epoch 032 | Train Loss: 0.4751 | Val Loss: 0.4483 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4483)\r\n",
      "Epoch 033 | Train Loss: 0.4714 | Val Loss: 0.4472 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4472)\r\n",
      "Epoch 034 | Train Loss: 0.4744 | Val Loss: 0.4466 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4466)\r\n",
      "Epoch 035 | Train Loss: 0.4740 | Val Loss: 0.4466 | Time: 3.26s\r\n",
      "Epoch 036 | Train Loss: 0.4716 | Val Loss: 0.4466 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4466)\r\n",
      "Epoch 037 | Train Loss: 0.4742 | Val Loss: 0.4459 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4459)\r\n",
      "Epoch 038 | Train Loss: 0.4730 | Val Loss: 0.4444 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4444)\r\n",
      "Epoch 039 | Train Loss: 0.4721 | Val Loss: 0.4443 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4443)\r\n",
      "Epoch 040 | Train Loss: 0.4774 | Val Loss: 0.4432 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4432)\r\n",
      "Epoch 041 | Train Loss: 0.4768 | Val Loss: 0.4425 | Time: 3.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 042 | Train Loss: 0.4701 | Val Loss: 0.4425 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 043 | Train Loss: 0.4716 | Val Loss: 0.4419 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "Epoch 044 | Train Loss: 0.4714 | Val Loss: 0.4414 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4414)\r\n",
      "Epoch 045 | Train Loss: 0.4715 | Val Loss: 0.4410 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4410)\r\n",
      "Epoch 046 | Train Loss: 0.4731 | Val Loss: 0.4402 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4402)\r\n",
      "Epoch 047 | Train Loss: 0.4713 | Val Loss: 0.4399 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4399)\r\n",
      "Epoch 048 | Train Loss: 0.4740 | Val Loss: 0.4395 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4395)\r\n",
      "Epoch 049 | Train Loss: 0.4696 | Val Loss: 0.4383 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4383)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4383)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. MonthlyCharges       (NUM): 0.1512\r\n",
      "   2. StreamingMovies      (CAT): 0.1489\r\n",
      "   3. tenure               (NUM): 0.1284\r\n",
      "   4. PhoneService         (CAT): 0.1205\r\n",
      "   5. TechSupport          (CAT): 0.0685\r\n",
      "   6. DeviceProtection     (CAT): 0.0563\r\n",
      "   7. PaperlessBilling     (CAT): 0.0551\r\n",
      "   8. OnlineSecurity       (CAT): 0.0545\r\n",
      "   9. gender               (CAT): 0.0545\r\n",
      "  10. InternetService      (CAT): 0.0544\r\n",
      "  11. Dependents           (CAT): 0.0539\r\n",
      "  12. StreamingTV          (CAT): 0.0537\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.1512\r\n",
      "   2. StreamingMovies     : 0.1489\r\n",
      "   3. tenure              : 0.1284\r\n",
      "   4. PhoneService        : 0.1205\r\n",
      "   5. TechSupport         : 0.0685\r\n",
      "   6. DeviceProtection    : 0.0563\r\n",
      "   7. PaperlessBilling    : 0.0551\r\n",
      "   8. OnlineSecurity      : 0.0545\r\n",
      "   9. gender              : 0.0545\r\n",
      "  10. InternetService     : 0.0544\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 452.5s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6470 | Val Loss: 0.5735 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5735)\r\n",
      "Epoch 001 | Train Loss: 0.5742 | Val Loss: 0.5493 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5493)\r\n",
      "Epoch 002 | Train Loss: 0.5523 | Val Loss: 0.5263 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5263)\r\n",
      "Epoch 003 | Train Loss: 0.5299 | Val Loss: 0.5028 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5028)\r\n",
      "Epoch 004 | Train Loss: 0.5107 | Val Loss: 0.4907 | Time: 5.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4907)\r\n",
      "Epoch 005 | Train Loss: 0.4895 | Val Loss: 0.4823 | Time: 5.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4823)\r\n",
      "Epoch 006 | Train Loss: 0.4786 | Val Loss: 0.4749 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4749)\r\n",
      "Epoch 007 | Train Loss: 0.4741 | Val Loss: 0.4700 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4700)\r\n",
      "Epoch 008 | Train Loss: 0.4666 | Val Loss: 0.4676 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4676)\r\n",
      "Epoch 009 | Train Loss: 0.4641 | Val Loss: 0.4594 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4594)\r\n",
      "Epoch 010 | Train Loss: 0.4562 | Val Loss: 0.4566 | Time: 5.90s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4566)\r\n",
      "Epoch 011 | Train Loss: 0.4525 | Val Loss: 0.4545 | Time: 5.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4545)\r\n",
      "Epoch 012 | Train Loss: 0.4502 | Val Loss: 0.4498 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4498)\r\n",
      "Epoch 013 | Train Loss: 0.4471 | Val Loss: 0.4485 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4485)\r\n",
      "Epoch 014 | Train Loss: 0.4440 | Val Loss: 0.4485 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4485)\r\n",
      "Epoch 015 | Train Loss: 0.4427 | Val Loss: 0.4447 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4447)\r\n",
      "Epoch 016 | Train Loss: 0.4394 | Val Loss: 0.4421 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4421)\r\n",
      "Epoch 017 | Train Loss: 0.4362 | Val Loss: 0.4404 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4404)\r\n",
      "Epoch 018 | Train Loss: 0.4348 | Val Loss: 0.4385 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 019 | Train Loss: 0.4358 | Val Loss: 0.4357 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4357)\r\n",
      "Epoch 020 | Train Loss: 0.4313 | Val Loss: 0.4345 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 021 | Train Loss: 0.4321 | Val Loss: 0.4339 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4339)\r\n",
      "Epoch 022 | Train Loss: 0.4301 | Val Loss: 0.4324 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4324)\r\n",
      "Epoch 023 | Train Loss: 0.4268 | Val Loss: 0.4317 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 024 | Train Loss: 0.4259 | Val Loss: 0.4300 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4300)\r\n",
      "Epoch 025 | Train Loss: 0.4260 | Val Loss: 0.4292 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4292)\r\n",
      "Epoch 026 | Train Loss: 0.4255 | Val Loss: 0.4284 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4284)\r\n",
      "Epoch 027 | Train Loss: 0.4218 | Val Loss: 0.4274 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4274)\r\n",
      "Epoch 028 | Train Loss: 0.4239 | Val Loss: 0.4265 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 029 | Train Loss: 0.4234 | Val Loss: 0.4246 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4246)\r\n",
      "Epoch 030 | Train Loss: 0.4214 | Val Loss: 0.4253 | Time: 5.70s\r\n",
      "Epoch 031 | Train Loss: 0.4216 | Val Loss: 0.4235 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 032 | Train Loss: 0.4233 | Val Loss: 0.4236 | Time: 5.81s\r\n",
      "Epoch 033 | Train Loss: 0.4202 | Val Loss: 0.4236 | Time: 5.79s\r\n",
      "Epoch 034 | Train Loss: 0.4188 | Val Loss: 0.4221 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4221)\r\n",
      "Epoch 035 | Train Loss: 0.4166 | Val Loss: 0.4217 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4217)\r\n",
      "Epoch 036 | Train Loss: 0.4155 | Val Loss: 0.4207 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4207)\r\n",
      "Epoch 037 | Train Loss: 0.4167 | Val Loss: 0.4199 | Time: 5.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4199)\r\n",
      "Epoch 038 | Train Loss: 0.4159 | Val Loss: 0.4203 | Time: 5.76s\r\n",
      "Epoch 039 | Train Loss: 0.4130 | Val Loss: 0.4214 | Time: 5.67s\r\n",
      "Epoch 040 | Train Loss: 0.4149 | Val Loss: 0.4206 | Time: 5.72s\r\n",
      "Epoch 041 | Train Loss: 0.4152 | Val Loss: 0.4194 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 042 | Train Loss: 0.4154 | Val Loss: 0.4190 | Time: 5.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 043 | Train Loss: 0.4143 | Val Loss: 0.4184 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 044 | Train Loss: 0.4133 | Val Loss: 0.4181 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4181)\r\n",
      "Epoch 045 | Train Loss: 0.4135 | Val Loss: 0.4178 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 046 | Train Loss: 0.4096 | Val Loss: 0.4170 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4170)\r\n",
      "Epoch 047 | Train Loss: 0.4136 | Val Loss: 0.4174 | Time: 5.74s\r\n",
      "Epoch 048 | Train Loss: 0.4095 | Val Loss: 0.4173 | Time: 5.74s\r\n",
      "Epoch 049 | Train Loss: 0.4095 | Val Loss: 0.4172 | Time: 5.74s\r\n",
      "✅ Meilleur modèle chargé (époque 46, val_loss: 0.4170)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineBackup        : 0.0738\r\n",
      "   2. MultipleLines       : 0.0652\r\n",
      "   3. TotalCharges        : 0.0639\r\n",
      "   4. tenure              : 0.0616\r\n",
      "   5. PaperlessBilling    : 0.0609\r\n",
      "   6. InternetService     : 0.0607\r\n",
      "   7. PhoneService        : 0.0606\r\n",
      "   8. StreamingMovies     : 0.0529\r\n",
      "   9. Dependents          : 0.0523\r\n",
      "  10. Contract            : 0.0505\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. OnlineBackup         (CAT): 0.0738\r\n",
      "   2. MultipleLines        (CAT): 0.0652\r\n",
      "   3. TotalCharges         (NUM): 0.0639\r\n",
      "   4. tenure               (NUM): 0.0616\r\n",
      "   5. PaperlessBilling     (CAT): 0.0609\r\n",
      "   6. InternetService      (CAT): 0.0607\r\n",
      "   7. PhoneService         (CAT): 0.0606\r\n",
      "   8. StreamingMovies      (CAT): 0.0529\r\n",
      "   9. Dependents           (CAT): 0.0523\r\n",
      "  10. Contract             (CAT): 0.0505\r\n",
      "  11. gender               (CAT): 0.0483\r\n",
      "  12. TechSupport          (CAT): 0.0478\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'tenure'] → indices [2, 0]\r\n",
      "   - Catégorielles sélectionnées: ['OnlineBackup', 'MultipleLines', 'PaperlessBilling', 'InternetService', 'PhoneService', 'StreamingMovies', 'Dependents', 'Contract', 'gender', 'TechSupport'] → indices [8, 5, 14, 6, 4, 12, 3, 13, 0, 10]\r\n",
      "📊 Features sélectionnées: 2 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 61,569 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.07%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6136 | Val Loss: 0.5821 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5821)\r\n",
      "Epoch 001 | Train Loss: 0.5964 | Val Loss: 0.5686 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5686)\r\n",
      "Epoch 002 | Train Loss: 0.5811 | Val Loss: 0.5595 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5595)\r\n",
      "Epoch 003 | Train Loss: 0.5729 | Val Loss: 0.5523 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5523)\r\n",
      "Epoch 004 | Train Loss: 0.5661 | Val Loss: 0.5444 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5444)\r\n",
      "Epoch 005 | Train Loss: 0.5570 | Val Loss: 0.5355 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5355)\r\n",
      "Epoch 006 | Train Loss: 0.5549 | Val Loss: 0.5239 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5239)\r\n",
      "Epoch 007 | Train Loss: 0.5487 | Val Loss: 0.5136 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5136)\r\n",
      "Epoch 008 | Train Loss: 0.5416 | Val Loss: 0.5046 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5046)\r\n",
      "Epoch 009 | Train Loss: 0.5340 | Val Loss: 0.4967 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4967)\r\n",
      "Epoch 010 | Train Loss: 0.5250 | Val Loss: 0.4901 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4901)\r\n",
      "Epoch 011 | Train Loss: 0.5221 | Val Loss: 0.4846 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4846)\r\n",
      "Epoch 012 | Train Loss: 0.5170 | Val Loss: 0.4793 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4793)\r\n",
      "Epoch 013 | Train Loss: 0.5125 | Val Loss: 0.4747 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4747)\r\n",
      "Epoch 014 | Train Loss: 0.5087 | Val Loss: 0.4698 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4698)\r\n",
      "Epoch 015 | Train Loss: 0.5052 | Val Loss: 0.4660 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4660)\r\n",
      "Epoch 016 | Train Loss: 0.4989 | Val Loss: 0.4625 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4625)\r\n",
      "Epoch 017 | Train Loss: 0.4959 | Val Loss: 0.4586 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4586)\r\n",
      "Epoch 018 | Train Loss: 0.4917 | Val Loss: 0.4552 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4552)\r\n",
      "Epoch 019 | Train Loss: 0.4952 | Val Loss: 0.4530 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4530)\r\n",
      "Epoch 020 | Train Loss: 0.4893 | Val Loss: 0.4509 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4509)\r\n",
      "Epoch 021 | Train Loss: 0.4872 | Val Loss: 0.4502 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4502)\r\n",
      "Epoch 022 | Train Loss: 0.4888 | Val Loss: 0.4474 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4474)\r\n",
      "Epoch 023 | Train Loss: 0.4866 | Val Loss: 0.4483 | Time: 3.28s\r\n",
      "Epoch 024 | Train Loss: 0.4827 | Val Loss: 0.4481 | Time: 3.22s\r\n",
      "Epoch 025 | Train Loss: 0.4803 | Val Loss: 0.4469 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4469)\r\n",
      "Epoch 026 | Train Loss: 0.4760 | Val Loss: 0.4448 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4448)\r\n",
      "Epoch 027 | Train Loss: 0.4802 | Val Loss: 0.4469 | Time: 3.22s\r\n",
      "Epoch 028 | Train Loss: 0.4783 | Val Loss: 0.4456 | Time: 3.33s\r\n",
      "Epoch 029 | Train Loss: 0.4748 | Val Loss: 0.4440 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4440)\r\n",
      "Epoch 030 | Train Loss: 0.4816 | Val Loss: 0.4430 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4430)\r\n",
      "Epoch 031 | Train Loss: 0.4779 | Val Loss: 0.4421 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4421)\r\n",
      "Epoch 032 | Train Loss: 0.4714 | Val Loss: 0.4426 | Time: 3.26s\r\n",
      "Epoch 033 | Train Loss: 0.4737 | Val Loss: 0.4413 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4413)\r\n",
      "Epoch 034 | Train Loss: 0.4740 | Val Loss: 0.4414 | Time: 3.26s\r\n",
      "Epoch 035 | Train Loss: 0.4747 | Val Loss: 0.4411 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4411)\r\n",
      "Epoch 036 | Train Loss: 0.4712 | Val Loss: 0.4395 | Time: 3.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4395)\r\n",
      "Epoch 037 | Train Loss: 0.4744 | Val Loss: 0.4411 | Time: 3.34s\r\n",
      "Epoch 038 | Train Loss: 0.4770 | Val Loss: 0.4417 | Time: 3.25s\r\n",
      "Epoch 039 | Train Loss: 0.4703 | Val Loss: 0.4406 | Time: 3.25s\r\n",
      "Epoch 040 | Train Loss: 0.4761 | Val Loss: 0.4408 | Time: 3.27s\r\n",
      "Epoch 041 | Train Loss: 0.4713 | Val Loss: 0.4394 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4394)\r\n",
      "Epoch 042 | Train Loss: 0.4723 | Val Loss: 0.4396 | Time: 3.26s\r\n",
      "Epoch 043 | Train Loss: 0.4690 | Val Loss: 0.4398 | Time: 3.30s\r\n",
      "Epoch 044 | Train Loss: 0.4746 | Val Loss: 0.4398 | Time: 3.23s\r\n",
      "Epoch 045 | Train Loss: 0.4720 | Val Loss: 0.4389 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4389)\r\n",
      "Epoch 046 | Train Loss: 0.4715 | Val Loss: 0.4372 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4372)\r\n",
      "Epoch 047 | Train Loss: 0.4677 | Val Loss: 0.4371 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4371)\r\n",
      "Epoch 048 | Train Loss: 0.4653 | Val Loss: 0.4373 | Time: 3.21s\r\n",
      "Epoch 049 | Train Loss: 0.4704 | Val Loss: 0.4370 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4370)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4370)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Contract             (CAT): 0.1683\r\n",
      "   2. MultipleLines        (CAT): 0.1220\r\n",
      "   3. tenure               (NUM): 0.1144\r\n",
      "   4. TotalCharges         (NUM): 0.1079\r\n",
      "   5. PhoneService         (CAT): 0.1026\r\n",
      "   6. InternetService      (CAT): 0.0822\r\n",
      "   7. TechSupport          (CAT): 0.0781\r\n",
      "   8. StreamingMovies      (CAT): 0.0457\r\n",
      "   9. Dependents           (CAT): 0.0454\r\n",
      "  10. PaperlessBilling     (CAT): 0.0453\r\n",
      "  11. OnlineBackup         (CAT): 0.0451\r\n",
      "  12. gender               (CAT): 0.0430\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.1683\r\n",
      "   2. MultipleLines       : 0.1220\r\n",
      "   3. tenure              : 0.1144\r\n",
      "   4. TotalCharges        : 0.1079\r\n",
      "   5. PhoneService        : 0.1026\r\n",
      "   6. InternetService     : 0.0822\r\n",
      "   7. TechSupport         : 0.0781\r\n",
      "   8. StreamingMovies     : 0.0457\r\n",
      "   9. Dependents          : 0.0454\r\n",
      "  10. PaperlessBilling    : 0.0453\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 452.6s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6182 | Val Loss: 0.5586 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5586)\r\n",
      "Epoch 001 | Train Loss: 0.5551 | Val Loss: 0.5276 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5276)\r\n",
      "Epoch 002 | Train Loss: 0.5265 | Val Loss: 0.4933 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4933)\r\n",
      "Epoch 003 | Train Loss: 0.4995 | Val Loss: 0.4668 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4668)\r\n",
      "Epoch 004 | Train Loss: 0.4816 | Val Loss: 0.4539 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4539)\r\n",
      "Epoch 005 | Train Loss: 0.4729 | Val Loss: 0.4450 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4450)\r\n",
      "Epoch 006 | Train Loss: 0.4673 | Val Loss: 0.4420 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4420)\r\n",
      "Epoch 007 | Train Loss: 0.4630 | Val Loss: 0.4380 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4380)\r\n",
      "Epoch 008 | Train Loss: 0.4583 | Val Loss: 0.4360 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4360)\r\n",
      "Epoch 009 | Train Loss: 0.4501 | Val Loss: 0.4342 | Time: 5.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 010 | Train Loss: 0.4474 | Val Loss: 0.4302 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4302)\r\n",
      "Epoch 011 | Train Loss: 0.4487 | Val Loss: 0.4297 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 012 | Train Loss: 0.4466 | Val Loss: 0.4274 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4274)\r\n",
      "Epoch 013 | Train Loss: 0.4395 | Val Loss: 0.4260 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4260)\r\n",
      "Epoch 014 | Train Loss: 0.4416 | Val Loss: 0.4259 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4259)\r\n",
      "Epoch 015 | Train Loss: 0.4385 | Val Loss: 0.4241 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 016 | Train Loss: 0.4415 | Val Loss: 0.4227 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4227)\r\n",
      "Epoch 017 | Train Loss: 0.4384 | Val Loss: 0.4206 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4206)\r\n",
      "Epoch 018 | Train Loss: 0.4374 | Val Loss: 0.4214 | Time: 5.67s\r\n",
      "Epoch 019 | Train Loss: 0.4315 | Val Loss: 0.4213 | Time: 5.65s\r\n",
      "Epoch 020 | Train Loss: 0.4343 | Val Loss: 0.4190 | Time: 5.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4190)\r\n",
      "Epoch 021 | Train Loss: 0.4273 | Val Loss: 0.4193 | Time: 5.63s\r\n",
      "Epoch 022 | Train Loss: 0.4299 | Val Loss: 0.4186 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 023 | Train Loss: 0.4332 | Val Loss: 0.4180 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 024 | Train Loss: 0.4310 | Val Loss: 0.4166 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4166)\r\n",
      "Epoch 025 | Train Loss: 0.4270 | Val Loss: 0.4154 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4154)\r\n",
      "Epoch 026 | Train Loss: 0.4307 | Val Loss: 0.4152 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4152)\r\n",
      "Epoch 027 | Train Loss: 0.4291 | Val Loss: 0.4149 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4149)\r\n",
      "Epoch 028 | Train Loss: 0.4274 | Val Loss: 0.4129 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4129)\r\n",
      "Epoch 029 | Train Loss: 0.4255 | Val Loss: 0.4122 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4122)\r\n",
      "Epoch 030 | Train Loss: 0.4286 | Val Loss: 0.4129 | Time: 5.72s\r\n",
      "Epoch 031 | Train Loss: 0.4258 | Val Loss: 0.4129 | Time: 5.86s\r\n",
      "Epoch 032 | Train Loss: 0.4255 | Val Loss: 0.4143 | Time: 5.73s\r\n",
      "Epoch 033 | Train Loss: 0.4249 | Val Loss: 0.4121 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4121)\r\n",
      "Epoch 034 | Train Loss: 0.4219 | Val Loss: 0.4130 | Time: 5.71s\r\n",
      "Epoch 035 | Train Loss: 0.4210 | Val Loss: 0.4129 | Time: 5.76s\r\n",
      "Epoch 036 | Train Loss: 0.4215 | Val Loss: 0.4133 | Time: 5.88s\r\n",
      "Epoch 037 | Train Loss: 0.4223 | Val Loss: 0.4126 | Time: 5.89s\r\n",
      "Epoch 038 | Train Loss: 0.4233 | Val Loss: 0.4120 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4120)\r\n",
      "Epoch 039 | Train Loss: 0.4205 | Val Loss: 0.4134 | Time: 5.70s\r\n",
      "Epoch 040 | Train Loss: 0.4182 | Val Loss: 0.4126 | Time: 5.63s\r\n",
      "Epoch 041 | Train Loss: 0.4214 | Val Loss: 0.4119 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4119)\r\n",
      "Epoch 042 | Train Loss: 0.4219 | Val Loss: 0.4128 | Time: 5.88s\r\n",
      "Epoch 043 | Train Loss: 0.4213 | Val Loss: 0.4117 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4117)\r\n",
      "Epoch 044 | Train Loss: 0.4173 | Val Loss: 0.4117 | Time: 5.74s\r\n",
      "Epoch 045 | Train Loss: 0.4188 | Val Loss: 0.4124 | Time: 5.66s\r\n",
      "Epoch 046 | Train Loss: 0.4172 | Val Loss: 0.4125 | Time: 5.73s\r\n",
      "Epoch 047 | Train Loss: 0.4202 | Val Loss: 0.4119 | Time: 5.85s\r\n",
      "Epoch 048 | Train Loss: 0.4172 | Val Loss: 0.4121 | Time: 5.92s\r\n",
      "Epoch 049 | Train Loss: 0.4182 | Val Loss: 0.4126 | Time: 5.73s\r\n",
      "✅ Meilleur modèle chargé (époque 43, val_loss: 0.4117)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaperlessBilling    : 0.0721\r\n",
      "   2. gender              : 0.0630\r\n",
      "   3. TechSupport         : 0.0581\r\n",
      "   4. PhoneService        : 0.0563\r\n",
      "   5. TotalCharges        : 0.0559\r\n",
      "   6. SeniorCitizen       : 0.0546\r\n",
      "   7. DeviceProtection    : 0.0542\r\n",
      "   8. PaymentMethod       : 0.0539\r\n",
      "   9. Dependents          : 0.0532\r\n",
      "  10. Contract            : 0.0524\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 12 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaperlessBilling     (CAT): 0.0721\r\n",
      "   2. gender               (CAT): 0.0630\r\n",
      "   3. TechSupport          (CAT): 0.0581\r\n",
      "   4. PhoneService         (CAT): 0.0563\r\n",
      "   5. TotalCharges         (NUM): 0.0559\r\n",
      "   6. SeniorCitizen        (CAT): 0.0546\r\n",
      "   7. DeviceProtection     (CAT): 0.0542\r\n",
      "   8. PaymentMethod        (CAT): 0.0539\r\n",
      "   9. Dependents           (CAT): 0.0532\r\n",
      "  10. Contract             (CAT): 0.0524\r\n",
      "  11. StreamingMovies      (CAT): 0.0515\r\n",
      "  12. StreamingTV          (CAT): 0.0505\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['PaperlessBilling', 'gender', 'TechSupport', 'PhoneService', 'SeniorCitizen', 'DeviceProtection', 'PaymentMethod', 'Dependents', 'Contract', 'StreamingMovies', 'StreamingTV'] → indices [14, 0, 10, 4, 1, 9, 15, 3, 13, 12, 11]\r\n",
      "📊 Features sélectionnées: 1 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 4 paires\r\n",
      "Modèle Random créé avec 61,697 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.07%\r\n",
      "   - Connexions feature-feature: 8\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5867 | Val Loss: 0.5619 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5619)\r\n",
      "Epoch 001 | Train Loss: 0.5746 | Val Loss: 0.5485 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5485)\r\n",
      "Epoch 002 | Train Loss: 0.5697 | Val Loss: 0.5380 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5380)\r\n",
      "Epoch 003 | Train Loss: 0.5616 | Val Loss: 0.5277 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5277)\r\n",
      "Epoch 004 | Train Loss: 0.5585 | Val Loss: 0.5180 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5180)\r\n",
      "Epoch 005 | Train Loss: 0.5553 | Val Loss: 0.5094 | Time: 3.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5094)\r\n",
      "Epoch 006 | Train Loss: 0.5525 | Val Loss: 0.5015 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5015)\r\n",
      "Epoch 007 | Train Loss: 0.5470 | Val Loss: 0.4940 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4940)\r\n",
      "Epoch 008 | Train Loss: 0.5419 | Val Loss: 0.4882 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4882)\r\n",
      "Epoch 009 | Train Loss: 0.5347 | Val Loss: 0.4829 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4829)\r\n",
      "Epoch 010 | Train Loss: 0.5293 | Val Loss: 0.4791 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4791)\r\n",
      "Epoch 011 | Train Loss: 0.5282 | Val Loss: 0.4762 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4762)\r\n",
      "Epoch 012 | Train Loss: 0.5257 | Val Loss: 0.4736 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4736)\r\n",
      "Epoch 013 | Train Loss: 0.5243 | Val Loss: 0.4710 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4710)\r\n",
      "Epoch 014 | Train Loss: 0.5226 | Val Loss: 0.4682 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4682)\r\n",
      "Epoch 015 | Train Loss: 0.5224 | Val Loss: 0.4655 | Time: 3.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4655)\r\n",
      "Epoch 016 | Train Loss: 0.5149 | Val Loss: 0.4635 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4635)\r\n",
      "Epoch 017 | Train Loss: 0.5186 | Val Loss: 0.4619 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4619)\r\n",
      "Epoch 018 | Train Loss: 0.5183 | Val Loss: 0.4604 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4604)\r\n",
      "Epoch 019 | Train Loss: 0.5110 | Val Loss: 0.4587 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4587)\r\n",
      "Epoch 020 | Train Loss: 0.5096 | Val Loss: 0.4578 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4578)\r\n",
      "Epoch 021 | Train Loss: 0.5058 | Val Loss: 0.4568 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4568)\r\n",
      "Epoch 022 | Train Loss: 0.5058 | Val Loss: 0.4554 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4554)\r\n",
      "Epoch 023 | Train Loss: 0.5076 | Val Loss: 0.4540 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4540)\r\n",
      "Epoch 024 | Train Loss: 0.5088 | Val Loss: 0.4522 | Time: 3.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4522)\r\n",
      "Epoch 025 | Train Loss: 0.5069 | Val Loss: 0.4518 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4518)\r\n",
      "Epoch 026 | Train Loss: 0.5007 | Val Loss: 0.4524 | Time: 3.20s\r\n",
      "Epoch 027 | Train Loss: 0.4983 | Val Loss: 0.4510 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4510)\r\n",
      "Epoch 028 | Train Loss: 0.4992 | Val Loss: 0.4499 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4499)\r\n",
      "Epoch 029 | Train Loss: 0.4993 | Val Loss: 0.4488 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4488)\r\n",
      "Epoch 030 | Train Loss: 0.5034 | Val Loss: 0.4486 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4486)\r\n",
      "Epoch 031 | Train Loss: 0.5001 | Val Loss: 0.4480 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4480)\r\n",
      "Epoch 032 | Train Loss: 0.4945 | Val Loss: 0.4487 | Time: 3.25s\r\n",
      "Epoch 033 | Train Loss: 0.5018 | Val Loss: 0.4473 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4473)\r\n",
      "Epoch 034 | Train Loss: 0.5037 | Val Loss: 0.4477 | Time: 3.37s\r\n",
      "Epoch 035 | Train Loss: 0.4993 | Val Loss: 0.4469 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4469)\r\n",
      "Epoch 036 | Train Loss: 0.5001 | Val Loss: 0.4467 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4467)\r\n",
      "Epoch 037 | Train Loss: 0.4984 | Val Loss: 0.4462 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4462)\r\n",
      "Epoch 038 | Train Loss: 0.4937 | Val Loss: 0.4463 | Time: 3.24s\r\n",
      "Epoch 039 | Train Loss: 0.5001 | Val Loss: 0.4461 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4461)\r\n",
      "Epoch 040 | Train Loss: 0.5001 | Val Loss: 0.4453 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4453)\r\n",
      "Epoch 041 | Train Loss: 0.4897 | Val Loss: 0.4455 | Time: 3.23s\r\n",
      "Epoch 042 | Train Loss: 0.4940 | Val Loss: 0.4448 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4448)\r\n",
      "Epoch 043 | Train Loss: 0.5018 | Val Loss: 0.4434 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4434)\r\n",
      "Epoch 044 | Train Loss: 0.4944 | Val Loss: 0.4441 | Time: 3.44s\r\n",
      "Epoch 045 | Train Loss: 0.4940 | Val Loss: 0.4446 | Time: 3.27s\r\n",
      "Epoch 046 | Train Loss: 0.4897 | Val Loss: 0.4446 | Time: 3.34s\r\n",
      "Epoch 047 | Train Loss: 0.4976 | Val Loss: 0.4453 | Time: 3.20s\r\n",
      "Epoch 048 | Train Loss: 0.4868 | Val Loss: 0.4451 | Time: 3.21s\r\n",
      "Epoch 049 | Train Loss: 0.4891 | Val Loss: 0.4455 | Time: 3.24s\r\n",
      "✅ Meilleur modèle Random chargé (époque 43, val_loss: 0.4434)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. gender               (CAT): 0.1714\r\n",
      "   2. Dependents           (CAT): 0.1009\r\n",
      "   3. TotalCharges         (NUM): 0.1004\r\n",
      "   4. StreamingTV          (CAT): 0.0790\r\n",
      "   5. PaperlessBilling     (CAT): 0.0700\r\n",
      "   6. TechSupport          (CAT): 0.0699\r\n",
      "   7. PhoneService         (CAT): 0.0698\r\n",
      "   8. SeniorCitizen        (CAT): 0.0692\r\n",
      "   9. Contract             (CAT): 0.0691\r\n",
      "  10. StreamingMovies      (CAT): 0.0670\r\n",
      "  11. PaymentMethod        (CAT): 0.0670\r\n",
      "  12. DeviceProtection     (CAT): 0.0663\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.1714\r\n",
      "   2. Dependents          : 0.1009\r\n",
      "   3. TotalCharges        : 0.1004\r\n",
      "   4. StreamingTV         : 0.0790\r\n",
      "   5. PaperlessBilling    : 0.0700\r\n",
      "   6. TechSupport         : 0.0699\r\n",
      "   7. PhoneService        : 0.0698\r\n",
      "   8. SeniorCitizen       : 0.0692\r\n",
      "   9. Contract            : 0.0691\r\n",
      "  10. StreamingMovies     : 0.0670\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_21/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 453.4s ===\r\n",
      "\u001b[32m[I 2025-07-20 01:11:22,623]\u001b[0m Trial 21 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 2, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.13778767654220575, 'ffn_dropout_stage1': 0.10678322003692638, 'residual_dropout_stage1': 0.12737254415123986, 'lr_stage1': 6.177386049214724e-05, 'weight_decay_stage1': 0.07984968783143429, 'd_token_stage2': 64, 'n_blocks_stage2': 2, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.26342745442058235, 'ffn_dropout_stage2': 0.26932088418580274, 'residual_dropout_stage2': 0.19866243085428, 'lr_stage2': 1.1064025483952259e-05, 'weight_decay_stage2': 0.00018258595571724203, 'batch_size': 32, 'patience': 20, 'embedding_type': 'T-LR', 'M': 12, 'k': 4}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  88%|███████▉ | 22/25 [6:58:25<55:34, 1111.66s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.7207 | Val Loss: 0.6406 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6406)\r\n",
      "Epoch 001 | Train Loss: 0.6384 | Val Loss: 0.6000 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6000)\r\n",
      "Epoch 002 | Train Loss: 0.6030 | Val Loss: 0.5689 | Time: 5.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5689)\r\n",
      "Epoch 003 | Train Loss: 0.5741 | Val Loss: 0.5436 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5436)\r\n",
      "Epoch 004 | Train Loss: 0.5536 | Val Loss: 0.5257 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5257)\r\n",
      "Epoch 005 | Train Loss: 0.5405 | Val Loss: 0.5114 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5114)\r\n",
      "Epoch 006 | Train Loss: 0.5278 | Val Loss: 0.5005 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5005)\r\n",
      "Epoch 007 | Train Loss: 0.5155 | Val Loss: 0.4915 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4915)\r\n",
      "Epoch 008 | Train Loss: 0.5089 | Val Loss: 0.4844 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4844)\r\n",
      "Epoch 009 | Train Loss: 0.5022 | Val Loss: 0.4806 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4806)\r\n",
      "Epoch 010 | Train Loss: 0.4964 | Val Loss: 0.4756 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4756)\r\n",
      "Epoch 011 | Train Loss: 0.4935 | Val Loss: 0.4703 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4703)\r\n",
      "Epoch 012 | Train Loss: 0.4903 | Val Loss: 0.4668 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4668)\r\n",
      "Epoch 013 | Train Loss: 0.4853 | Val Loss: 0.4645 | Time: 5.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4645)\r\n",
      "Epoch 014 | Train Loss: 0.4829 | Val Loss: 0.4615 | Time: 5.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4615)\r\n",
      "Epoch 015 | Train Loss: 0.4811 | Val Loss: 0.4597 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4597)\r\n",
      "Epoch 016 | Train Loss: 0.4774 | Val Loss: 0.4583 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4583)\r\n",
      "Epoch 017 | Train Loss: 0.4747 | Val Loss: 0.4551 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4551)\r\n",
      "Epoch 018 | Train Loss: 0.4725 | Val Loss: 0.4522 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4522)\r\n",
      "Epoch 019 | Train Loss: 0.4695 | Val Loss: 0.4520 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4520)\r\n",
      "Epoch 020 | Train Loss: 0.4711 | Val Loss: 0.4496 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4496)\r\n",
      "Epoch 021 | Train Loss: 0.4694 | Val Loss: 0.4494 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4494)\r\n",
      "Epoch 022 | Train Loss: 0.4621 | Val Loss: 0.4474 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4474)\r\n",
      "Epoch 023 | Train Loss: 0.4635 | Val Loss: 0.4463 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4463)\r\n",
      "Epoch 024 | Train Loss: 0.4620 | Val Loss: 0.4442 | Time: 5.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4442)\r\n",
      "Epoch 025 | Train Loss: 0.4576 | Val Loss: 0.4426 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4426)\r\n",
      "Epoch 026 | Train Loss: 0.4562 | Val Loss: 0.4420 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4420)\r\n",
      "Epoch 027 | Train Loss: 0.4581 | Val Loss: 0.4410 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4410)\r\n",
      "Epoch 028 | Train Loss: 0.4538 | Val Loss: 0.4394 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4394)\r\n",
      "Epoch 029 | Train Loss: 0.4555 | Val Loss: 0.4385 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 030 | Train Loss: 0.4510 | Val Loss: 0.4375 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4375)\r\n",
      "Epoch 031 | Train Loss: 0.4521 | Val Loss: 0.4360 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4360)\r\n",
      "Epoch 032 | Train Loss: 0.4460 | Val Loss: 0.4347 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4347)\r\n",
      "Epoch 033 | Train Loss: 0.4490 | Val Loss: 0.4341 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4341)\r\n",
      "Epoch 034 | Train Loss: 0.4456 | Val Loss: 0.4332 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4332)\r\n",
      "Epoch 035 | Train Loss: 0.4476 | Val Loss: 0.4318 | Time: 5.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4318)\r\n",
      "Epoch 036 | Train Loss: 0.4472 | Val Loss: 0.4323 | Time: 5.68s\r\n",
      "Epoch 037 | Train Loss: 0.4448 | Val Loss: 0.4313 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4313)\r\n",
      "Epoch 038 | Train Loss: 0.4455 | Val Loss: 0.4311 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4311)\r\n",
      "Epoch 039 | Train Loss: 0.4413 | Val Loss: 0.4293 | Time: 5.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 040 | Train Loss: 0.4386 | Val Loss: 0.4287 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4287)\r\n",
      "Epoch 041 | Train Loss: 0.4401 | Val Loss: 0.4280 | Time: 5.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4280)\r\n",
      "Epoch 042 | Train Loss: 0.4393 | Val Loss: 0.4276 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4276)\r\n",
      "Epoch 043 | Train Loss: 0.4404 | Val Loss: 0.4269 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4269)\r\n",
      "Epoch 044 | Train Loss: 0.4390 | Val Loss: 0.4262 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4262)\r\n",
      "Epoch 045 | Train Loss: 0.4379 | Val Loss: 0.4258 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4258)\r\n",
      "Epoch 046 | Train Loss: 0.4391 | Val Loss: 0.4255 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 047 | Train Loss: 0.4380 | Val Loss: 0.4249 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4249)\r\n",
      "Epoch 048 | Train Loss: 0.4340 | Val Loss: 0.4238 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4238)\r\n",
      "Epoch 049 | Train Loss: 0.4385 | Val Loss: 0.4238 | Time: 5.72s\r\n",
      "✅ Meilleur modèle chargé (époque 48, val_loss: 0.4238)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.1035\r\n",
      "   2. TechSupport         : 0.0587\r\n",
      "   3. InternetService     : 0.0578\r\n",
      "   4. StreamingMovies     : 0.0570\r\n",
      "   5. OnlineSecurity      : 0.0563\r\n",
      "   6. SeniorCitizen       : 0.0527\r\n",
      "   7. gender              : 0.0519\r\n",
      "   8. Dependents          : 0.0515\r\n",
      "   9. PaperlessBilling    : 0.0502\r\n",
      "  10. PaymentMethod       : 0.0498\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 11 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MonthlyCharges       (NUM): 0.1035\r\n",
      "   2. TechSupport          (CAT): 0.0587\r\n",
      "   3. InternetService      (CAT): 0.0578\r\n",
      "   4. StreamingMovies      (CAT): 0.0570\r\n",
      "   5. OnlineSecurity       (CAT): 0.0563\r\n",
      "   6. SeniorCitizen        (CAT): 0.0527\r\n",
      "   7. gender               (CAT): 0.0519\r\n",
      "   8. Dependents           (CAT): 0.0515\r\n",
      "   9. PaperlessBilling     (CAT): 0.0502\r\n",
      "  10. PaymentMethod        (CAT): 0.0498\r\n",
      "  11. StreamingTV          (CAT): 0.0480\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['TechSupport', 'InternetService', 'StreamingMovies', 'OnlineSecurity', 'SeniorCitizen', 'gender', 'Dependents', 'PaperlessBilling', 'PaymentMethod', 'StreamingTV'] → indices [10, 6, 12, 7, 1, 0, 3, 14, 15, 11]\r\n",
      "📊 Features sélectionnées: 1 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 3 paires\r\n",
      "Modèle Random créé avec 61,505 paramètres\r\n",
      "🔗 Sparsité d'attention: 80.56%\r\n",
      "   - Connexions feature-feature: 6\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6542 | Val Loss: 0.5592 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5592)\r\n",
      "Epoch 001 | Train Loss: 0.5718 | Val Loss: 0.5422 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5422)\r\n",
      "Epoch 002 | Train Loss: 0.5558 | Val Loss: 0.5277 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5277)\r\n",
      "Epoch 003 | Train Loss: 0.5470 | Val Loss: 0.5116 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5116)\r\n",
      "Epoch 004 | Train Loss: 0.5308 | Val Loss: 0.5016 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5016)\r\n",
      "Epoch 005 | Train Loss: 0.5217 | Val Loss: 0.4938 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4938)\r\n",
      "Epoch 006 | Train Loss: 0.5166 | Val Loss: 0.4850 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4850)\r\n",
      "Epoch 007 | Train Loss: 0.5098 | Val Loss: 0.4794 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4794)\r\n",
      "Epoch 008 | Train Loss: 0.5025 | Val Loss: 0.4733 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4733)\r\n",
      "Epoch 009 | Train Loss: 0.4999 | Val Loss: 0.4703 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4703)\r\n",
      "Epoch 010 | Train Loss: 0.5014 | Val Loss: 0.4678 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4678)\r\n",
      "Epoch 011 | Train Loss: 0.4980 | Val Loss: 0.4640 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4640)\r\n",
      "Epoch 012 | Train Loss: 0.4958 | Val Loss: 0.4625 | Time: 3.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4625)\r\n",
      "Epoch 013 | Train Loss: 0.4963 | Val Loss: 0.4624 | Time: 3.28s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4624)\r\n",
      "Epoch 014 | Train Loss: 0.4910 | Val Loss: 0.4612 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4612)\r\n",
      "Epoch 015 | Train Loss: 0.4920 | Val Loss: 0.4604 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4604)\r\n",
      "Epoch 016 | Train Loss: 0.4942 | Val Loss: 0.4600 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4600)\r\n",
      "Epoch 017 | Train Loss: 0.4889 | Val Loss: 0.4603 | Time: 3.21s\r\n",
      "Epoch 018 | Train Loss: 0.4865 | Val Loss: 0.4595 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4595)\r\n",
      "Epoch 019 | Train Loss: 0.4917 | Val Loss: 0.4587 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4587)\r\n",
      "Epoch 020 | Train Loss: 0.4904 | Val Loss: 0.4591 | Time: 3.32s\r\n",
      "Epoch 021 | Train Loss: 0.4891 | Val Loss: 0.4588 | Time: 3.23s\r\n",
      "Epoch 022 | Train Loss: 0.4887 | Val Loss: 0.4589 | Time: 3.36s\r\n",
      "Epoch 023 | Train Loss: 0.4900 | Val Loss: 0.4587 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4587)\r\n",
      "Epoch 024 | Train Loss: 0.4901 | Val Loss: 0.4587 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4587)\r\n",
      "Epoch 025 | Train Loss: 0.4895 | Val Loss: 0.4588 | Time: 3.23s\r\n",
      "Epoch 026 | Train Loss: 0.4896 | Val Loss: 0.4585 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4585)\r\n",
      "Epoch 027 | Train Loss: 0.4853 | Val Loss: 0.4583 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4583)\r\n",
      "Epoch 028 | Train Loss: 0.4867 | Val Loss: 0.4584 | Time: 3.23s\r\n",
      "Epoch 029 | Train Loss: 0.4868 | Val Loss: 0.4575 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4575)\r\n",
      "Epoch 030 | Train Loss: 0.4840 | Val Loss: 0.4578 | Time: 3.32s\r\n",
      "Epoch 031 | Train Loss: 0.4849 | Val Loss: 0.4578 | Time: 3.20s\r\n",
      "Epoch 032 | Train Loss: 0.4841 | Val Loss: 0.4576 | Time: 3.49s\r\n",
      "Epoch 033 | Train Loss: 0.4888 | Val Loss: 0.4567 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4567)\r\n",
      "Epoch 034 | Train Loss: 0.4859 | Val Loss: 0.4566 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4566)\r\n",
      "Epoch 035 | Train Loss: 0.4860 | Val Loss: 0.4562 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4562)\r\n",
      "Epoch 036 | Train Loss: 0.4817 | Val Loss: 0.4568 | Time: 3.20s\r\n",
      "Epoch 037 | Train Loss: 0.4832 | Val Loss: 0.4568 | Time: 3.21s\r\n",
      "Epoch 038 | Train Loss: 0.4860 | Val Loss: 0.4566 | Time: 3.31s\r\n",
      "Epoch 039 | Train Loss: 0.4830 | Val Loss: 0.4564 | Time: 3.20s\r\n",
      "Epoch 040 | Train Loss: 0.4865 | Val Loss: 0.4564 | Time: 3.21s\r\n",
      "Epoch 041 | Train Loss: 0.4821 | Val Loss: 0.4566 | Time: 3.26s\r\n",
      "Epoch 042 | Train Loss: 0.4831 | Val Loss: 0.4562 | Time: 3.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4562)\r\n",
      "Epoch 043 | Train Loss: 0.4845 | Val Loss: 0.4566 | Time: 3.22s\r\n",
      "Epoch 044 | Train Loss: 0.4836 | Val Loss: 0.4568 | Time: 3.23s\r\n",
      "Epoch 045 | Train Loss: 0.4844 | Val Loss: 0.4570 | Time: 3.23s\r\n",
      "Epoch 046 | Train Loss: 0.4818 | Val Loss: 0.4567 | Time: 3.21s\r\n",
      "Epoch 047 | Train Loss: 0.4855 | Val Loss: 0.4572 | Time: 3.23s\r\n",
      "Epoch 048 | Train Loss: 0.4822 | Val Loss: 0.4569 | Time: 3.30s\r\n",
      "Epoch 049 | Train Loss: 0.4824 | Val Loss: 0.4570 | Time: 3.24s\r\n",
      "✅ Meilleur modèle Random chargé (époque 42, val_loss: 0.4562)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingTV          (CAT): 0.2069\r\n",
      "   2. PaymentMethod        (CAT): 0.1822\r\n",
      "   3. Dependents           (CAT): 0.0911\r\n",
      "   4. StreamingMovies      (CAT): 0.0678\r\n",
      "   5. SeniorCitizen        (CAT): 0.0674\r\n",
      "   6. gender               (CAT): 0.0666\r\n",
      "   7. MonthlyCharges       (NUM): 0.0666\r\n",
      "   8. TechSupport          (CAT): 0.0664\r\n",
      "   9. InternetService      (CAT): 0.0662\r\n",
      "  10. OnlineSecurity       (CAT): 0.0625\r\n",
      "  11. PaperlessBilling     (CAT): 0.0562\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingTV         : 0.2069\r\n",
      "   2. PaymentMethod       : 0.1822\r\n",
      "   3. Dependents          : 0.0911\r\n",
      "   4. StreamingMovies     : 0.0678\r\n",
      "   5. SeniorCitizen       : 0.0674\r\n",
      "   6. gender              : 0.0666\r\n",
      "   7. MonthlyCharges      : 0.0666\r\n",
      "   8. TechSupport         : 0.0664\r\n",
      "   9. InternetService     : 0.0662\r\n",
      "  10. OnlineSecurity      : 0.0625\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 452.1s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6722 | Val Loss: 0.6239 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6239)\r\n",
      "Epoch 001 | Train Loss: 0.6043 | Val Loss: 0.5835 | Time: 5.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5835)\r\n",
      "Epoch 002 | Train Loss: 0.5783 | Val Loss: 0.5588 | Time: 5.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5588)\r\n",
      "Epoch 003 | Train Loss: 0.5545 | Val Loss: 0.5356 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5356)\r\n",
      "Epoch 004 | Train Loss: 0.5350 | Val Loss: 0.5200 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5200)\r\n",
      "Epoch 005 | Train Loss: 0.5213 | Val Loss: 0.5095 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5095)\r\n",
      "Epoch 006 | Train Loss: 0.5123 | Val Loss: 0.5021 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5021)\r\n",
      "Epoch 007 | Train Loss: 0.5025 | Val Loss: 0.4992 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4992)\r\n",
      "Epoch 008 | Train Loss: 0.4942 | Val Loss: 0.4895 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4895)\r\n",
      "Epoch 009 | Train Loss: 0.4882 | Val Loss: 0.4848 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4848)\r\n",
      "Epoch 010 | Train Loss: 0.4832 | Val Loss: 0.4801 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4801)\r\n",
      "Epoch 011 | Train Loss: 0.4812 | Val Loss: 0.4737 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4737)\r\n",
      "Epoch 012 | Train Loss: 0.4760 | Val Loss: 0.4712 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4712)\r\n",
      "Epoch 013 | Train Loss: 0.4749 | Val Loss: 0.4682 | Time: 5.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4682)\r\n",
      "Epoch 014 | Train Loss: 0.4695 | Val Loss: 0.4654 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4654)\r\n",
      "Epoch 015 | Train Loss: 0.4648 | Val Loss: 0.4621 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4621)\r\n",
      "Epoch 016 | Train Loss: 0.4638 | Val Loss: 0.4605 | Time: 5.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4605)\r\n",
      "Epoch 017 | Train Loss: 0.4615 | Val Loss: 0.4596 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4596)\r\n",
      "Epoch 018 | Train Loss: 0.4600 | Val Loss: 0.4558 | Time: 5.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4558)\r\n",
      "Epoch 019 | Train Loss: 0.4580 | Val Loss: 0.4543 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4543)\r\n",
      "Epoch 020 | Train Loss: 0.4547 | Val Loss: 0.4523 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4523)\r\n",
      "Epoch 021 | Train Loss: 0.4558 | Val Loss: 0.4525 | Time: 5.76s\r\n",
      "Epoch 022 | Train Loss: 0.4535 | Val Loss: 0.4515 | Time: 5.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4515)\r\n",
      "Epoch 023 | Train Loss: 0.4492 | Val Loss: 0.4509 | Time: 5.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4509)\r\n",
      "Epoch 024 | Train Loss: 0.4512 | Val Loss: 0.4482 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4482)\r\n",
      "Epoch 025 | Train Loss: 0.4497 | Val Loss: 0.4473 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4473)\r\n",
      "Epoch 026 | Train Loss: 0.4454 | Val Loss: 0.4477 | Time: 5.73s\r\n",
      "Epoch 027 | Train Loss: 0.4456 | Val Loss: 0.4448 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4448)\r\n",
      "Epoch 028 | Train Loss: 0.4429 | Val Loss: 0.4449 | Time: 5.76s\r\n",
      "Epoch 029 | Train Loss: 0.4436 | Val Loss: 0.4434 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4434)\r\n",
      "Epoch 030 | Train Loss: 0.4396 | Val Loss: 0.4428 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4428)\r\n",
      "Epoch 031 | Train Loss: 0.4385 | Val Loss: 0.4413 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4413)\r\n",
      "Epoch 032 | Train Loss: 0.4421 | Val Loss: 0.4422 | Time: 5.76s\r\n",
      "Epoch 033 | Train Loss: 0.4405 | Val Loss: 0.4414 | Time: 5.78s\r\n",
      "Epoch 034 | Train Loss: 0.4376 | Val Loss: 0.4414 | Time: 5.63s\r\n",
      "Epoch 035 | Train Loss: 0.4349 | Val Loss: 0.4399 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4399)\r\n",
      "Epoch 036 | Train Loss: 0.4337 | Val Loss: 0.4391 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4391)\r\n",
      "Epoch 037 | Train Loss: 0.4358 | Val Loss: 0.4385 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 038 | Train Loss: 0.4332 | Val Loss: 0.4385 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4385)\r\n",
      "Epoch 039 | Train Loss: 0.4319 | Val Loss: 0.4372 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4372)\r\n",
      "Epoch 040 | Train Loss: 0.4310 | Val Loss: 0.4364 | Time: 5.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 041 | Train Loss: 0.4299 | Val Loss: 0.4352 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4352)\r\n",
      "Epoch 042 | Train Loss: 0.4327 | Val Loss: 0.4347 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4347)\r\n",
      "Epoch 043 | Train Loss: 0.4296 | Val Loss: 0.4343 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4343)\r\n",
      "Epoch 044 | Train Loss: 0.4290 | Val Loss: 0.4334 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4334)\r\n",
      "Epoch 045 | Train Loss: 0.4299 | Val Loss: 0.4321 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4321)\r\n",
      "Epoch 046 | Train Loss: 0.4259 | Val Loss: 0.4315 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4315)\r\n",
      "Epoch 047 | Train Loss: 0.4264 | Val Loss: 0.4307 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 048 | Train Loss: 0.4265 | Val Loss: 0.4302 | Time: 5.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4302)\r\n",
      "Epoch 049 | Train Loss: 0.4240 | Val Loss: 0.4296 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4296)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. MonthlyCharges      : 0.0793\r\n",
      "   2. OnlineSecurity      : 0.0640\r\n",
      "   3. StreamingTV         : 0.0636\r\n",
      "   4. TechSupport         : 0.0580\r\n",
      "   5. MultipleLines       : 0.0572\r\n",
      "   6. InternetService     : 0.0567\r\n",
      "   7. SeniorCitizen       : 0.0554\r\n",
      "   8. Dependents          : 0.0538\r\n",
      "   9. PaperlessBilling    : 0.0534\r\n",
      "  10. gender              : 0.0532\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 11 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. MonthlyCharges       (NUM): 0.0793\r\n",
      "   2. OnlineSecurity       (CAT): 0.0640\r\n",
      "   3. StreamingTV          (CAT): 0.0636\r\n",
      "   4. TechSupport          (CAT): 0.0580\r\n",
      "   5. MultipleLines        (CAT): 0.0572\r\n",
      "   6. InternetService      (CAT): 0.0567\r\n",
      "   7. SeniorCitizen        (CAT): 0.0554\r\n",
      "   8. Dependents           (CAT): 0.0538\r\n",
      "   9. PaperlessBilling     (CAT): 0.0534\r\n",
      "  10. gender               (CAT): 0.0532\r\n",
      "  11. Partner              (CAT): 0.0519\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges'] → indices [1]\r\n",
      "   - Catégorielles sélectionnées: ['OnlineSecurity', 'StreamingTV', 'TechSupport', 'MultipleLines', 'InternetService', 'SeniorCitizen', 'Dependents', 'PaperlessBilling', 'gender', 'Partner'] → indices [7, 11, 10, 5, 6, 1, 3, 14, 0, 2]\r\n",
      "📊 Features sélectionnées: 1 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 3 paires\r\n",
      "Modèle Random créé avec 61,377 paramètres\r\n",
      "🔗 Sparsité d'attention: 80.56%\r\n",
      "   - Connexions feature-feature: 6\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5686 | Val Loss: 0.5497 | Time: 3.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5497)\r\n",
      "Epoch 001 | Train Loss: 0.5507 | Val Loss: 0.5292 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5292)\r\n",
      "Epoch 002 | Train Loss: 0.5401 | Val Loss: 0.5111 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5111)\r\n",
      "Epoch 003 | Train Loss: 0.5296 | Val Loss: 0.4973 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4973)\r\n",
      "Epoch 004 | Train Loss: 0.5174 | Val Loss: 0.4885 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4885)\r\n",
      "Epoch 005 | Train Loss: 0.5114 | Val Loss: 0.4830 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4830)\r\n",
      "Epoch 006 | Train Loss: 0.5083 | Val Loss: 0.4811 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4811)\r\n",
      "Epoch 007 | Train Loss: 0.5033 | Val Loss: 0.4776 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4776)\r\n",
      "Epoch 008 | Train Loss: 0.4991 | Val Loss: 0.4755 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4755)\r\n",
      "Epoch 009 | Train Loss: 0.4991 | Val Loss: 0.4750 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4750)\r\n",
      "Epoch 010 | Train Loss: 0.5008 | Val Loss: 0.4752 | Time: 3.40s\r\n",
      "Epoch 011 | Train Loss: 0.5004 | Val Loss: 0.4748 | Time: 3.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4748)\r\n",
      "Epoch 012 | Train Loss: 0.4964 | Val Loss: 0.4746 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4746)\r\n",
      "Epoch 013 | Train Loss: 0.4958 | Val Loss: 0.4723 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4723)\r\n",
      "Epoch 014 | Train Loss: 0.4956 | Val Loss: 0.4720 | Time: 3.21s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4720)\r\n",
      "Epoch 015 | Train Loss: 0.4955 | Val Loss: 0.4719 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4719)\r\n",
      "Epoch 016 | Train Loss: 0.4959 | Val Loss: 0.4733 | Time: 3.27s\r\n",
      "Epoch 017 | Train Loss: 0.4965 | Val Loss: 0.4726 | Time: 3.22s\r\n",
      "Epoch 018 | Train Loss: 0.4901 | Val Loss: 0.4743 | Time: 3.35s\r\n",
      "Epoch 019 | Train Loss: 0.4960 | Val Loss: 0.4736 | Time: 3.33s\r\n",
      "Epoch 020 | Train Loss: 0.4920 | Val Loss: 0.4732 | Time: 3.40s\r\n",
      "Epoch 021 | Train Loss: 0.4946 | Val Loss: 0.4725 | Time: 3.22s\r\n",
      "Epoch 022 | Train Loss: 0.4929 | Val Loss: 0.4721 | Time: 3.26s\r\n",
      "Epoch 023 | Train Loss: 0.4920 | Val Loss: 0.4719 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4719)\r\n",
      "Epoch 024 | Train Loss: 0.4913 | Val Loss: 0.4730 | Time: 3.26s\r\n",
      "Epoch 025 | Train Loss: 0.4931 | Val Loss: 0.4727 | Time: 3.26s\r\n",
      "Epoch 026 | Train Loss: 0.4906 | Val Loss: 0.4718 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4718)\r\n",
      "Epoch 027 | Train Loss: 0.4920 | Val Loss: 0.4718 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4718)\r\n",
      "Epoch 028 | Train Loss: 0.4901 | Val Loss: 0.4721 | Time: 3.24s\r\n",
      "Epoch 029 | Train Loss: 0.4914 | Val Loss: 0.4715 | Time: 3.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4715)\r\n",
      "Epoch 030 | Train Loss: 0.4928 | Val Loss: 0.4713 | Time: 3.20s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4713)\r\n",
      "Epoch 031 | Train Loss: 0.4891 | Val Loss: 0.4710 | Time: 3.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4710)\r\n",
      "Epoch 032 | Train Loss: 0.4910 | Val Loss: 0.4711 | Time: 3.23s\r\n",
      "Epoch 033 | Train Loss: 0.4922 | Val Loss: 0.4710 | Time: 3.23s\r\n",
      "Epoch 034 | Train Loss: 0.4897 | Val Loss: 0.4706 | Time: 3.25s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4706)\r\n",
      "Epoch 035 | Train Loss: 0.4894 | Val Loss: 0.4711 | Time: 3.21s\r\n",
      "Epoch 036 | Train Loss: 0.4911 | Val Loss: 0.4713 | Time: 3.18s\r\n",
      "Epoch 037 | Train Loss: 0.4891 | Val Loss: 0.4708 | Time: 3.24s\r\n",
      "Epoch 038 | Train Loss: 0.4866 | Val Loss: 0.4706 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4706)\r\n",
      "Epoch 039 | Train Loss: 0.4850 | Val Loss: 0.4712 | Time: 3.33s\r\n",
      "Epoch 040 | Train Loss: 0.4898 | Val Loss: 0.4701 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4701)\r\n",
      "Epoch 041 | Train Loss: 0.4890 | Val Loss: 0.4707 | Time: 3.22s\r\n",
      "Epoch 042 | Train Loss: 0.4850 | Val Loss: 0.4701 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4701)\r\n",
      "Epoch 043 | Train Loss: 0.4868 | Val Loss: 0.4706 | Time: 3.19s\r\n",
      "Epoch 044 | Train Loss: 0.4844 | Val Loss: 0.4719 | Time: 3.27s\r\n",
      "Epoch 045 | Train Loss: 0.4873 | Val Loss: 0.4715 | Time: 3.24s\r\n",
      "Epoch 046 | Train Loss: 0.4866 | Val Loss: 0.4715 | Time: 3.25s\r\n",
      "Epoch 047 | Train Loss: 0.4861 | Val Loss: 0.4707 | Time: 3.27s\r\n",
      "Epoch 048 | Train Loss: 0.4851 | Val Loss: 0.4713 | Time: 3.24s\r\n",
      "Epoch 049 | Train Loss: 0.4877 | Val Loss: 0.4703 | Time: 3.43s\r\n",
      "✅ Meilleur modèle Random chargé (époque 42, val_loss: 0.4701)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. OnlineSecurity       (CAT): 0.2074\r\n",
      "   2. StreamingTV          (CAT): 0.1908\r\n",
      "   3. Dependents           (CAT): 0.1282\r\n",
      "   4. Partner              (CAT): 0.1276\r\n",
      "   5. MonthlyCharges       (NUM): 0.0513\r\n",
      "   6. MultipleLines        (CAT): 0.0505\r\n",
      "   7. PaperlessBilling     (CAT): 0.0504\r\n",
      "   8. InternetService      (CAT): 0.0494\r\n",
      "   9. SeniorCitizen        (CAT): 0.0485\r\n",
      "  10. TechSupport          (CAT): 0.0481\r\n",
      "  11. gender               (CAT): 0.0478\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. OnlineSecurity      : 0.2074\r\n",
      "   2. StreamingTV         : 0.1908\r\n",
      "   3. Dependents          : 0.1282\r\n",
      "   4. Partner             : 0.1276\r\n",
      "   5. MonthlyCharges      : 0.0513\r\n",
      "   6. MultipleLines       : 0.0505\r\n",
      "   7. PaperlessBilling    : 0.0504\r\n",
      "   8. InternetService     : 0.0494\r\n",
      "   9. SeniorCitizen       : 0.0485\r\n",
      "  10. TechSupport         : 0.0481\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 452.0s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6030 | Val Loss: 0.5721 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5721)\r\n",
      "Epoch 001 | Train Loss: 0.5690 | Val Loss: 0.5491 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5491)\r\n",
      "Epoch 002 | Train Loss: 0.5521 | Val Loss: 0.5289 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5289)\r\n",
      "Epoch 003 | Train Loss: 0.5315 | Val Loss: 0.5038 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5038)\r\n",
      "Epoch 004 | Train Loss: 0.5079 | Val Loss: 0.4821 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4821)\r\n",
      "Epoch 005 | Train Loss: 0.4963 | Val Loss: 0.4652 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4652)\r\n",
      "Epoch 006 | Train Loss: 0.4847 | Val Loss: 0.4556 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4556)\r\n",
      "Epoch 007 | Train Loss: 0.4758 | Val Loss: 0.4493 | Time: 5.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4493)\r\n",
      "Epoch 008 | Train Loss: 0.4671 | Val Loss: 0.4446 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4446)\r\n",
      "Epoch 009 | Train Loss: 0.4674 | Val Loss: 0.4412 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4412)\r\n",
      "Epoch 010 | Train Loss: 0.4642 | Val Loss: 0.4377 | Time: 5.85s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4377)\r\n",
      "Epoch 011 | Train Loss: 0.4639 | Val Loss: 0.4369 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4369)\r\n",
      "Epoch 012 | Train Loss: 0.4585 | Val Loss: 0.4345 | Time: 5.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4345)\r\n",
      "Epoch 013 | Train Loss: 0.4578 | Val Loss: 0.4330 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4330)\r\n",
      "Epoch 014 | Train Loss: 0.4542 | Val Loss: 0.4311 | Time: 5.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4311)\r\n",
      "Epoch 015 | Train Loss: 0.4539 | Val Loss: 0.4291 | Time: 6.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4291)\r\n",
      "Epoch 016 | Train Loss: 0.4471 | Val Loss: 0.4289 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 017 | Train Loss: 0.4513 | Val Loss: 0.4283 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 018 | Train Loss: 0.4503 | Val Loss: 0.4271 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4271)\r\n",
      "Epoch 019 | Train Loss: 0.4473 | Val Loss: 0.4254 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 020 | Train Loss: 0.4472 | Val Loss: 0.4255 | Time: 5.79s\r\n",
      "Epoch 021 | Train Loss: 0.4425 | Val Loss: 0.4248 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4248)\r\n",
      "Epoch 022 | Train Loss: 0.4428 | Val Loss: 0.4241 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4241)\r\n",
      "Epoch 023 | Train Loss: 0.4456 | Val Loss: 0.4235 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4235)\r\n",
      "Epoch 024 | Train Loss: 0.4420 | Val Loss: 0.4236 | Time: 5.73s\r\n",
      "Epoch 025 | Train Loss: 0.4408 | Val Loss: 0.4231 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4231)\r\n",
      "Epoch 026 | Train Loss: 0.4413 | Val Loss: 0.4218 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4218)\r\n",
      "Epoch 027 | Train Loss: 0.4408 | Val Loss: 0.4215 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4215)\r\n",
      "Epoch 028 | Train Loss: 0.4384 | Val Loss: 0.4216 | Time: 5.72s\r\n",
      "Epoch 029 | Train Loss: 0.4367 | Val Loss: 0.4210 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4210)\r\n",
      "Epoch 030 | Train Loss: 0.4377 | Val Loss: 0.4211 | Time: 5.72s\r\n",
      "Epoch 031 | Train Loss: 0.4348 | Val Loss: 0.4207 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4207)\r\n",
      "Epoch 032 | Train Loss: 0.4380 | Val Loss: 0.4204 | Time: 5.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 033 | Train Loss: 0.4360 | Val Loss: 0.4192 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 034 | Train Loss: 0.4365 | Val Loss: 0.4187 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4187)\r\n",
      "Epoch 035 | Train Loss: 0.4356 | Val Loss: 0.4184 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4184)\r\n",
      "Epoch 036 | Train Loss: 0.4316 | Val Loss: 0.4184 | Time: 5.69s\r\n",
      "Epoch 037 | Train Loss: 0.4354 | Val Loss: 0.4183 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 038 | Train Loss: 0.4311 | Val Loss: 0.4173 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4173)\r\n",
      "Epoch 039 | Train Loss: 0.4325 | Val Loss: 0.4167 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4167)\r\n",
      "Epoch 040 | Train Loss: 0.4329 | Val Loss: 0.4169 | Time: 5.69s\r\n",
      "Epoch 041 | Train Loss: 0.4290 | Val Loss: 0.4164 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4164)\r\n",
      "Epoch 042 | Train Loss: 0.4316 | Val Loss: 0.4165 | Time: 5.67s\r\n",
      "Epoch 043 | Train Loss: 0.4307 | Val Loss: 0.4158 | Time: 5.97s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4158)\r\n",
      "Epoch 044 | Train Loss: 0.4268 | Val Loss: 0.4157 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4157)\r\n",
      "Epoch 045 | Train Loss: 0.4256 | Val Loss: 0.4153 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4153)\r\n",
      "Epoch 046 | Train Loss: 0.4270 | Val Loss: 0.4154 | Time: 5.74s\r\n",
      "Epoch 047 | Train Loss: 0.4288 | Val Loss: 0.4151 | Time: 5.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4151)\r\n",
      "Epoch 048 | Train Loss: 0.4300 | Val Loss: 0.4150 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4150)\r\n",
      "Epoch 049 | Train Loss: 0.4272 | Val Loss: 0.4147 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4147)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4147)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PhoneService        : 0.0751\r\n",
      "   2. DeviceProtection    : 0.0687\r\n",
      "   3. StreamingTV         : 0.0629\r\n",
      "   4. PaperlessBilling    : 0.0601\r\n",
      "   5. gender              : 0.0589\r\n",
      "   6. MultipleLines       : 0.0571\r\n",
      "   7. OnlineSecurity      : 0.0565\r\n",
      "   8. Partner             : 0.0547\r\n",
      "   9. StreamingMovies     : 0.0540\r\n",
      "  10. OnlineBackup        : 0.0533\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 11 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PhoneService         (CAT): 0.0751\r\n",
      "   2. DeviceProtection     (CAT): 0.0687\r\n",
      "   3. StreamingTV          (CAT): 0.0629\r\n",
      "   4. PaperlessBilling     (CAT): 0.0601\r\n",
      "   5. gender               (CAT): 0.0589\r\n",
      "   6. MultipleLines        (CAT): 0.0571\r\n",
      "   7. OnlineSecurity       (CAT): 0.0565\r\n",
      "   8. Partner              (CAT): 0.0547\r\n",
      "   9. StreamingMovies      (CAT): 0.0540\r\n",
      "  10. OnlineBackup         (CAT): 0.0533\r\n",
      "  11. TotalCharges         (NUM): 0.0500\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges'] → indices [2]\r\n",
      "   - Catégorielles sélectionnées: ['PhoneService', 'DeviceProtection', 'StreamingTV', 'PaperlessBilling', 'gender', 'MultipleLines', 'OnlineSecurity', 'Partner', 'StreamingMovies', 'OnlineBackup'] → indices [4, 9, 11, 14, 0, 5, 7, 2, 12, 8]\r\n",
      "📊 Features sélectionnées: 1 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 3 paires\r\n",
      "Modèle Random créé avec 61,441 paramètres\r\n",
      "🔗 Sparsité d'attention: 80.56%\r\n",
      "   - Connexions feature-feature: 6\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5755 | Val Loss: 0.5312 | Time: 3.27s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5312)\r\n",
      "Epoch 001 | Train Loss: 0.5405 | Val Loss: 0.4989 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4989)\r\n",
      "Epoch 002 | Train Loss: 0.5257 | Val Loss: 0.4799 | Time: 3.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4799)\r\n",
      "Epoch 003 | Train Loss: 0.5111 | Val Loss: 0.4682 | Time: 3.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4682)\r\n",
      "Epoch 004 | Train Loss: 0.5072 | Val Loss: 0.4608 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4608)\r\n",
      "Epoch 005 | Train Loss: 0.4994 | Val Loss: 0.4560 | Time: 3.18s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4560)\r\n",
      "Epoch 006 | Train Loss: 0.4951 | Val Loss: 0.4533 | Time: 3.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4533)\r\n",
      "Epoch 007 | Train Loss: 0.4904 | Val Loss: 0.4522 | Time: 3.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4522)\r\n",
      "Epoch 008 | Train Loss: 0.4892 | Val Loss: 0.4505 | Time: 3.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4505)\r\n",
      "Epoch 009 | Train Loss: 0.4843 | Val Loss: 0.4504 | Time: 3.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4504)\r\n",
      "Epoch 010 | Train Loss: 0.4842 | Val Loss: 0.4512 | Time: 3.23s\r\n",
      "Epoch 011 | Train Loss: 0.4806 | Val Loss: 0.4512 | Time: 3.24s\r\n",
      "Epoch 012 | Train Loss: 0.4789 | Val Loss: 0.4517 | Time: 3.25s\r\n",
      "Epoch 013 | Train Loss: 0.4828 | Val Loss: 0.4514 | Time: 3.24s\r\n",
      "Epoch 014 | Train Loss: 0.4809 | Val Loss: 0.4526 | Time: 3.21s\r\n",
      "Epoch 015 | Train Loss: 0.4860 | Val Loss: 0.4522 | Time: 3.26s\r\n",
      "Epoch 016 | Train Loss: 0.4822 | Val Loss: 0.4513 | Time: 3.24s\r\n",
      "Epoch 017 | Train Loss: 0.4793 | Val Loss: 0.4514 | Time: 3.37s\r\n",
      "Epoch 018 | Train Loss: 0.4776 | Val Loss: 0.4512 | Time: 3.27s\r\n",
      "Epoch 019 | Train Loss: 0.4760 | Val Loss: 0.4534 | Time: 3.22s\r\n",
      "Epoch 020 | Train Loss: 0.4743 | Val Loss: 0.4548 | Time: 3.24s\r\n",
      "Epoch 021 | Train Loss: 0.4757 | Val Loss: 0.4548 | Time: 3.26s\r\n",
      "Epoch 022 | Train Loss: 0.4822 | Val Loss: 0.4525 | Time: 3.24s\r\n",
      "Epoch 023 | Train Loss: 0.4779 | Val Loss: 0.4524 | Time: 3.21s\r\n",
      "Epoch 024 | Train Loss: 0.4805 | Val Loss: 0.4527 | Time: 3.22s\r\n",
      "Epoch 025 | Train Loss: 0.4767 | Val Loss: 0.4534 | Time: 3.24s\r\n",
      "Epoch 026 | Train Loss: 0.4785 | Val Loss: 0.4535 | Time: 3.28s\r\n",
      "Epoch 027 | Train Loss: 0.4768 | Val Loss: 0.4514 | Time: 3.29s\r\n",
      "Epoch 028 | Train Loss: 0.4771 | Val Loss: 0.4522 | Time: 3.31s\r\n",
      "Epoch 029 | Train Loss: 0.4762 | Val Loss: 0.4509 | Time: 3.31s\r\n",
      "Epoch 030 | Train Loss: 0.4761 | Val Loss: 0.4513 | Time: 3.28s\r\n",
      "Epoch 031 | Train Loss: 0.4745 | Val Loss: 0.4523 | Time: 3.23s\r\n",
      "Epoch 032 | Train Loss: 0.4750 | Val Loss: 0.4512 | Time: 3.24s\r\n",
      "\r\n",
      "Early stopping à l'époque 32 (patience: 23)\r\n",
      "✅ Meilleur modèle Random chargé (époque 9, val_loss: 0.4504)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. StreamingMovies      (CAT): 0.1680\r\n",
      "   2. TotalCharges         (NUM): 0.1334\r\n",
      "   3. PhoneService         (CAT): 0.1328\r\n",
      "   4. DeviceProtection     (CAT): 0.1152\r\n",
      "   5. StreamingTV          (CAT): 0.0857\r\n",
      "   6. MultipleLines        (CAT): 0.0623\r\n",
      "   7. OnlineSecurity       (CAT): 0.0612\r\n",
      "   8. PaperlessBilling     (CAT): 0.0612\r\n",
      "   9. OnlineBackup         (CAT): 0.0611\r\n",
      "  10. Partner              (CAT): 0.0602\r\n",
      "  11. gender               (CAT): 0.0589\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. StreamingMovies     : 0.1680\r\n",
      "   2. TotalCharges        : 0.1334\r\n",
      "   3. PhoneService        : 0.1328\r\n",
      "   4. DeviceProtection    : 0.1152\r\n",
      "   5. StreamingTV         : 0.0857\r\n",
      "   6. MultipleLines       : 0.0623\r\n",
      "   7. OnlineSecurity      : 0.0612\r\n",
      "   8. PaperlessBilling    : 0.0612\r\n",
      "   9. OnlineBackup        : 0.0611\r\n",
      "  10. Partner             : 0.0602\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_22/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 397.9s ===\r\n",
      "\u001b[32m[I 2025-07-20 01:33:05,194]\u001b[0m Trial 22 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 2, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.13016518062188578, 'ffn_dropout_stage1': 0.12010921122257459, 'residual_dropout_stage1': 0.114034715781325, 'lr_stage1': 2.9191459665515695e-05, 'weight_decay_stage1': 0.011669414450976936, 'd_token_stage2': 64, 'n_blocks_stage2': 2, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.1259175987442271, 'ffn_dropout_stage2': 0.24360874153944323, 'residual_dropout_stage2': 0.1651381396298865, 'lr_stage2': 2.8567464873461746e-05, 'weight_decay_stage2': 0.00034051252445714764, 'batch_size': 32, 'patience': 23, 'embedding_type': 'T-LR', 'M': 11, 'k': 3}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  92%|████████▎| 23/25 [7:20:07<38:57, 1168.95s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 43,249 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6466 | Val Loss: 0.5884 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5884)\r\n",
      "Epoch 001 | Train Loss: 0.5826 | Val Loss: 0.5611 | Time: 8.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5611)\r\n",
      "Epoch 002 | Train Loss: 0.5628 | Val Loss: 0.5389 | Time: 8.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5389)\r\n",
      "Epoch 003 | Train Loss: 0.5434 | Val Loss: 0.5149 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5149)\r\n",
      "Epoch 004 | Train Loss: 0.5243 | Val Loss: 0.4941 | Time: 8.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4941)\r\n",
      "Epoch 005 | Train Loss: 0.5073 | Val Loss: 0.4804 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4804)\r\n",
      "Epoch 006 | Train Loss: 0.4970 | Val Loss: 0.4729 | Time: 8.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4729)\r\n",
      "Epoch 007 | Train Loss: 0.4912 | Val Loss: 0.4680 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4680)\r\n",
      "Epoch 008 | Train Loss: 0.4832 | Val Loss: 0.4642 | Time: 8.49s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4642)\r\n",
      "Epoch 009 | Train Loss: 0.4789 | Val Loss: 0.4605 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4605)\r\n",
      "Epoch 010 | Train Loss: 0.4753 | Val Loss: 0.4582 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4582)\r\n",
      "Epoch 011 | Train Loss: 0.4703 | Val Loss: 0.4561 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4561)\r\n",
      "Epoch 012 | Train Loss: 0.4709 | Val Loss: 0.4533 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4533)\r\n",
      "Epoch 013 | Train Loss: 0.4652 | Val Loss: 0.4516 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4516)\r\n",
      "Epoch 014 | Train Loss: 0.4660 | Val Loss: 0.4501 | Time: 8.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4501)\r\n",
      "Epoch 015 | Train Loss: 0.4608 | Val Loss: 0.4495 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4495)\r\n",
      "Epoch 016 | Train Loss: 0.4583 | Val Loss: 0.4485 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4485)\r\n",
      "Epoch 017 | Train Loss: 0.4544 | Val Loss: 0.4463 | Time: 8.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4463)\r\n",
      "Epoch 018 | Train Loss: 0.4542 | Val Loss: 0.4448 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4448)\r\n",
      "Epoch 019 | Train Loss: 0.4553 | Val Loss: 0.4435 | Time: 8.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4435)\r\n",
      "Epoch 020 | Train Loss: 0.4532 | Val Loss: 0.4419 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "Epoch 021 | Train Loss: 0.4515 | Val Loss: 0.4402 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4402)\r\n",
      "Epoch 022 | Train Loss: 0.4499 | Val Loss: 0.4400 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4400)\r\n",
      "Epoch 023 | Train Loss: 0.4517 | Val Loss: 0.4390 | Time: 8.48s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4390)\r\n",
      "Epoch 024 | Train Loss: 0.4495 | Val Loss: 0.4372 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4372)\r\n",
      "Epoch 025 | Train Loss: 0.4426 | Val Loss: 0.4381 | Time: 8.36s\r\n",
      "Epoch 026 | Train Loss: 0.4440 | Val Loss: 0.4368 | Time: 8.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 027 | Train Loss: 0.4455 | Val Loss: 0.4366 | Time: 8.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4366)\r\n",
      "Epoch 028 | Train Loss: 0.4450 | Val Loss: 0.4341 | Time: 8.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4341)\r\n",
      "Epoch 029 | Train Loss: 0.4400 | Val Loss: 0.4350 | Time: 8.30s\r\n",
      "Epoch 030 | Train Loss: 0.4414 | Val Loss: 0.4346 | Time: 8.43s\r\n",
      "Epoch 031 | Train Loss: 0.4427 | Val Loss: 0.4354 | Time: 8.52s\r\n",
      "Epoch 032 | Train Loss: 0.4431 | Val Loss: 0.4336 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4336)\r\n",
      "Epoch 033 | Train Loss: 0.4383 | Val Loss: 0.4334 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4334)\r\n",
      "Epoch 034 | Train Loss: 0.4389 | Val Loss: 0.4331 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4331)\r\n",
      "Epoch 035 | Train Loss: 0.4385 | Val Loss: 0.4317 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4317)\r\n",
      "Epoch 036 | Train Loss: 0.4351 | Val Loss: 0.4309 | Time: 8.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4309)\r\n",
      "Epoch 037 | Train Loss: 0.4375 | Val Loss: 0.4305 | Time: 8.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4305)\r\n",
      "Epoch 038 | Train Loss: 0.4354 | Val Loss: 0.4300 | Time: 8.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4300)\r\n",
      "Epoch 039 | Train Loss: 0.4363 | Val Loss: 0.4288 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4288)\r\n",
      "Epoch 040 | Train Loss: 0.4344 | Val Loss: 0.4283 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 041 | Train Loss: 0.4356 | Val Loss: 0.4270 | Time: 8.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4270)\r\n",
      "Epoch 042 | Train Loss: 0.4324 | Val Loss: 0.4270 | Time: 8.50s\r\n",
      "Epoch 043 | Train Loss: 0.4335 | Val Loss: 0.4274 | Time: 8.39s\r\n",
      "Epoch 044 | Train Loss: 0.4314 | Val Loss: 0.4264 | Time: 8.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4264)\r\n",
      "Epoch 045 | Train Loss: 0.4337 | Val Loss: 0.4280 | Time: 8.34s\r\n",
      "Epoch 046 | Train Loss: 0.4323 | Val Loss: 0.4263 | Time: 8.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4263)\r\n",
      "Epoch 047 | Train Loss: 0.4331 | Val Loss: 0.4255 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4255)\r\n",
      "Epoch 048 | Train Loss: 0.4308 | Val Loss: 0.4254 | Time: 8.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 049 | Train Loss: 0.4294 | Val Loss: 0.4239 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4239)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4239)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Partner             : 0.0556\r\n",
      "   2. tenure              : 0.0552\r\n",
      "   3. TotalCharges        : 0.0549\r\n",
      "   4. MonthlyCharges      : 0.0548\r\n",
      "   5. OnlineBackup        : 0.0545\r\n",
      "   6. DeviceProtection    : 0.0543\r\n",
      "   7. Dependents          : 0.0539\r\n",
      "   8. PaperlessBilling    : 0.0534\r\n",
      "   9. OnlineSecurity      : 0.0534\r\n",
      "  10. gender              : 0.0527\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 13 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Partner              (CAT): 0.0556\r\n",
      "   2. tenure               (NUM): 0.0552\r\n",
      "   3. TotalCharges         (NUM): 0.0549\r\n",
      "   4. MonthlyCharges       (NUM): 0.0548\r\n",
      "   5. OnlineBackup         (CAT): 0.0545\r\n",
      "   6. DeviceProtection     (CAT): 0.0543\r\n",
      "   7. Dependents           (CAT): 0.0539\r\n",
      "   8. PaperlessBilling     (CAT): 0.0534\r\n",
      "   9. OnlineSecurity       (CAT): 0.0534\r\n",
      "  10. gender               (CAT): 0.0527\r\n",
      "  11. InternetService      (CAT): 0.0525\r\n",
      "  12. PaymentMethod        (CAT): 0.0525\r\n",
      "  13. TechSupport          (CAT): 0.0519\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['Partner', 'OnlineBackup', 'DeviceProtection', 'Dependents', 'PaperlessBilling', 'OnlineSecurity', 'gender', 'InternetService', 'PaymentMethod', 'TechSupport'] → indices [2, 8, 9, 3, 14, 7, 0, 6, 15, 10]\r\n",
      "📊 Features sélectionnées: 3 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 91,137 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.63%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6956 | Val Loss: 0.5857 | Time: 4.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5857)\r\n",
      "Epoch 001 | Train Loss: 0.5908 | Val Loss: 0.5256 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5256)\r\n",
      "Epoch 002 | Train Loss: 0.5551 | Val Loss: 0.5038 | Time: 4.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5038)\r\n",
      "Epoch 003 | Train Loss: 0.5336 | Val Loss: 0.4866 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4866)\r\n",
      "Epoch 004 | Train Loss: 0.5207 | Val Loss: 0.4730 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4730)\r\n",
      "Epoch 005 | Train Loss: 0.5054 | Val Loss: 0.4614 | Time: 4.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4614)\r\n",
      "Epoch 006 | Train Loss: 0.4951 | Val Loss: 0.4547 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4547)\r\n",
      "Epoch 007 | Train Loss: 0.4915 | Val Loss: 0.4506 | Time: 4.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4506)\r\n",
      "Epoch 008 | Train Loss: 0.4811 | Val Loss: 0.4476 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4476)\r\n",
      "Epoch 009 | Train Loss: 0.4741 | Val Loss: 0.4457 | Time: 4.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4457)\r\n",
      "Epoch 010 | Train Loss: 0.4745 | Val Loss: 0.4436 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4436)\r\n",
      "Epoch 011 | Train Loss: 0.4647 | Val Loss: 0.4403 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4403)\r\n",
      "Epoch 012 | Train Loss: 0.4628 | Val Loss: 0.4388 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4388)\r\n",
      "Epoch 013 | Train Loss: 0.4616 | Val Loss: 0.4381 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4381)\r\n",
      "Epoch 014 | Train Loss: 0.4605 | Val Loss: 0.4390 | Time: 4.72s\r\n",
      "Epoch 015 | Train Loss: 0.4576 | Val Loss: 0.4382 | Time: 4.66s\r\n",
      "Epoch 016 | Train Loss: 0.4607 | Val Loss: 0.4375 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4375)\r\n",
      "Epoch 017 | Train Loss: 0.4555 | Val Loss: 0.4369 | Time: 4.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4369)\r\n",
      "Epoch 018 | Train Loss: 0.4537 | Val Loss: 0.4352 | Time: 4.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4352)\r\n",
      "Epoch 019 | Train Loss: 0.4575 | Val Loss: 0.4350 | Time: 4.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4350)\r\n",
      "Epoch 020 | Train Loss: 0.4553 | Val Loss: 0.4349 | Time: 4.93s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4349)\r\n",
      "Epoch 021 | Train Loss: 0.4532 | Val Loss: 0.4344 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4344)\r\n",
      "Epoch 022 | Train Loss: 0.4558 | Val Loss: 0.4342 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4342)\r\n",
      "Epoch 023 | Train Loss: 0.4520 | Val Loss: 0.4337 | Time: 4.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4337)\r\n",
      "Epoch 024 | Train Loss: 0.4537 | Val Loss: 0.4334 | Time: 4.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4334)\r\n",
      "Epoch 025 | Train Loss: 0.4510 | Val Loss: 0.4321 | Time: 4.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4321)\r\n",
      "Epoch 026 | Train Loss: 0.4492 | Val Loss: 0.4318 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4318)\r\n",
      "Epoch 027 | Train Loss: 0.4481 | Val Loss: 0.4310 | Time: 4.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4310)\r\n",
      "Epoch 028 | Train Loss: 0.4493 | Val Loss: 0.4300 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4300)\r\n",
      "Epoch 029 | Train Loss: 0.4521 | Val Loss: 0.4295 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4295)\r\n",
      "Epoch 030 | Train Loss: 0.4471 | Val Loss: 0.4291 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4291)\r\n",
      "Epoch 031 | Train Loss: 0.4472 | Val Loss: 0.4290 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4290)\r\n",
      "Epoch 032 | Train Loss: 0.4477 | Val Loss: 0.4293 | Time: 4.57s\r\n",
      "Epoch 033 | Train Loss: 0.4458 | Val Loss: 0.4277 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4277)\r\n",
      "Epoch 034 | Train Loss: 0.4502 | Val Loss: 0.4284 | Time: 4.66s\r\n",
      "Epoch 035 | Train Loss: 0.4467 | Val Loss: 0.4283 | Time: 4.69s\r\n",
      "Epoch 036 | Train Loss: 0.4453 | Val Loss: 0.4287 | Time: 4.67s\r\n",
      "Epoch 037 | Train Loss: 0.4474 | Val Loss: 0.4290 | Time: 4.66s\r\n",
      "Epoch 038 | Train Loss: 0.4419 | Val Loss: 0.4291 | Time: 4.68s\r\n",
      "Epoch 039 | Train Loss: 0.4417 | Val Loss: 0.4293 | Time: 4.65s\r\n",
      "Epoch 040 | Train Loss: 0.4465 | Val Loss: 0.4290 | Time: 4.78s\r\n",
      "Epoch 041 | Train Loss: 0.4426 | Val Loss: 0.4273 | Time: 4.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4273)\r\n",
      "Epoch 042 | Train Loss: 0.4438 | Val Loss: 0.4274 | Time: 4.70s\r\n",
      "Epoch 043 | Train Loss: 0.4448 | Val Loss: 0.4275 | Time: 4.66s\r\n",
      "Epoch 044 | Train Loss: 0.4449 | Val Loss: 0.4276 | Time: 4.79s\r\n",
      "Epoch 045 | Train Loss: 0.4389 | Val Loss: 0.4279 | Time: 4.67s\r\n",
      "Epoch 046 | Train Loss: 0.4429 | Val Loss: 0.4276 | Time: 4.70s\r\n",
      "Epoch 047 | Train Loss: 0.4420 | Val Loss: 0.4268 | Time: 4.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4268)\r\n",
      "Epoch 048 | Train Loss: 0.4403 | Val Loss: 0.4264 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4264)\r\n",
      "Epoch 049 | Train Loss: 0.4418 | Val Loss: 0.4263 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4263)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4263)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. DeviceProtection     (CAT): 0.1725\r\n",
      "   2. OnlineSecurity       (CAT): 0.0811\r\n",
      "   3. PaperlessBilling     (CAT): 0.0810\r\n",
      "   4. PaymentMethod        (CAT): 0.0793\r\n",
      "   5. Partner              (CAT): 0.0770\r\n",
      "   6. InternetService      (CAT): 0.0755\r\n",
      "   7. Dependents           (CAT): 0.0691\r\n",
      "   8. TechSupport          (CAT): 0.0674\r\n",
      "   9. tenure               (NUM): 0.0649\r\n",
      "  10. OnlineBackup         (CAT): 0.0597\r\n",
      "  11. gender               (CAT): 0.0594\r\n",
      "  12. MonthlyCharges       (NUM): 0.0587\r\n",
      "  13. TotalCharges         (NUM): 0.0544\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. DeviceProtection    : 0.1725\r\n",
      "   2. OnlineSecurity      : 0.0811\r\n",
      "   3. PaperlessBilling    : 0.0810\r\n",
      "   4. PaymentMethod       : 0.0793\r\n",
      "   5. Partner             : 0.0770\r\n",
      "   6. InternetService     : 0.0755\r\n",
      "   7. Dependents          : 0.0691\r\n",
      "   8. TechSupport         : 0.0674\r\n",
      "   9. tenure              : 0.0649\r\n",
      "  10. OnlineBackup        : 0.0597\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 658.7s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 43,249 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6285 | Val Loss: 0.5934 | Time: 8.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5934)\r\n",
      "Epoch 001 | Train Loss: 0.5749 | Val Loss: 0.5499 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5499)\r\n",
      "Epoch 002 | Train Loss: 0.5386 | Val Loss: 0.5127 | Time: 8.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5127)\r\n",
      "Epoch 003 | Train Loss: 0.5114 | Val Loss: 0.4916 | Time: 8.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4916)\r\n",
      "Epoch 004 | Train Loss: 0.4931 | Val Loss: 0.4812 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4812)\r\n",
      "Epoch 005 | Train Loss: 0.4837 | Val Loss: 0.4727 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4727)\r\n",
      "Epoch 006 | Train Loss: 0.4789 | Val Loss: 0.4691 | Time: 8.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4691)\r\n",
      "Epoch 007 | Train Loss: 0.4725 | Val Loss: 0.4663 | Time: 8.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4663)\r\n",
      "Epoch 008 | Train Loss: 0.4691 | Val Loss: 0.4649 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4649)\r\n",
      "Epoch 009 | Train Loss: 0.4667 | Val Loss: 0.4621 | Time: 8.52s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4621)\r\n",
      "Epoch 010 | Train Loss: 0.4590 | Val Loss: 0.4580 | Time: 8.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4580)\r\n",
      "Epoch 011 | Train Loss: 0.4587 | Val Loss: 0.4562 | Time: 8.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4562)\r\n",
      "Epoch 012 | Train Loss: 0.4575 | Val Loss: 0.4542 | Time: 8.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4542)\r\n",
      "Epoch 013 | Train Loss: 0.4525 | Val Loss: 0.4553 | Time: 8.46s\r\n",
      "Epoch 014 | Train Loss: 0.4517 | Val Loss: 0.4534 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4534)\r\n",
      "Epoch 015 | Train Loss: 0.4496 | Val Loss: 0.4531 | Time: 8.36s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4531)\r\n",
      "Epoch 016 | Train Loss: 0.4505 | Val Loss: 0.4509 | Time: 8.54s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4509)\r\n",
      "Epoch 017 | Train Loss: 0.4504 | Val Loss: 0.4504 | Time: 8.59s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4504)\r\n",
      "Epoch 018 | Train Loss: 0.4458 | Val Loss: 0.4485 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4485)\r\n",
      "Epoch 019 | Train Loss: 0.4442 | Val Loss: 0.4477 | Time: 8.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4477)\r\n",
      "Epoch 020 | Train Loss: 0.4445 | Val Loss: 0.4469 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4469)\r\n",
      "Epoch 021 | Train Loss: 0.4431 | Val Loss: 0.4465 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4465)\r\n",
      "Epoch 022 | Train Loss: 0.4418 | Val Loss: 0.4441 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4441)\r\n",
      "Epoch 023 | Train Loss: 0.4422 | Val Loss: 0.4438 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4438)\r\n",
      "Epoch 024 | Train Loss: 0.4395 | Val Loss: 0.4442 | Time: 8.60s\r\n",
      "Epoch 025 | Train Loss: 0.4372 | Val Loss: 0.4414 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4414)\r\n",
      "Epoch 026 | Train Loss: 0.4372 | Val Loss: 0.4410 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4410)\r\n",
      "Epoch 027 | Train Loss: 0.4344 | Val Loss: 0.4406 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4406)\r\n",
      "Epoch 028 | Train Loss: 0.4325 | Val Loss: 0.4388 | Time: 8.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4388)\r\n",
      "Epoch 029 | Train Loss: 0.4330 | Val Loss: 0.4377 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4377)\r\n",
      "Epoch 030 | Train Loss: 0.4336 | Val Loss: 0.4365 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4365)\r\n",
      "Epoch 031 | Train Loss: 0.4300 | Val Loss: 0.4380 | Time: 8.37s\r\n",
      "Epoch 032 | Train Loss: 0.4305 | Val Loss: 0.4359 | Time: 8.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4359)\r\n",
      "Epoch 033 | Train Loss: 0.4305 | Val Loss: 0.4376 | Time: 8.43s\r\n",
      "Epoch 034 | Train Loss: 0.4302 | Val Loss: 0.4344 | Time: 8.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4344)\r\n",
      "Epoch 035 | Train Loss: 0.4284 | Val Loss: 0.4351 | Time: 8.50s\r\n",
      "Epoch 036 | Train Loss: 0.4275 | Val Loss: 0.4345 | Time: 8.39s\r\n",
      "Epoch 037 | Train Loss: 0.4260 | Val Loss: 0.4331 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4331)\r\n",
      "Epoch 038 | Train Loss: 0.4299 | Val Loss: 0.4329 | Time: 8.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4329)\r\n",
      "Epoch 039 | Train Loss: 0.4264 | Val Loss: 0.4322 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4322)\r\n",
      "Epoch 040 | Train Loss: 0.4243 | Val Loss: 0.4319 | Time: 8.33s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4319)\r\n",
      "Epoch 041 | Train Loss: 0.4233 | Val Loss: 0.4322 | Time: 8.32s\r\n",
      "Epoch 042 | Train Loss: 0.4248 | Val Loss: 0.4313 | Time: 8.30s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4313)\r\n",
      "Epoch 043 | Train Loss: 0.4218 | Val Loss: 0.4307 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4307)\r\n",
      "Epoch 044 | Train Loss: 0.4224 | Val Loss: 0.4306 | Time: 8.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4306)\r\n",
      "Epoch 045 | Train Loss: 0.4230 | Val Loss: 0.4304 | Time: 8.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4304)\r\n",
      "Epoch 046 | Train Loss: 0.4243 | Val Loss: 0.4297 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 047 | Train Loss: 0.4213 | Val Loss: 0.4288 | Time: 8.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4288)\r\n",
      "Epoch 048 | Train Loss: 0.4220 | Val Loss: 0.4294 | Time: 8.34s\r\n",
      "Epoch 049 | Train Loss: 0.4186 | Val Loss: 0.4291 | Time: 8.41s\r\n",
      "✅ Meilleur modèle chargé (époque 47, val_loss: 0.4288)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.0598\r\n",
      "   2. OnlineSecurity      : 0.0582\r\n",
      "   3. Dependents          : 0.0564\r\n",
      "   4. DeviceProtection    : 0.0559\r\n",
      "   5. MonthlyCharges      : 0.0556\r\n",
      "   6. SeniorCitizen       : 0.0551\r\n",
      "   7. InternetService     : 0.0541\r\n",
      "   8. gender              : 0.0518\r\n",
      "   9. StreamingTV         : 0.0517\r\n",
      "  10. PaperlessBilling    : 0.0514\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 13 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. Contract             (CAT): 0.0598\r\n",
      "   2. OnlineSecurity       (CAT): 0.0582\r\n",
      "   3. Dependents           (CAT): 0.0564\r\n",
      "   4. DeviceProtection     (CAT): 0.0559\r\n",
      "   5. MonthlyCharges       (NUM): 0.0556\r\n",
      "   6. SeniorCitizen        (CAT): 0.0551\r\n",
      "   7. InternetService      (CAT): 0.0541\r\n",
      "   8. gender               (CAT): 0.0518\r\n",
      "   9. StreamingTV          (CAT): 0.0517\r\n",
      "  10. PaperlessBilling     (CAT): 0.0514\r\n",
      "  11. TechSupport          (CAT): 0.0514\r\n",
      "  12. tenure               (NUM): 0.0510\r\n",
      "  13. PhoneService         (CAT): 0.0507\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['MonthlyCharges', 'tenure'] → indices [1, 0]\r\n",
      "   - Catégorielles sélectionnées: ['Contract', 'OnlineSecurity', 'Dependents', 'DeviceProtection', 'SeniorCitizen', 'InternetService', 'gender', 'StreamingTV', 'PaperlessBilling', 'TechSupport', 'PhoneService'] → indices [13, 7, 3, 9, 1, 6, 0, 11, 14, 10, 4]\r\n",
      "📊 Features sélectionnées: 2 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 91,137 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.63%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6551 | Val Loss: 0.5838 | Time: 4.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5838)\r\n",
      "Epoch 001 | Train Loss: 0.5781 | Val Loss: 0.5504 | Time: 4.84s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5504)\r\n",
      "Epoch 002 | Train Loss: 0.5494 | Val Loss: 0.5260 | Time: 4.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5260)\r\n",
      "Epoch 003 | Train Loss: 0.5291 | Val Loss: 0.5058 | Time: 4.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5058)\r\n",
      "Epoch 004 | Train Loss: 0.5129 | Val Loss: 0.4880 | Time: 4.63s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4880)\r\n",
      "Epoch 005 | Train Loss: 0.5022 | Val Loss: 0.4799 | Time: 4.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4799)\r\n",
      "Epoch 006 | Train Loss: 0.4937 | Val Loss: 0.4732 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4732)\r\n",
      "Epoch 007 | Train Loss: 0.4881 | Val Loss: 0.4696 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4696)\r\n",
      "Epoch 008 | Train Loss: 0.4806 | Val Loss: 0.4665 | Time: 4.89s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4665)\r\n",
      "Epoch 009 | Train Loss: 0.4792 | Val Loss: 0.4637 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4637)\r\n",
      "Epoch 010 | Train Loss: 0.4720 | Val Loss: 0.4598 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4598)\r\n",
      "Epoch 011 | Train Loss: 0.4679 | Val Loss: 0.4569 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4569)\r\n",
      "Epoch 012 | Train Loss: 0.4644 | Val Loss: 0.4534 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4534)\r\n",
      "Epoch 013 | Train Loss: 0.4622 | Val Loss: 0.4517 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4517)\r\n",
      "Epoch 014 | Train Loss: 0.4537 | Val Loss: 0.4490 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4490)\r\n",
      "Epoch 015 | Train Loss: 0.4533 | Val Loss: 0.4491 | Time: 4.82s\r\n",
      "Epoch 016 | Train Loss: 0.4549 | Val Loss: 0.4472 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4472)\r\n",
      "Epoch 017 | Train Loss: 0.4463 | Val Loss: 0.4475 | Time: 4.67s\r\n",
      "Epoch 018 | Train Loss: 0.4484 | Val Loss: 0.4467 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4467)\r\n",
      "Epoch 019 | Train Loss: 0.4463 | Val Loss: 0.4456 | Time: 4.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4456)\r\n",
      "Epoch 020 | Train Loss: 0.4501 | Val Loss: 0.4445 | Time: 4.60s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4445)\r\n",
      "Epoch 021 | Train Loss: 0.4485 | Val Loss: 0.4437 | Time: 4.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4437)\r\n",
      "Epoch 022 | Train Loss: 0.4442 | Val Loss: 0.4419 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4419)\r\n",
      "Epoch 023 | Train Loss: 0.4388 | Val Loss: 0.4417 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 024 | Train Loss: 0.4403 | Val Loss: 0.4415 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4415)\r\n",
      "Epoch 025 | Train Loss: 0.4412 | Val Loss: 0.4405 | Time: 4.80s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4405)\r\n",
      "Epoch 026 | Train Loss: 0.4384 | Val Loss: 0.4396 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4396)\r\n",
      "Epoch 027 | Train Loss: 0.4358 | Val Loss: 0.4389 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4389)\r\n",
      "Epoch 028 | Train Loss: 0.4402 | Val Loss: 0.4382 | Time: 4.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4382)\r\n",
      "Epoch 029 | Train Loss: 0.4378 | Val Loss: 0.4387 | Time: 4.78s\r\n",
      "Epoch 030 | Train Loss: 0.4356 | Val Loss: 0.4376 | Time: 4.62s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4376)\r\n",
      "Epoch 031 | Train Loss: 0.4428 | Val Loss: 0.4360 | Time: 4.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4360)\r\n",
      "Epoch 032 | Train Loss: 0.4339 | Val Loss: 0.4354 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4354)\r\n",
      "Epoch 033 | Train Loss: 0.4296 | Val Loss: 0.4358 | Time: 4.72s\r\n",
      "Epoch 034 | Train Loss: 0.4334 | Val Loss: 0.4341 | Time: 4.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4341)\r\n",
      "Epoch 035 | Train Loss: 0.4385 | Val Loss: 0.4333 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4333)\r\n",
      "Epoch 036 | Train Loss: 0.4339 | Val Loss: 0.4326 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4326)\r\n",
      "Epoch 037 | Train Loss: 0.4331 | Val Loss: 0.4328 | Time: 4.67s\r\n",
      "Epoch 038 | Train Loss: 0.4365 | Val Loss: 0.4316 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4316)\r\n",
      "Epoch 039 | Train Loss: 0.4361 | Val Loss: 0.4299 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4299)\r\n",
      "Epoch 040 | Train Loss: 0.4300 | Val Loss: 0.4297 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 041 | Train Loss: 0.4354 | Val Loss: 0.4296 | Time: 4.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 042 | Train Loss: 0.4341 | Val Loss: 0.4293 | Time: 4.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4293)\r\n",
      "Epoch 043 | Train Loss: 0.4325 | Val Loss: 0.4283 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 044 | Train Loss: 0.4353 | Val Loss: 0.4271 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4271)\r\n",
      "Epoch 045 | Train Loss: 0.4297 | Val Loss: 0.4278 | Time: 4.66s\r\n",
      "Epoch 046 | Train Loss: 0.4338 | Val Loss: 0.4284 | Time: 4.71s\r\n",
      "Epoch 047 | Train Loss: 0.4303 | Val Loss: 0.4277 | Time: 4.64s\r\n",
      "Epoch 048 | Train Loss: 0.4271 | Val Loss: 0.4276 | Time: 4.84s\r\n",
      "Epoch 049 | Train Loss: 0.4271 | Val Loss: 0.4271 | Time: 4.60s\r\n",
      "✅ Meilleur modèle Random chargé (époque 44, val_loss: 0.4271)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. SeniorCitizen        (CAT): 0.1018\r\n",
      "   2. PhoneService         (CAT): 0.0944\r\n",
      "   3. OnlineSecurity       (CAT): 0.0914\r\n",
      "   4. InternetService      (CAT): 0.0867\r\n",
      "   5. gender               (CAT): 0.0820\r\n",
      "   6. Dependents           (CAT): 0.0809\r\n",
      "   7. tenure               (NUM): 0.0761\r\n",
      "   8. DeviceProtection     (CAT): 0.0689\r\n",
      "   9. StreamingTV          (CAT): 0.0663\r\n",
      "  10. MonthlyCharges       (NUM): 0.0636\r\n",
      "  11. Contract             (CAT): 0.0636\r\n",
      "  12. PaperlessBilling     (CAT): 0.0624\r\n",
      "  13. TechSupport          (CAT): 0.0620\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. SeniorCitizen       : 0.1018\r\n",
      "   2. PhoneService        : 0.0944\r\n",
      "   3. OnlineSecurity      : 0.0914\r\n",
      "   4. InternetService     : 0.0867\r\n",
      "   5. gender              : 0.0820\r\n",
      "   6. Dependents          : 0.0809\r\n",
      "   7. tenure              : 0.0761\r\n",
      "   8. DeviceProtection    : 0.0689\r\n",
      "   9. StreamingTV         : 0.0663\r\n",
      "  10. MonthlyCharges      : 0.0636\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 659.9s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: Q\r\n",
      "Modèle FTT+ créé avec 43,249 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.7135 | Val Loss: 0.6434 | Time: 8.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6434)\r\n",
      "Epoch 001 | Train Loss: 0.6373 | Val Loss: 0.6108 | Time: 8.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.6108)\r\n",
      "Epoch 002 | Train Loss: 0.6137 | Val Loss: 0.5963 | Time: 8.51s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5963)\r\n",
      "Epoch 003 | Train Loss: 0.5971 | Val Loss: 0.5807 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5807)\r\n",
      "Epoch 004 | Train Loss: 0.5797 | Val Loss: 0.5632 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5632)\r\n",
      "Epoch 005 | Train Loss: 0.5652 | Val Loss: 0.5481 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5481)\r\n",
      "Epoch 006 | Train Loss: 0.5513 | Val Loss: 0.5346 | Time: 8.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5346)\r\n",
      "Epoch 007 | Train Loss: 0.5400 | Val Loss: 0.5218 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5218)\r\n",
      "Epoch 008 | Train Loss: 0.5291 | Val Loss: 0.5108 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5108)\r\n",
      "Epoch 009 | Train Loss: 0.5236 | Val Loss: 0.5008 | Time: 8.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5008)\r\n",
      "Epoch 010 | Train Loss: 0.5134 | Val Loss: 0.4928 | Time: 8.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4928)\r\n",
      "Epoch 011 | Train Loss: 0.5055 | Val Loss: 0.4859 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4859)\r\n",
      "Epoch 012 | Train Loss: 0.5037 | Val Loss: 0.4805 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4805)\r\n",
      "Epoch 013 | Train Loss: 0.4996 | Val Loss: 0.4747 | Time: 8.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4747)\r\n",
      "Epoch 014 | Train Loss: 0.4915 | Val Loss: 0.4696 | Time: 8.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4696)\r\n",
      "Epoch 015 | Train Loss: 0.4867 | Val Loss: 0.4650 | Time: 8.45s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4650)\r\n",
      "Epoch 016 | Train Loss: 0.4846 | Val Loss: 0.4621 | Time: 8.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4621)\r\n",
      "Epoch 017 | Train Loss: 0.4832 | Val Loss: 0.4583 | Time: 8.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4583)\r\n",
      "Epoch 018 | Train Loss: 0.4762 | Val Loss: 0.4550 | Time: 8.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4550)\r\n",
      "Epoch 019 | Train Loss: 0.4761 | Val Loss: 0.4517 | Time: 8.41s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4517)\r\n",
      "Epoch 020 | Train Loss: 0.4703 | Val Loss: 0.4499 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4499)\r\n",
      "Epoch 021 | Train Loss: 0.4707 | Val Loss: 0.4475 | Time: 8.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4475)\r\n",
      "Epoch 022 | Train Loss: 0.4690 | Val Loss: 0.4448 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4448)\r\n",
      "Epoch 023 | Train Loss: 0.4629 | Val Loss: 0.4425 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4425)\r\n",
      "Epoch 024 | Train Loss: 0.4630 | Val Loss: 0.4410 | Time: 8.26s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4410)\r\n",
      "Epoch 025 | Train Loss: 0.4616 | Val Loss: 0.4400 | Time: 8.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4400)\r\n",
      "Epoch 026 | Train Loss: 0.4582 | Val Loss: 0.4383 | Time: 8.42s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4383)\r\n",
      "Epoch 027 | Train Loss: 0.4547 | Val Loss: 0.4364 | Time: 8.43s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4364)\r\n",
      "Epoch 028 | Train Loss: 0.4553 | Val Loss: 0.4347 | Time: 8.50s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4347)\r\n",
      "Epoch 029 | Train Loss: 0.4552 | Val Loss: 0.4353 | Time: 8.48s\r\n",
      "Epoch 030 | Train Loss: 0.4556 | Val Loss: 0.4330 | Time: 8.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4330)\r\n",
      "Epoch 031 | Train Loss: 0.4513 | Val Loss: 0.4320 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4320)\r\n",
      "Epoch 032 | Train Loss: 0.4519 | Val Loss: 0.4310 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4310)\r\n",
      "Epoch 033 | Train Loss: 0.4478 | Val Loss: 0.4296 | Time: 8.47s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4296)\r\n",
      "Epoch 034 | Train Loss: 0.4467 | Val Loss: 0.4276 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4276)\r\n",
      "Epoch 035 | Train Loss: 0.4480 | Val Loss: 0.4269 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4269)\r\n",
      "Epoch 036 | Train Loss: 0.4401 | Val Loss: 0.4262 | Time: 8.58s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4262)\r\n",
      "Epoch 037 | Train Loss: 0.4438 | Val Loss: 0.4265 | Time: 8.33s\r\n",
      "Epoch 038 | Train Loss: 0.4449 | Val Loss: 0.4248 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4248)\r\n",
      "Epoch 039 | Train Loss: 0.4427 | Val Loss: 0.4236 | Time: 8.40s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4236)\r\n",
      "Epoch 040 | Train Loss: 0.4412 | Val Loss: 0.4233 | Time: 8.46s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4233)\r\n",
      "Epoch 041 | Train Loss: 0.4392 | Val Loss: 0.4237 | Time: 8.42s\r\n",
      "Epoch 042 | Train Loss: 0.4405 | Val Loss: 0.4231 | Time: 8.32s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4231)\r\n",
      "Epoch 043 | Train Loss: 0.4351 | Val Loss: 0.4216 | Time: 8.44s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 044 | Train Loss: 0.4385 | Val Loss: 0.4224 | Time: 8.46s\r\n",
      "Epoch 045 | Train Loss: 0.4379 | Val Loss: 0.4209 | Time: 8.35s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 046 | Train Loss: 0.4362 | Val Loss: 0.4206 | Time: 8.39s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4206)\r\n",
      "Epoch 047 | Train Loss: 0.4352 | Val Loss: 0.4203 | Time: 8.53s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4203)\r\n",
      "Epoch 048 | Train Loss: 0.4338 | Val Loss: 0.4202 | Time: 8.55s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4202)\r\n",
      "Epoch 049 | Train Loss: 0.4336 | Val Loss: 0.4197 | Time: 8.31s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "✅ Meilleur modèle chargé (époque 49, val_loss: 0.4197)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TechSupport         : 0.0573\r\n",
      "   2. Contract            : 0.0557\r\n",
      "   3. TotalCharges        : 0.0557\r\n",
      "   4. Dependents          : 0.0554\r\n",
      "   5. MultipleLines       : 0.0554\r\n",
      "   6. Partner             : 0.0544\r\n",
      "   7. StreamingMovies     : 0.0536\r\n",
      "   8. DeviceProtection    : 0.0531\r\n",
      "   9. MonthlyCharges      : 0.0528\r\n",
      "  10. PaperlessBilling    : 0.0528\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 13 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. TechSupport          (CAT): 0.0573\r\n",
      "   2. Contract             (CAT): 0.0557\r\n",
      "   3. TotalCharges         (NUM): 0.0557\r\n",
      "   4. Dependents           (CAT): 0.0554\r\n",
      "   5. MultipleLines        (CAT): 0.0554\r\n",
      "   6. Partner              (CAT): 0.0544\r\n",
      "   7. StreamingMovies      (CAT): 0.0536\r\n",
      "   8. DeviceProtection     (CAT): 0.0531\r\n",
      "   9. MonthlyCharges       (NUM): 0.0528\r\n",
      "  10. PaperlessBilling     (CAT): 0.0528\r\n",
      "  11. PhoneService         (CAT): 0.0521\r\n",
      "  12. InternetService      (CAT): 0.0520\r\n",
      "  13. gender               (CAT): 0.0516\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['TotalCharges', 'MonthlyCharges'] → indices [2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['TechSupport', 'Contract', 'Dependents', 'MultipleLines', 'Partner', 'StreamingMovies', 'DeviceProtection', 'PaperlessBilling', 'PhoneService', 'InternetService', 'gender'] → indices [10, 13, 3, 5, 2, 12, 9, 14, 4, 6, 0]\r\n",
      "📊 Features sélectionnées: 2 numériques, 11 catégorielles\r\n",
      "🎲 Interactions aléatoires: 5 paires\r\n",
      "Modèle Random créé avec 91,137 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.63%\r\n",
      "   - Connexions feature-feature: 10\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6649 | Val Loss: 0.5458 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5458)\r\n",
      "Epoch 001 | Train Loss: 0.5730 | Val Loss: 0.5037 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5037)\r\n",
      "Epoch 002 | Train Loss: 0.5442 | Val Loss: 0.4784 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4784)\r\n",
      "Epoch 003 | Train Loss: 0.5262 | Val Loss: 0.4617 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4617)\r\n",
      "Epoch 004 | Train Loss: 0.5135 | Val Loss: 0.4520 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4520)\r\n",
      "Epoch 005 | Train Loss: 0.5066 | Val Loss: 0.4452 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4452)\r\n",
      "Epoch 006 | Train Loss: 0.4927 | Val Loss: 0.4420 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4420)\r\n",
      "Epoch 007 | Train Loss: 0.4895 | Val Loss: 0.4399 | Time: 4.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4399)\r\n",
      "Epoch 008 | Train Loss: 0.4822 | Val Loss: 0.4406 | Time: 4.75s\r\n",
      "Epoch 009 | Train Loss: 0.4811 | Val Loss: 0.4405 | Time: 4.80s\r\n",
      "Epoch 010 | Train Loss: 0.4713 | Val Loss: 0.4380 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4380)\r\n",
      "Epoch 011 | Train Loss: 0.4749 | Val Loss: 0.4383 | Time: 4.67s\r\n",
      "Epoch 012 | Train Loss: 0.4735 | Val Loss: 0.4348 | Time: 4.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4348)\r\n",
      "Epoch 013 | Train Loss: 0.4679 | Val Loss: 0.4352 | Time: 4.62s\r\n",
      "Epoch 014 | Train Loss: 0.4655 | Val Loss: 0.4369 | Time: 4.70s\r\n",
      "Epoch 015 | Train Loss: 0.4640 | Val Loss: 0.4340 | Time: 4.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4340)\r\n",
      "Epoch 016 | Train Loss: 0.4609 | Val Loss: 0.4325 | Time: 4.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4325)\r\n",
      "Epoch 017 | Train Loss: 0.4613 | Val Loss: 0.4327 | Time: 4.65s\r\n",
      "Epoch 018 | Train Loss: 0.4572 | Val Loss: 0.4320 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4320)\r\n",
      "Epoch 019 | Train Loss: 0.4564 | Val Loss: 0.4328 | Time: 4.70s\r\n",
      "Epoch 020 | Train Loss: 0.4589 | Val Loss: 0.4289 | Time: 4.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4289)\r\n",
      "Epoch 021 | Train Loss: 0.4538 | Val Loss: 0.4284 | Time: 4.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4284)\r\n",
      "Epoch 022 | Train Loss: 0.4569 | Val Loss: 0.4290 | Time: 4.88s\r\n",
      "Epoch 023 | Train Loss: 0.4552 | Val Loss: 0.4287 | Time: 4.70s\r\n",
      "Epoch 024 | Train Loss: 0.4556 | Val Loss: 0.4275 | Time: 4.66s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4275)\r\n",
      "Epoch 025 | Train Loss: 0.4598 | Val Loss: 0.4278 | Time: 4.66s\r\n",
      "Epoch 026 | Train Loss: 0.4504 | Val Loss: 0.4265 | Time: 4.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4265)\r\n",
      "Epoch 027 | Train Loss: 0.4503 | Val Loss: 0.4263 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4263)\r\n",
      "Epoch 028 | Train Loss: 0.4532 | Val Loss: 0.4247 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4247)\r\n",
      "Epoch 029 | Train Loss: 0.4513 | Val Loss: 0.4257 | Time: 5.02s\r\n",
      "Epoch 030 | Train Loss: 0.4507 | Val Loss: 0.4263 | Time: 4.74s\r\n",
      "Epoch 031 | Train Loss: 0.4517 | Val Loss: 0.4249 | Time: 4.78s\r\n",
      "Epoch 032 | Train Loss: 0.4498 | Val Loss: 0.4225 | Time: 4.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4225)\r\n",
      "Epoch 033 | Train Loss: 0.4510 | Val Loss: 0.4222 | Time: 4.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4222)\r\n",
      "Epoch 034 | Train Loss: 0.4485 | Val Loss: 0.4239 | Time: 4.67s\r\n",
      "Epoch 035 | Train Loss: 0.4463 | Val Loss: 0.4226 | Time: 4.73s\r\n",
      "Epoch 036 | Train Loss: 0.4474 | Val Loss: 0.4226 | Time: 4.72s\r\n",
      "Epoch 037 | Train Loss: 0.4479 | Val Loss: 0.4209 | Time: 4.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 038 | Train Loss: 0.4489 | Val Loss: 0.4214 | Time: 4.64s\r\n",
      "Epoch 039 | Train Loss: 0.4481 | Val Loss: 0.4209 | Time: 4.63s\r\n",
      "Epoch 040 | Train Loss: 0.4457 | Val Loss: 0.4228 | Time: 4.84s\r\n",
      "Epoch 041 | Train Loss: 0.4483 | Val Loss: 0.4211 | Time: 4.62s\r\n",
      "Epoch 042 | Train Loss: 0.4441 | Val Loss: 0.4209 | Time: 4.77s\r\n",
      "Epoch 043 | Train Loss: 0.4472 | Val Loss: 0.4210 | Time: 4.67s\r\n",
      "Epoch 044 | Train Loss: 0.4470 | Val Loss: 0.4219 | Time: 4.72s\r\n",
      "Epoch 045 | Train Loss: 0.4451 | Val Loss: 0.4215 | Time: 4.66s\r\n",
      "Epoch 046 | Train Loss: 0.4433 | Val Loss: 0.4194 | Time: 4.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 047 | Train Loss: 0.4444 | Val Loss: 0.4205 | Time: 4.68s\r\n",
      "Epoch 048 | Train Loss: 0.4429 | Val Loss: 0.4212 | Time: 4.74s\r\n",
      "Epoch 049 | Train Loss: 0.4467 | Val Loss: 0.4199 | Time: 4.75s\r\n",
      "✅ Meilleur modèle Random chargé (époque 46, val_loss: 0.4194)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Contract             (CAT): 0.0887\r\n",
      "   2. gender               (CAT): 0.0861\r\n",
      "   3. Dependents           (CAT): 0.0842\r\n",
      "   4. MonthlyCharges       (NUM): 0.0801\r\n",
      "   5. Partner              (CAT): 0.0774\r\n",
      "   6. TechSupport          (CAT): 0.0773\r\n",
      "   7. PhoneService         (CAT): 0.0766\r\n",
      "   8. InternetService      (CAT): 0.0743\r\n",
      "   9. TotalCharges         (NUM): 0.0734\r\n",
      "  10. MultipleLines        (CAT): 0.0734\r\n",
      "  11. StreamingMovies      (CAT): 0.0729\r\n",
      "  12. DeviceProtection     (CAT): 0.0706\r\n",
      "  13. PaperlessBilling     (CAT): 0.0651\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.0887\r\n",
      "   2. gender              : 0.0861\r\n",
      "   3. Dependents          : 0.0842\r\n",
      "   4. MonthlyCharges      : 0.0801\r\n",
      "   5. Partner             : 0.0774\r\n",
      "   6. TechSupport         : 0.0773\r\n",
      "   7. PhoneService        : 0.0766\r\n",
      "   8. InternetService     : 0.0743\r\n",
      "   9. TotalCharges        : 0.0734\r\n",
      "  10. MultipleLines       : 0.0734\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_23/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 660.5s ===\r\n",
      "\u001b[32m[I 2025-07-20 02:06:04,919]\u001b[0m Trial 23 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 3, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.1550046088858983, 'ffn_dropout_stage1': 0.11817506540127426, 'residual_dropout_stage1': 0.10255560807205794, 'lr_stage1': 3.3359487769024305e-05, 'weight_decay_stage1': 0.018116519170776155, 'd_token_stage2': 64, 'n_blocks_stage2': 3, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.16114195094336808, 'ffn_dropout_stage2': 0.27647577364172954, 'residual_dropout_stage2': 0.17575959211931483, 'lr_stage2': 2.058993639103954e-05, 'weight_decay_stage2': 0.0018111692629504317, 'batch_size': 32, 'patience': 21, 'embedding_type': 'Q', 'M': 13, 'k': 5}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0:  96%|████████▋| 24/25 [7:53:07<23:32, 1412.23s/it]Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6097 | Val Loss: 0.5362 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5362)\r\n",
      "Epoch 001 | Train Loss: 0.5089 | Val Loss: 0.4741 | Time: 5.64s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4741)\r\n",
      "Epoch 002 | Train Loss: 0.4704 | Val Loss: 0.4463 | Time: 5.61s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4463)\r\n",
      "Epoch 003 | Train Loss: 0.4521 | Val Loss: 0.4320 | Time: 5.65s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4320)\r\n",
      "Epoch 004 | Train Loss: 0.4449 | Val Loss: 0.4253 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4253)\r\n",
      "Epoch 005 | Train Loss: 0.4394 | Val Loss: 0.4192 | Time: 5.92s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 006 | Train Loss: 0.4351 | Val Loss: 0.4178 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 007 | Train Loss: 0.4312 | Val Loss: 0.4134 | Time: 5.76s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4134)\r\n",
      "Epoch 008 | Train Loss: 0.4292 | Val Loss: 0.4120 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4120)\r\n",
      "Epoch 009 | Train Loss: 0.4293 | Val Loss: 0.4089 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4089)\r\n",
      "Epoch 010 | Train Loss: 0.4279 | Val Loss: 0.4074 | Time: 5.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4074)\r\n",
      "Epoch 011 | Train Loss: 0.4253 | Val Loss: 0.4063 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4063)\r\n",
      "Epoch 012 | Train Loss: 0.4208 | Val Loss: 0.4056 | Time: 5.67s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4056)\r\n",
      "Epoch 013 | Train Loss: 0.4206 | Val Loss: 0.4045 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4045)\r\n",
      "Epoch 014 | Train Loss: 0.4190 | Val Loss: 0.4063 | Time: 5.73s\r\n",
      "Epoch 015 | Train Loss: 0.4222 | Val Loss: 0.4071 | Time: 5.83s\r\n",
      "Epoch 016 | Train Loss: 0.4197 | Val Loss: 0.4033 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4033)\r\n",
      "Epoch 017 | Train Loss: 0.4185 | Val Loss: 0.4056 | Time: 5.71s\r\n",
      "Epoch 018 | Train Loss: 0.4169 | Val Loss: 0.4040 | Time: 5.70s\r\n",
      "Epoch 019 | Train Loss: 0.4187 | Val Loss: 0.4026 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4026)\r\n",
      "Epoch 020 | Train Loss: 0.4181 | Val Loss: 0.4042 | Time: 5.67s\r\n",
      "Epoch 021 | Train Loss: 0.4135 | Val Loss: 0.4091 | Time: 5.84s\r\n",
      "Epoch 022 | Train Loss: 0.4146 | Val Loss: 0.4026 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4026)\r\n",
      "Epoch 023 | Train Loss: 0.4161 | Val Loss: 0.4060 | Time: 5.74s\r\n",
      "Epoch 024 | Train Loss: 0.4130 | Val Loss: 0.4080 | Time: 5.69s\r\n",
      "Epoch 025 | Train Loss: 0.4123 | Val Loss: 0.4071 | Time: 5.67s\r\n",
      "Epoch 026 | Train Loss: 0.4162 | Val Loss: 0.4024 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4024)\r\n",
      "Epoch 027 | Train Loss: 0.4153 | Val Loss: 0.4015 | Time: 5.75s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4015)\r\n",
      "Epoch 028 | Train Loss: 0.4111 | Val Loss: 0.4035 | Time: 5.68s\r\n",
      "Epoch 029 | Train Loss: 0.4147 | Val Loss: 0.4063 | Time: 5.72s\r\n",
      "Epoch 030 | Train Loss: 0.4104 | Val Loss: 0.4029 | Time: 5.68s\r\n",
      "Epoch 031 | Train Loss: 0.4113 | Val Loss: 0.3997 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.3997)\r\n",
      "Epoch 032 | Train Loss: 0.4115 | Val Loss: 0.4037 | Time: 5.75s\r\n",
      "Epoch 033 | Train Loss: 0.4143 | Val Loss: 0.4031 | Time: 5.76s\r\n",
      "Epoch 034 | Train Loss: 0.4112 | Val Loss: 0.4040 | Time: 5.64s\r\n",
      "Epoch 035 | Train Loss: 0.4073 | Val Loss: 0.4098 | Time: 5.72s\r\n",
      "Epoch 036 | Train Loss: 0.4106 | Val Loss: 0.4064 | Time: 5.81s\r\n",
      "Epoch 037 | Train Loss: 0.4093 | Val Loss: 0.4093 | Time: 5.74s\r\n",
      "Epoch 038 | Train Loss: 0.4069 | Val Loss: 0.4096 | Time: 5.85s\r\n",
      "Epoch 039 | Train Loss: 0.4049 | Val Loss: 0.4125 | Time: 5.67s\r\n",
      "Epoch 040 | Train Loss: 0.4082 | Val Loss: 0.4080 | Time: 5.73s\r\n",
      "Epoch 041 | Train Loss: 0.4087 | Val Loss: 0.4080 | Time: 5.67s\r\n",
      "Epoch 042 | Train Loss: 0.4072 | Val Loss: 0.4086 | Time: 5.64s\r\n",
      "Epoch 043 | Train Loss: 0.4055 | Val Loss: 0.4077 | Time: 5.92s\r\n",
      "Epoch 044 | Train Loss: 0.4098 | Val Loss: 0.4076 | Time: 5.67s\r\n",
      "Epoch 045 | Train Loss: 0.4086 | Val Loss: 0.4066 | Time: 5.66s\r\n",
      "Epoch 046 | Train Loss: 0.4040 | Val Loss: 0.4063 | Time: 5.65s\r\n",
      "Epoch 047 | Train Loss: 0.4058 | Val Loss: 0.4115 | Time: 5.68s\r\n",
      "Epoch 048 | Train Loss: 0.4055 | Val Loss: 0.4081 | Time: 5.69s\r\n",
      "Epoch 049 | Train Loss: 0.4047 | Val Loss: 0.4092 | Time: 5.90s\r\n",
      "\r\n",
      "Early stopping à l'époque 49 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 31, val_loss: 0.3997)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0740\r\n",
      "   2. tenure              : 0.0623\r\n",
      "   3. StreamingMovies     : 0.0593\r\n",
      "   4. SeniorCitizen       : 0.0563\r\n",
      "   5. StreamingTV         : 0.0540\r\n",
      "   6. DeviceProtection    : 0.0528\r\n",
      "   7. Partner             : 0.0521\r\n",
      "   8. TechSupport         : 0.0515\r\n",
      "   9. InternetService     : 0.0513\r\n",
      "  10. OnlineSecurity      : 0.0511\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_0.pt\r\n",
      "\r\n",
      "🎯 Sélection des 11 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0740\r\n",
      "   2. tenure               (NUM): 0.0623\r\n",
      "   3. StreamingMovies      (CAT): 0.0593\r\n",
      "   4. SeniorCitizen        (CAT): 0.0563\r\n",
      "   5. StreamingTV          (CAT): 0.0540\r\n",
      "   6. DeviceProtection     (CAT): 0.0528\r\n",
      "   7. Partner              (CAT): 0.0521\r\n",
      "   8. TechSupport          (CAT): 0.0515\r\n",
      "   9. InternetService      (CAT): 0.0513\r\n",
      "  10. OnlineSecurity       (CAT): 0.0511\r\n",
      "  11. Dependents           (CAT): 0.0510\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure'] → indices [0]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'StreamingMovies', 'SeniorCitizen', 'StreamingTV', 'DeviceProtection', 'Partner', 'TechSupport', 'InternetService', 'OnlineSecurity', 'Dependents'] → indices [15, 12, 1, 11, 9, 2, 10, 6, 7, 3]\r\n",
      "📊 Features sélectionnées: 1 numériques, 10 catégorielles\r\n",
      "🎲 Interactions aléatoires: 2 paires\r\n",
      "Modèle Random créé avec 120,321 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.94%\r\n",
      "   - Connexions feature-feature: 4\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5527 | Val Loss: 0.4600 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4600)\r\n",
      "Epoch 001 | Train Loss: 0.4845 | Val Loss: 0.4482 | Time: 6.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4482)\r\n",
      "Epoch 002 | Train Loss: 0.4718 | Val Loss: 0.4432 | Time: 6.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4432)\r\n",
      "Epoch 003 | Train Loss: 0.4667 | Val Loss: 0.4392 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4392)\r\n",
      "Epoch 004 | Train Loss: 0.4622 | Val Loss: 0.4334 | Time: 6.12s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4334)\r\n",
      "Epoch 005 | Train Loss: 0.4576 | Val Loss: 0.4325 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4325)\r\n",
      "Epoch 006 | Train Loss: 0.4529 | Val Loss: 0.4297 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4297)\r\n",
      "Epoch 007 | Train Loss: 0.4511 | Val Loss: 0.4272 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4272)\r\n",
      "Epoch 008 | Train Loss: 0.4528 | Val Loss: 0.4263 | Time: 6.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4263)\r\n",
      "Epoch 009 | Train Loss: 0.4511 | Val Loss: 0.4223 | Time: 6.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4223)\r\n",
      "Epoch 010 | Train Loss: 0.4438 | Val Loss: 0.4220 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4220)\r\n",
      "Epoch 011 | Train Loss: 0.4530 | Val Loss: 0.4200 | Time: 6.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4200)\r\n",
      "Epoch 012 | Train Loss: 0.4432 | Val Loss: 0.4198 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4198)\r\n",
      "Epoch 013 | Train Loss: 0.4453 | Val Loss: 0.4185 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4185)\r\n",
      "Epoch 014 | Train Loss: 0.4408 | Val Loss: 0.4174 | Time: 6.12s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4174)\r\n",
      "Epoch 015 | Train Loss: 0.4461 | Val Loss: 0.4163 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4163)\r\n",
      "Epoch 016 | Train Loss: 0.4435 | Val Loss: 0.4165 | Time: 6.07s\r\n",
      "Epoch 017 | Train Loss: 0.4400 | Val Loss: 0.4155 | Time: 6.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4155)\r\n",
      "Epoch 018 | Train Loss: 0.4384 | Val Loss: 0.4157 | Time: 6.07s\r\n",
      "Epoch 019 | Train Loss: 0.4339 | Val Loss: 0.4146 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 020 | Train Loss: 0.4425 | Val Loss: 0.4137 | Time: 6.24s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4137)\r\n",
      "Epoch 021 | Train Loss: 0.4442 | Val Loss: 0.4150 | Time: 6.07s\r\n",
      "Epoch 022 | Train Loss: 0.4377 | Val Loss: 0.4140 | Time: 6.02s\r\n",
      "Epoch 023 | Train Loss: 0.4421 | Val Loss: 0.4135 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4135)\r\n",
      "Epoch 024 | Train Loss: 0.4384 | Val Loss: 0.4138 | Time: 6.13s\r\n",
      "Epoch 025 | Train Loss: 0.4379 | Val Loss: 0.4140 | Time: 6.17s\r\n",
      "Epoch 026 | Train Loss: 0.4338 | Val Loss: 0.4125 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4125)\r\n",
      "Epoch 027 | Train Loss: 0.4367 | Val Loss: 0.4101 | Time: 6.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4101)\r\n",
      "Epoch 028 | Train Loss: 0.4372 | Val Loss: 0.4115 | Time: 6.11s\r\n",
      "Epoch 029 | Train Loss: 0.4376 | Val Loss: 0.4109 | Time: 6.06s\r\n",
      "Epoch 030 | Train Loss: 0.4365 | Val Loss: 0.4111 | Time: 6.16s\r\n",
      "Epoch 031 | Train Loss: 0.4377 | Val Loss: 0.4119 | Time: 6.01s\r\n",
      "Epoch 032 | Train Loss: 0.4359 | Val Loss: 0.4100 | Time: 5.98s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4100)\r\n",
      "Epoch 033 | Train Loss: 0.4349 | Val Loss: 0.4107 | Time: 6.06s\r\n",
      "Epoch 034 | Train Loss: 0.4374 | Val Loss: 0.4110 | Time: 6.06s\r\n",
      "Epoch 035 | Train Loss: 0.4341 | Val Loss: 0.4111 | Time: 6.27s\r\n",
      "Epoch 036 | Train Loss: 0.4347 | Val Loss: 0.4104 | Time: 6.05s\r\n",
      "Epoch 037 | Train Loss: 0.4365 | Val Loss: 0.4094 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4094)\r\n",
      "Epoch 038 | Train Loss: 0.4317 | Val Loss: 0.4084 | Time: 6.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4084)\r\n",
      "Epoch 039 | Train Loss: 0.4317 | Val Loss: 0.4088 | Time: 6.17s\r\n",
      "Epoch 040 | Train Loss: 0.4312 | Val Loss: 0.4076 | Time: 6.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4076)\r\n",
      "Epoch 041 | Train Loss: 0.4359 | Val Loss: 0.4073 | Time: 6.19s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4073)\r\n",
      "Epoch 042 | Train Loss: 0.4349 | Val Loss: 0.4092 | Time: 6.00s\r\n",
      "Epoch 043 | Train Loss: 0.4339 | Val Loss: 0.4097 | Time: 6.05s\r\n",
      "Epoch 044 | Train Loss: 0.4333 | Val Loss: 0.4085 | Time: 6.01s\r\n",
      "Epoch 045 | Train Loss: 0.4285 | Val Loss: 0.4086 | Time: 6.11s\r\n",
      "Epoch 046 | Train Loss: 0.4347 | Val Loss: 0.4078 | Time: 6.06s\r\n",
      "Epoch 047 | Train Loss: 0.4329 | Val Loss: 0.4084 | Time: 6.15s\r\n",
      "Epoch 048 | Train Loss: 0.4342 | Val Loss: 0.4081 | Time: 6.04s\r\n",
      "Epoch 049 | Train Loss: 0.4303 | Val Loss: 0.4076 | Time: 6.11s\r\n",
      "✅ Meilleur modèle Random chargé (époque 41, val_loss: 0.4073)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Dependents           (CAT): 0.1382\r\n",
      "   2. StreamingTV          (CAT): 0.1368\r\n",
      "   3. StreamingMovies      (CAT): 0.0949\r\n",
      "   4. TechSupport          (CAT): 0.0942\r\n",
      "   5. tenure               (NUM): 0.0861\r\n",
      "   6. InternetService      (CAT): 0.0827\r\n",
      "   7. DeviceProtection     (CAT): 0.0766\r\n",
      "   8. OnlineSecurity       (CAT): 0.0765\r\n",
      "   9. Partner              (CAT): 0.0762\r\n",
      "  10. SeniorCitizen        (CAT): 0.0743\r\n",
      "  11. PaymentMethod        (CAT): 0.0635\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Dependents          : 0.1382\r\n",
      "   2. StreamingTV         : 0.1368\r\n",
      "   3. StreamingMovies     : 0.0949\r\n",
      "   4. TechSupport         : 0.0942\r\n",
      "   5. tenure              : 0.0861\r\n",
      "   6. InternetService     : 0.0827\r\n",
      "   7. DeviceProtection    : 0.0766\r\n",
      "   8. OnlineSecurity      : 0.0765\r\n",
      "   9. Partner             : 0.0762\r\n",
      "  10. SeniorCitizen       : 0.0743\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/heatmaps/interpretable_ftt_plus_plus_importance_seed_0.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/heatmaps/interpretable_ftt_plus_plus_attention_seed_0.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_plus_metrics_seed_0.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_plus_importance_seed_0.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_plus_{importance|attention}_seed_0.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/interpretable_ftt_plus_plus_weights_seed_0.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_0/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 593.8s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.6015 | Val Loss: 0.5252 | Time: 5.88s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.5252)\r\n",
      "Epoch 001 | Train Loss: 0.5119 | Val Loss: 0.4899 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4899)\r\n",
      "Epoch 002 | Train Loss: 0.4786 | Val Loss: 0.4628 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4628)\r\n",
      "Epoch 003 | Train Loss: 0.4580 | Val Loss: 0.4469 | Time: 5.69s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4469)\r\n",
      "Epoch 004 | Train Loss: 0.4454 | Val Loss: 0.4390 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4390)\r\n",
      "Epoch 005 | Train Loss: 0.4355 | Val Loss: 0.4338 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4338)\r\n",
      "Epoch 006 | Train Loss: 0.4294 | Val Loss: 0.4311 | Time: 5.81s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4311)\r\n",
      "Epoch 007 | Train Loss: 0.4218 | Val Loss: 0.4276 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4276)\r\n",
      "Epoch 008 | Train Loss: 0.4205 | Val Loss: 0.4254 | Time: 5.83s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 009 | Train Loss: 0.4195 | Val Loss: 0.4229 | Time: 5.70s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4229)\r\n",
      "Epoch 010 | Train Loss: 0.4147 | Val Loss: 0.4249 | Time: 5.68s\r\n",
      "Epoch 011 | Train Loss: 0.4153 | Val Loss: 0.4213 | Time: 5.86s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4213)\r\n",
      "Epoch 012 | Train Loss: 0.4120 | Val Loss: 0.4213 | Time: 5.69s\r\n",
      "Epoch 013 | Train Loss: 0.4106 | Val Loss: 0.4197 | Time: 5.71s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 014 | Train Loss: 0.4120 | Val Loss: 0.4214 | Time: 5.84s\r\n",
      "Epoch 015 | Train Loss: 0.4098 | Val Loss: 0.4208 | Time: 5.77s\r\n",
      "Epoch 016 | Train Loss: 0.4066 | Val Loss: 0.4186 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4186)\r\n",
      "Epoch 017 | Train Loss: 0.4075 | Val Loss: 0.4178 | Time: 5.95s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4178)\r\n",
      "Epoch 018 | Train Loss: 0.4041 | Val Loss: 0.4162 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4162)\r\n",
      "Epoch 019 | Train Loss: 0.4066 | Val Loss: 0.4148 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4148)\r\n",
      "Epoch 020 | Train Loss: 0.4052 | Val Loss: 0.4152 | Time: 5.72s\r\n",
      "Epoch 021 | Train Loss: 0.4017 | Val Loss: 0.4192 | Time: 5.71s\r\n",
      "Epoch 022 | Train Loss: 0.4025 | Val Loss: 0.4146 | Time: 5.78s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4146)\r\n",
      "Epoch 023 | Train Loss: 0.4042 | Val Loss: 0.4167 | Time: 5.76s\r\n",
      "Epoch 024 | Train Loss: 0.4023 | Val Loss: 0.4161 | Time: 5.70s\r\n",
      "Epoch 025 | Train Loss: 0.4014 | Val Loss: 0.4151 | Time: 5.71s\r\n",
      "Epoch 026 | Train Loss: 0.4003 | Val Loss: 0.4157 | Time: 5.69s\r\n",
      "Epoch 027 | Train Loss: 0.3994 | Val Loss: 0.4153 | Time: 5.69s\r\n",
      "Epoch 028 | Train Loss: 0.4008 | Val Loss: 0.4174 | Time: 5.89s\r\n",
      "Epoch 029 | Train Loss: 0.3988 | Val Loss: 0.4144 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4144)\r\n",
      "Epoch 030 | Train Loss: 0.3983 | Val Loss: 0.4156 | Time: 5.76s\r\n",
      "Epoch 031 | Train Loss: 0.3994 | Val Loss: 0.4168 | Time: 5.71s\r\n",
      "Epoch 032 | Train Loss: 0.3982 | Val Loss: 0.4176 | Time: 5.84s\r\n",
      "Epoch 033 | Train Loss: 0.3978 | Val Loss: 0.4184 | Time: 5.78s\r\n",
      "Epoch 034 | Train Loss: 0.3930 | Val Loss: 0.4177 | Time: 5.69s\r\n",
      "Epoch 035 | Train Loss: 0.3952 | Val Loss: 0.4194 | Time: 5.71s\r\n",
      "Epoch 036 | Train Loss: 0.3942 | Val Loss: 0.4187 | Time: 5.66s\r\n",
      "Epoch 037 | Train Loss: 0.3932 | Val Loss: 0.4195 | Time: 5.72s\r\n",
      "Epoch 038 | Train Loss: 0.3972 | Val Loss: 0.4220 | Time: 5.77s\r\n",
      "Epoch 039 | Train Loss: 0.3983 | Val Loss: 0.4201 | Time: 5.88s\r\n",
      "Epoch 040 | Train Loss: 0.3929 | Val Loss: 0.4199 | Time: 5.73s\r\n",
      "Epoch 041 | Train Loss: 0.3943 | Val Loss: 0.4204 | Time: 5.78s\r\n",
      "Epoch 042 | Train Loss: 0.3959 | Val Loss: 0.4198 | Time: 5.69s\r\n",
      "Epoch 043 | Train Loss: 0.3923 | Val Loss: 0.4227 | Time: 5.81s\r\n",
      "Epoch 044 | Train Loss: 0.3901 | Val Loss: 0.4232 | Time: 5.78s\r\n",
      "Epoch 045 | Train Loss: 0.3913 | Val Loss: 0.4248 | Time: 5.70s\r\n",
      "Epoch 046 | Train Loss: 0.3891 | Val Loss: 0.4277 | Time: 5.75s\r\n",
      "Epoch 047 | Train Loss: 0.3902 | Val Loss: 0.4264 | Time: 5.72s\r\n",
      "\r\n",
      "Early stopping à l'époque 47 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 29, val_loss: 0.4144)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. PaymentMethod       : 0.0700\r\n",
      "   2. SeniorCitizen       : 0.0634\r\n",
      "   3. PhoneService        : 0.0602\r\n",
      "   4. OnlineBackup        : 0.0535\r\n",
      "   5. MultipleLines       : 0.0531\r\n",
      "   6. Contract            : 0.0530\r\n",
      "   7. tenure              : 0.0522\r\n",
      "   8. InternetService     : 0.0520\r\n",
      "   9. TotalCharges        : 0.0518\r\n",
      "  10. PaperlessBilling    : 0.0513\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_1.pt\r\n",
      "\r\n",
      "🎯 Sélection des 11 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. PaymentMethod        (CAT): 0.0700\r\n",
      "   2. SeniorCitizen        (CAT): 0.0634\r\n",
      "   3. PhoneService         (CAT): 0.0602\r\n",
      "   4. OnlineBackup         (CAT): 0.0535\r\n",
      "   5. MultipleLines        (CAT): 0.0531\r\n",
      "   6. Contract             (CAT): 0.0530\r\n",
      "   7. tenure               (NUM): 0.0522\r\n",
      "   8. InternetService      (CAT): 0.0520\r\n",
      "   9. TotalCharges         (NUM): 0.0518\r\n",
      "  10. PaperlessBilling     (CAT): 0.0513\r\n",
      "  11. StreamingMovies      (CAT): 0.0512\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges'] → indices [0, 2]\r\n",
      "   - Catégorielles sélectionnées: ['PaymentMethod', 'SeniorCitizen', 'PhoneService', 'OnlineBackup', 'MultipleLines', 'Contract', 'InternetService', 'PaperlessBilling', 'StreamingMovies'] → indices [15, 1, 4, 8, 5, 13, 6, 14, 12]\r\n",
      "📊 Features sélectionnées: 2 numériques, 9 catégorielles\r\n",
      "🎲 Interactions aléatoires: 2 paires\r\n",
      "Modèle Random créé avec 120,193 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.94%\r\n",
      "   - Connexions feature-feature: 4\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5421 | Val Loss: 0.4846 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4846)\r\n",
      "Epoch 001 | Train Loss: 0.4872 | Val Loss: 0.4622 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4622)\r\n",
      "Epoch 002 | Train Loss: 0.4657 | Val Loss: 0.4514 | Time: 6.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4514)\r\n",
      "Epoch 003 | Train Loss: 0.4520 | Val Loss: 0.4456 | Time: 6.13s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4456)\r\n",
      "Epoch 004 | Train Loss: 0.4468 | Val Loss: 0.4390 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4390)\r\n",
      "Epoch 005 | Train Loss: 0.4398 | Val Loss: 0.4368 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4368)\r\n",
      "Epoch 006 | Train Loss: 0.4420 | Val Loss: 0.4346 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4346)\r\n",
      "Epoch 007 | Train Loss: 0.4342 | Val Loss: 0.4318 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4318)\r\n",
      "Epoch 008 | Train Loss: 0.4328 | Val Loss: 0.4283 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 009 | Train Loss: 0.4269 | Val Loss: 0.4285 | Time: 6.04s\r\n",
      "Epoch 010 | Train Loss: 0.4307 | Val Loss: 0.4259 | Time: 6.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4259)\r\n",
      "Epoch 011 | Train Loss: 0.4304 | Val Loss: 0.4261 | Time: 6.12s\r\n",
      "Epoch 012 | Train Loss: 0.4275 | Val Loss: 0.4242 | Time: 6.38s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4242)\r\n",
      "Epoch 013 | Train Loss: 0.4262 | Val Loss: 0.4234 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4234)\r\n",
      "Epoch 014 | Train Loss: 0.4291 | Val Loss: 0.4240 | Time: 6.11s\r\n",
      "Epoch 015 | Train Loss: 0.4234 | Val Loss: 0.4228 | Time: 6.03s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4228)\r\n",
      "Epoch 016 | Train Loss: 0.4247 | Val Loss: 0.4216 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4216)\r\n",
      "Epoch 017 | Train Loss: 0.4240 | Val Loss: 0.4220 | Time: 6.26s\r\n",
      "Epoch 018 | Train Loss: 0.4203 | Val Loss: 0.4215 | Time: 6.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4215)\r\n",
      "Epoch 019 | Train Loss: 0.4206 | Val Loss: 0.4212 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4212)\r\n",
      "Epoch 020 | Train Loss: 0.4198 | Val Loss: 0.4209 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4209)\r\n",
      "Epoch 021 | Train Loss: 0.4202 | Val Loss: 0.4211 | Time: 6.08s\r\n",
      "Epoch 022 | Train Loss: 0.4213 | Val Loss: 0.4213 | Time: 6.24s\r\n",
      "Epoch 023 | Train Loss: 0.4164 | Val Loss: 0.4203 | Time: 6.04s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4203)\r\n",
      "Epoch 024 | Train Loss: 0.4190 | Val Loss: 0.4197 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4197)\r\n",
      "Epoch 025 | Train Loss: 0.4155 | Val Loss: 0.4205 | Time: 6.06s\r\n",
      "Epoch 026 | Train Loss: 0.4193 | Val Loss: 0.4195 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4195)\r\n",
      "Epoch 027 | Train Loss: 0.4164 | Val Loss: 0.4194 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4194)\r\n",
      "Epoch 028 | Train Loss: 0.4184 | Val Loss: 0.4192 | Time: 6.34s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4192)\r\n",
      "Epoch 029 | Train Loss: 0.4206 | Val Loss: 0.4195 | Time: 6.09s\r\n",
      "Epoch 030 | Train Loss: 0.4156 | Val Loss: 0.4208 | Time: 6.12s\r\n",
      "Epoch 031 | Train Loss: 0.4141 | Val Loss: 0.4203 | Time: 6.08s\r\n",
      "Epoch 032 | Train Loss: 0.4154 | Val Loss: 0.4198 | Time: 6.14s\r\n",
      "Epoch 033 | Train Loss: 0.4142 | Val Loss: 0.4188 | Time: 6.13s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 034 | Train Loss: 0.4154 | Val Loss: 0.4188 | Time: 6.07s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4188)\r\n",
      "Epoch 035 | Train Loss: 0.4147 | Val Loss: 0.4185 | Time: 6.13s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4185)\r\n",
      "Epoch 036 | Train Loss: 0.4157 | Val Loss: 0.4183 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 037 | Train Loss: 0.4150 | Val Loss: 0.4168 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4168)\r\n",
      "Epoch 038 | Train Loss: 0.4148 | Val Loss: 0.4178 | Time: 6.15s\r\n",
      "Epoch 039 | Train Loss: 0.4118 | Val Loss: 0.4180 | Time: 6.06s\r\n",
      "Epoch 040 | Train Loss: 0.4145 | Val Loss: 0.4181 | Time: 6.14s\r\n",
      "Epoch 041 | Train Loss: 0.4118 | Val Loss: 0.4169 | Time: 6.02s\r\n",
      "Epoch 042 | Train Loss: 0.4156 | Val Loss: 0.4178 | Time: 6.27s\r\n",
      "Epoch 043 | Train Loss: 0.4150 | Val Loss: 0.4182 | Time: 6.36s\r\n",
      "Epoch 044 | Train Loss: 0.4155 | Val Loss: 0.4182 | Time: 6.11s\r\n",
      "Epoch 045 | Train Loss: 0.4100 | Val Loss: 0.4179 | Time: 6.03s\r\n",
      "Epoch 046 | Train Loss: 0.4157 | Val Loss: 0.4174 | Time: 6.00s\r\n",
      "Epoch 047 | Train Loss: 0.4111 | Val Loss: 0.4171 | Time: 6.09s\r\n",
      "Epoch 048 | Train Loss: 0.4088 | Val Loss: 0.4170 | Time: 6.29s\r\n",
      "Epoch 049 | Train Loss: 0.4148 | Val Loss: 0.4163 | Time: 6.00s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4163)\r\n",
      "✅ Meilleur modèle Random chargé (époque 49, val_loss: 0.4163)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. TotalCharges         (NUM): 0.1197\r\n",
      "   2. InternetService      (CAT): 0.1023\r\n",
      "   3. PhoneService         (CAT): 0.0976\r\n",
      "   4. StreamingMovies      (CAT): 0.0900\r\n",
      "   5. Contract             (CAT): 0.0890\r\n",
      "   6. SeniorCitizen        (CAT): 0.0880\r\n",
      "   7. PaymentMethod        (CAT): 0.0869\r\n",
      "   8. tenure               (NUM): 0.0834\r\n",
      "   9. PaperlessBilling     (CAT): 0.0834\r\n",
      "  10. MultipleLines        (CAT): 0.0830\r\n",
      "  11. OnlineBackup         (CAT): 0.0766\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. TotalCharges        : 0.1197\r\n",
      "   2. InternetService     : 0.1023\r\n",
      "   3. PhoneService        : 0.0976\r\n",
      "   4. StreamingMovies     : 0.0900\r\n",
      "   5. Contract            : 0.0890\r\n",
      "   6. SeniorCitizen       : 0.0880\r\n",
      "   7. PaymentMethod       : 0.0869\r\n",
      "   8. tenure              : 0.0834\r\n",
      "   9. PaperlessBilling    : 0.0834\r\n",
      "  10. MultipleLines       : 0.0830\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/heatmaps/interpretable_ftt_plus_plus_importance_seed_1.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/heatmaps/interpretable_ftt_plus_plus_attention_seed_1.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_plus_metrics_seed_1.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_plus_importance_seed_1.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_plus_{importance|attention}_seed_1.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/interpretable_ftt_plus_plus_weights_seed_1.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_1/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 585.1s ===\r\n",
      "Données originales: 7043 lignes\r\n",
      "Après nettoyage: 7032 lignes (11 supprimées)\r\n",
      "\r\n",
      "Informations sur les features:\r\n",
      "Features numériques (3): ['tenure', 'MonthlyCharges', 'TotalCharges']\r\n",
      "Variables catégorielles (16): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\r\n",
      "Cardinalités des variables catégorielles: [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4]\r\n",
      "Distribution des classes: [5163 1869]\r\n",
      "Tailles des ensembles:\r\n",
      "Train: 4781 | Val: 844 | Test: 1407\r\n",
      "🚀 === PIPELINE FTT++ COMPLET ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "🚀 === ÉTAPE 1: Entraînement FTT+ Complet ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "✅ Validation mapping: 3 num + 16 cat = 19 total\r\n",
      "Type d'embedding numérique: T-LR\r\n",
      "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:499: UserWarning: Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays. The provided PyTorch tensors are not located on CPU, so the conversion has some overhead.\r\n",
      "  warnings.warn(\r\n",
      "Modèle FTT+ créé avec 29,713 paramètres\r\n",
      "⏳ Entraînement du modèle FTT+ en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5406 | Val Loss: 0.4754 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4754)\r\n",
      "Epoch 001 | Train Loss: 0.4740 | Val Loss: 0.4417 | Time: 5.73s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4417)\r\n",
      "Epoch 002 | Train Loss: 0.4563 | Val Loss: 0.4355 | Time: 5.74s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4355)\r\n",
      "Epoch 003 | Train Loss: 0.4441 | Val Loss: 0.4298 | Time: 5.82s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4298)\r\n",
      "Epoch 004 | Train Loss: 0.4401 | Val Loss: 0.4232 | Time: 5.72s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4232)\r\n",
      "Epoch 005 | Train Loss: 0.4323 | Val Loss: 0.4255 | Time: 5.73s\r\n",
      "Epoch 006 | Train Loss: 0.4294 | Val Loss: 0.4300 | Time: 5.66s\r\n",
      "Epoch 007 | Train Loss: 0.4324 | Val Loss: 0.4206 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4206)\r\n",
      "Epoch 008 | Train Loss: 0.4284 | Val Loss: 0.4204 | Time: 5.68s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4204)\r\n",
      "Epoch 009 | Train Loss: 0.4264 | Val Loss: 0.4141 | Time: 5.77s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4141)\r\n",
      "Epoch 010 | Train Loss: 0.4238 | Val Loss: 0.4142 | Time: 5.74s\r\n",
      "Epoch 011 | Train Loss: 0.4239 | Val Loss: 0.4158 | Time: 5.79s\r\n",
      "Epoch 012 | Train Loss: 0.4186 | Val Loss: 0.4190 | Time: 5.78s\r\n",
      "Epoch 013 | Train Loss: 0.4216 | Val Loss: 0.4141 | Time: 5.71s\r\n",
      "Epoch 014 | Train Loss: 0.4173 | Val Loss: 0.4085 | Time: 5.79s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4085)\r\n",
      "Epoch 015 | Train Loss: 0.4191 | Val Loss: 0.4157 | Time: 5.73s\r\n",
      "Epoch 016 | Train Loss: 0.4153 | Val Loss: 0.4136 | Time: 5.72s\r\n",
      "Epoch 017 | Train Loss: 0.4168 | Val Loss: 0.4094 | Time: 5.71s\r\n",
      "Epoch 018 | Train Loss: 0.4164 | Val Loss: 0.4123 | Time: 5.70s\r\n",
      "Epoch 019 | Train Loss: 0.4164 | Val Loss: 0.4116 | Time: 5.74s\r\n",
      "Epoch 020 | Train Loss: 0.4155 | Val Loss: 0.4140 | Time: 5.78s\r\n",
      "Epoch 021 | Train Loss: 0.4138 | Val Loss: 0.4156 | Time: 5.76s\r\n",
      "Epoch 022 | Train Loss: 0.4158 | Val Loss: 0.4122 | Time: 5.72s\r\n",
      "Epoch 023 | Train Loss: 0.4129 | Val Loss: 0.4137 | Time: 5.77s\r\n",
      "Epoch 024 | Train Loss: 0.4148 | Val Loss: 0.4140 | Time: 5.73s\r\n",
      "Epoch 025 | Train Loss: 0.4144 | Val Loss: 0.4145 | Time: 5.70s\r\n",
      "Epoch 026 | Train Loss: 0.4153 | Val Loss: 0.4125 | Time: 5.78s\r\n",
      "Epoch 027 | Train Loss: 0.4123 | Val Loss: 0.4126 | Time: 5.65s\r\n",
      "Epoch 028 | Train Loss: 0.4108 | Val Loss: 0.4148 | Time: 5.70s\r\n",
      "Epoch 029 | Train Loss: 0.4119 | Val Loss: 0.4148 | Time: 5.78s\r\n",
      "Epoch 030 | Train Loss: 0.4089 | Val Loss: 0.4169 | Time: 5.70s\r\n",
      "Epoch 031 | Train Loss: 0.4084 | Val Loss: 0.4139 | Time: 5.84s\r\n",
      "Epoch 032 | Train Loss: 0.4118 | Val Loss: 0.4169 | Time: 5.72s\r\n",
      "\r\n",
      "Early stopping à l'époque 32 (patience: 18)\r\n",
      "✅ Meilleur modèle chargé (époque 14, val_loss: 0.4085)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle FTT+...\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité avec interpretability_analyzer...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. gender              : 0.0648\r\n",
      "   2. MultipleLines       : 0.0635\r\n",
      "   3. tenure              : 0.0609\r\n",
      "   4. Contract            : 0.0584\r\n",
      "   5. OnlineSecurity      : 0.0563\r\n",
      "   6. PaperlessBilling    : 0.0559\r\n",
      "   7. StreamingTV         : 0.0550\r\n",
      "   8. Dependents          : 0.0548\r\n",
      "   9. TotalCharges        : 0.0509\r\n",
      "  10. MonthlyCharges      : 0.0508\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/heatmaps/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_from_ftt_plus_plus_weights_seed_2.pt\r\n",
      "\r\n",
      "🎯 Sélection des 11 features les plus importantes...\r\n",
      "📋 Features sélectionnées:\r\n",
      "   1. gender               (CAT): 0.0648\r\n",
      "   2. MultipleLines        (CAT): 0.0635\r\n",
      "   3. tenure               (NUM): 0.0609\r\n",
      "   4. Contract             (CAT): 0.0584\r\n",
      "   5. OnlineSecurity       (CAT): 0.0563\r\n",
      "   6. PaperlessBilling     (CAT): 0.0559\r\n",
      "   7. StreamingTV          (CAT): 0.0550\r\n",
      "   8. Dependents           (CAT): 0.0548\r\n",
      "   9. TotalCharges         (NUM): 0.0509\r\n",
      "  10. MonthlyCharges       (NUM): 0.0508\r\n",
      "  11. PhoneService         (CAT): 0.0502\r\n",
      "\r\n",
      "🎯 === ÉTAPE 2: Entraînement Modèle Random ===\r\n",
      "🖥️  Device utilisé: cuda\r\n",
      "📋 Mapping des features sélectionnées:\r\n",
      "   - Numériques sélectionnées: ['tenure', 'TotalCharges', 'MonthlyCharges'] → indices [0, 2, 1]\r\n",
      "   - Catégorielles sélectionnées: ['gender', 'MultipleLines', 'Contract', 'OnlineSecurity', 'PaperlessBilling', 'StreamingTV', 'Dependents', 'PhoneService'] → indices [0, 5, 13, 7, 14, 11, 3, 4]\r\n",
      "📊 Features sélectionnées: 3 numériques, 8 catégorielles\r\n",
      "🎲 Interactions aléatoires: 2 paires\r\n",
      "Modèle Random créé avec 119,937 paramètres\r\n",
      "🔗 Sparsité d'attention: 81.94%\r\n",
      "   - Connexions feature-feature: 4\r\n",
      "⏳ Entraînement du modèle Random en cours...\r\n",
      "Epoch 000 | Train Loss: 0.5639 | Val Loss: 0.4543 | Time: 6.14s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4543)\r\n",
      "Epoch 001 | Train Loss: 0.4969 | Val Loss: 0.4423 | Time: 6.13s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4423)\r\n",
      "Epoch 002 | Train Loss: 0.4822 | Val Loss: 0.4375 | Time: 6.11s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4375)\r\n",
      "Epoch 003 | Train Loss: 0.4738 | Val Loss: 0.4360 | Time: 6.22s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4360)\r\n",
      "Epoch 004 | Train Loss: 0.4676 | Val Loss: 0.4323 | Time: 6.05s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4323)\r\n",
      "Epoch 005 | Train Loss: 0.4684 | Val Loss: 0.4283 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4283)\r\n",
      "Epoch 006 | Train Loss: 0.4624 | Val Loss: 0.4254 | Time: 6.02s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4254)\r\n",
      "Epoch 007 | Train Loss: 0.4577 | Val Loss: 0.4233 | Time: 6.06s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4233)\r\n",
      "Epoch 008 | Train Loss: 0.4531 | Val Loss: 0.4237 | Time: 6.21s\r\n",
      "Epoch 009 | Train Loss: 0.4494 | Val Loss: 0.4235 | Time: 6.14s\r\n",
      "Epoch 010 | Train Loss: 0.4532 | Val Loss: 0.4221 | Time: 6.10s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4221)\r\n",
      "Epoch 011 | Train Loss: 0.4499 | Val Loss: 0.4231 | Time: 6.07s\r\n",
      "Epoch 012 | Train Loss: 0.4499 | Val Loss: 0.4213 | Time: 6.09s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4213)\r\n",
      "Epoch 013 | Train Loss: 0.4460 | Val Loss: 0.4202 | Time: 6.29s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4202)\r\n",
      "Epoch 014 | Train Loss: 0.4423 | Val Loss: 0.4183 | Time: 6.23s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4183)\r\n",
      "Epoch 015 | Train Loss: 0.4394 | Val Loss: 0.4187 | Time: 6.07s\r\n",
      "Epoch 016 | Train Loss: 0.4430 | Val Loss: 0.4180 | Time: 6.12s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4180)\r\n",
      "Epoch 017 | Train Loss: 0.4423 | Val Loss: 0.4164 | Time: 6.08s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4164)\r\n",
      "Epoch 018 | Train Loss: 0.4454 | Val Loss: 0.4169 | Time: 6.12s\r\n",
      "Epoch 019 | Train Loss: 0.4408 | Val Loss: 0.4159 | Time: 6.37s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4159)\r\n",
      "Epoch 020 | Train Loss: 0.4376 | Val Loss: 0.4181 | Time: 6.09s\r\n",
      "Epoch 021 | Train Loss: 0.4390 | Val Loss: 0.4167 | Time: 6.09s\r\n",
      "Epoch 022 | Train Loss: 0.4365 | Val Loss: 0.4187 | Time: 6.06s\r\n",
      "Epoch 023 | Train Loss: 0.4362 | Val Loss: 0.4165 | Time: 6.09s\r\n",
      "Epoch 024 | Train Loss: 0.4390 | Val Loss: 0.4154 | Time: 6.17s\r\n",
      " <<< NOUVEAU MEILLEUR MODÈLE (val_loss: 0.4154)\r\n",
      "Epoch 025 | Train Loss: 0.4386 | Val Loss: 0.4165 | Time: 6.05s\r\n",
      "Epoch 026 | Train Loss: 0.4323 | Val Loss: 0.4174 | Time: 6.10s\r\n",
      "Epoch 027 | Train Loss: 0.4359 | Val Loss: 0.4177 | Time: 6.10s\r\n",
      "Epoch 028 | Train Loss: 0.4333 | Val Loss: 0.4192 | Time: 6.02s\r\n",
      "Epoch 029 | Train Loss: 0.4294 | Val Loss: 0.4174 | Time: 6.25s\r\n",
      "Epoch 030 | Train Loss: 0.4343 | Val Loss: 0.4180 | Time: 6.06s\r\n",
      "Epoch 031 | Train Loss: 0.4312 | Val Loss: 0.4172 | Time: 6.04s\r\n",
      "Epoch 032 | Train Loss: 0.4300 | Val Loss: 0.4192 | Time: 6.12s\r\n",
      "Epoch 033 | Train Loss: 0.4308 | Val Loss: 0.4197 | Time: 6.11s\r\n",
      "Epoch 034 | Train Loss: 0.4299 | Val Loss: 0.4212 | Time: 6.22s\r\n",
      "Epoch 035 | Train Loss: 0.4332 | Val Loss: 0.4180 | Time: 6.02s\r\n",
      "Epoch 036 | Train Loss: 0.4315 | Val Loss: 0.4194 | Time: 6.04s\r\n",
      "Epoch 037 | Train Loss: 0.4303 | Val Loss: 0.4200 | Time: 6.09s\r\n",
      "Epoch 038 | Train Loss: 0.4328 | Val Loss: 0.4169 | Time: 6.07s\r\n",
      "Epoch 039 | Train Loss: 0.4309 | Val Loss: 0.4180 | Time: 6.31s\r\n",
      "Epoch 040 | Train Loss: 0.4297 | Val Loss: 0.4195 | Time: 6.15s\r\n",
      "Epoch 041 | Train Loss: 0.4286 | Val Loss: 0.4205 | Time: 6.05s\r\n",
      "Epoch 042 | Train Loss: 0.4291 | Val Loss: 0.4183 | Time: 6.10s\r\n",
      "\r\n",
      "Early stopping à l'époque 42 (patience: 18)\r\n",
      "✅ Meilleur modèle Random chargé (époque 24, val_loss: 0.4154)\r\n",
      "\r\n",
      "📊 Évaluation finale du modèle Random...\r\n",
      "\r\n",
      "🔍 Analyse d'importance des features dans le modèle Random...\r\n",
      "\r\n",
      "📊 Importance des features dans le modèle Random:\r\n",
      "   1. Contract             (CAT): 0.1154\r\n",
      "   2. gender               (CAT): 0.1127\r\n",
      "   3. PhoneService         (CAT): 0.0933\r\n",
      "   4. TotalCharges         (NUM): 0.0918\r\n",
      "   5. MonthlyCharges       (NUM): 0.0882\r\n",
      "   6. StreamingTV          (CAT): 0.0848\r\n",
      "   7. Dependents           (CAT): 0.0836\r\n",
      "   8. OnlineSecurity       (CAT): 0.0830\r\n",
      "   9. tenure               (NUM): 0.0829\r\n",
      "  10. PaperlessBilling     (CAT): 0.0822\r\n",
      "  11. MultipleLines        (CAT): 0.0820\r\n",
      "\r\n",
      "🔍 Analyse d'interprétabilité complète du modèle Random...\r\n",
      "\r\n",
      "=== Analyse d'interprétabilité ===\r\n",
      "\r\n",
      "Top 10 features importantes:\r\n",
      "   1. Contract            : 0.1154\r\n",
      "   2. gender              : 0.1127\r\n",
      "   3. PhoneService        : 0.0933\r\n",
      "   4. TotalCharges        : 0.0918\r\n",
      "   5. MonthlyCharges      : 0.0882\r\n",
      "   6. StreamingTV         : 0.0848\r\n",
      "   7. Dependents          : 0.0836\r\n",
      "   8. OnlineSecurity      : 0.0830\r\n",
      "   9. tenure              : 0.0829\r\n",
      "  10. PaperlessBilling    : 0.0822\r\n",
      "\r\n",
      "Génération des visualisations...\r\n",
      "Graphique d'importance sauvegardé: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/heatmaps/interpretable_ftt_plus_plus_importance_seed_2.png\r\n",
      "Heatmap d'attention sparse sauvegardée: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/heatmaps/interpretable_ftt_plus_plus_attention_seed_2.png\r\n",
      "\r\n",
      "=== Résultats sauvegardés ===\r\n",
      "Métriques: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_plus_metrics_seed_2.json\r\n",
      "Importance: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_plus_importance_seed_2.json\r\n",
      "Visualisations: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_plus_{importance|attention}_seed_2.png\r\n",
      "Modèle: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/interpretable_ftt_plus_plus_weights_seed_2.pt\r\n",
      "💾 Résultats complets sauvegardés: results/results_telecom/ftt_plus_plus_optuna/trial_24/seed_2/métriques/ftt_plus_plus_results.json\r\n",
      "\r\n",
      "✅ === PIPELINE TERMINÉ EN 455.7s ===\r\n",
      "\u001b[32m[I 2025-07-20 02:33:20,178]\u001b[0m Trial 24 finished with value: 0.0 and parameters: {'d_token_stage1': 16, 'n_blocks_stage1': 2, 'n_heads_stage1': 16, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.20686863777767972, 'ffn_dropout_stage1': 0.14053358361333793, 'residual_dropout_stage1': 0.12493781858895901, 'lr_stage1': 0.00031358950542749003, 'weight_decay_stage1': 0.00020140975511427904, 'd_token_stage2': 64, 'n_blocks_stage2': 4, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 64, 'attention_dropout_stage2': 0.19180761615402797, 'ffn_dropout_stage2': 0.23548378508859255, 'residual_dropout_stage2': 0.11104736889560297, 'lr_stage2': 6.505179979516136e-05, 'weight_decay_stage2': 0.0003256228241831873, 'batch_size': 32, 'patience': 18, 'embedding_type': 'T-LR', 'M': 11, 'k': 2}. Best is trial 0 with value: 0.0.\u001b[0m\r\n",
      "Best trial: 0. Best value: 0: 100%|█████████| 25/25 [8:20:22<00:00, 1200.92s/it]\r\n",
      "Optimization completed!\r\n",
      "Best trial: 0\r\n",
      "Best mean AUC: 0.0000\r\n",
      "Best params: {'d_token_stage1': 32, 'n_blocks_stage1': 2, 'n_heads_stage1': 8, 'ffn_hidden_stage1': 256, 'attention_dropout_stage1': 0.26648852816008434, 'ffn_dropout_stage1': 0.14246782213565523, 'residual_dropout_stage1': 0.11818249672071007, 'lr_stage1': 5.415244119402538e-05, 'weight_decay_stage1': 3.320559103751961e-05, 'd_token_stage2': 128, 'n_blocks_stage2': 2, 'n_heads_stage2': 16, 'ffn_hidden_stage2': 256, 'attention_dropout_stage2': 0.10929008254399955, 'ffn_dropout_stage2': 0.22150897038028766, 'residual_dropout_stage2': 0.11705241236872915, 'lr_stage2': 1.8205657658407255e-05, 'weight_decay_stage2': 0.055517216852447225, 'batch_size': 32, 'patience': 16, 'embedding_type': 'T-LR', 'M': 9, 'k': 6}\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/customer-churn-ft_transformer/train/Telecom/train_ftt_plus_plus/experiment_with_optuna.py\", line 319, in <module>\r\n",
      "    importance = optuna.importance.get_param_importances(study)\r\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/importance/__init__.py\", line 111, in get_param_importances\r\n",
      "    res = evaluator.evaluate(study, params=params, target=target)\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/importance/_fanova/_evaluator.py\", line 117, in evaluate\r\n",
      "    evaluator.fit(\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/importance/_fanova/_fanova.py\", line 73, in fit\r\n",
      "    raise RuntimeError(\"Encountered zero total variance in all trees.\")\r\n",
      "RuntimeError: Encountered zero total variance in all trees.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Changer vers le répertoire racine\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# Utiliser PYTHONPATH pour que train.py trouve les modules\n",
    "!PYTHONPATH=/kaggle/working/customer-churn-ft_transformer python train/Telecom/train_ftt_plus_plus/experiment_with_optuna.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829b6b2",
   "metadata": {
    "papermill": {
     "duration": 0.382462,
     "end_time": "2025-07-20T02:33:22.018231",
     "exception": false,
     "start_time": "2025-07-20T02:33:21.635769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743ceeba",
   "metadata": {
    "papermill": {
     "duration": 0.285769,
     "end_time": "2025-07-20T02:33:22.589551",
     "exception": false,
     "start_time": "2025-07-20T02:33:22.303782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Downloading the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5bec5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T02:33:23.166624Z",
     "iopub.status.busy": "2025-07-20T02:33:23.166309Z",
     "iopub.status.idle": "2025-07-20T02:33:23.407093Z",
     "shell.execute_reply": "2025-07-20T02:33:23.406410Z"
    },
    "papermill": {
     "duration": 0.530875,
     "end_time": "2025-07-20T02:33:23.408340",
     "exception": false,
     "start_time": "2025-07-20T02:33:22.877465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/customer-churn-ft_transformer\r\n",
      "total 1304\r\n",
      "drwxr-xr-x 10 root root   4096 Jul 19 18:12  .\r\n",
      "drwxr-xr-x  3 root root   4096 Jul 19 18:09  ..\r\n",
      "drwxr-xr-x  3 root root   4096 Jul 19 18:09  data\r\n",
      "-rw-r--r--  1 root root  33381 Jul 19 18:09  Experiments.ipynb\r\n",
      "drwxr-xr-x  3 root root   4096 Jul 19 18:12  ftt_plus\r\n",
      "drwxr-xr-x  8 root root   4096 Jul 19 18:12  ftt_plus_plus\r\n",
      "-rw-r--r--  1 root root  16100 Jul 19 18:09 'FT_Transformer architecture.png'\r\n",
      "drwxr-xr-x  8 root root   4096 Jul 19 18:09  .git\r\n",
      "-rw-r--r--  1 root root  36514 Jul 19 18:09 \"Illustration d'un Feature Tokenizer.png\"\r\n",
      "-rw-r--r--  1 root root   8324 Jul 19 18:09  interpretability_analyzer.py\r\n",
      "-rw-r--r--  1 root root 311104 Jul 19 18:09 'Interpretable Multi-Head Attention.png'\r\n",
      "-rw-r--r--  1 root root  10266 Jul 19 18:09  num_embedding_factory.py\r\n",
      "-rw-r--r--  1 root root 252941 Jul 19 18:09 'One Transformer layer.png'\r\n",
      "-rw-r--r--  1 root root 183257 Jul 19 18:09  pipeline_ftt_plus_plus.png\r\n",
      "drwxr-xr-x  2 root root   4096 Jul 19 18:15  __pycache__\r\n",
      "-rw-r--r--  1 root root   6862 Jul 19 18:09  README.md\r\n",
      "drwxr-xr-x  3 root root   4096 Jul 19 18:12  results\r\n",
      "drwxr-xr-x  5 root root   4096 Jul 19 18:09  rtdl_lib\r\n",
      "-rw-r--r--  1 root root 409306 Jul 19 18:09 'Scaled Dot-Product Attention.png'\r\n",
      "-rw-r--r--  1 root root   6821 Jul 19 18:09  test_embeddings.py\r\n",
      "drwxr-xr-x  3 root root   4096 Jul 19 18:09  train\r\n",
      "-rw-r--r--  1 root root   2333 Jul 19 18:09  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf306ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T02:33:23.990003Z",
     "iopub.status.busy": "2025-07-20T02:33:23.989727Z",
     "iopub.status.idle": "2025-07-20T02:33:34.673788Z",
     "shell.execute_reply": "2025-07-20T02:33:34.673177Z"
    },
    "papermill": {
     "duration": 10.974833,
     "end_time": "2025-07-20T02:33:34.674938",
     "exception": false,
     "start_time": "2025-07-20T02:33:23.700105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/results.zip'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# On se place dans le bon dossier racine\n",
    "import os\n",
    "os.chdir('/kaggle/working/customer-churn-ft_transformer')\n",
    "\n",
    "# On crée une archive ZIP du dossier results\n",
    "shutil.make_archive('/kaggle/working/results', 'zip', 'results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fc925",
   "metadata": {
    "papermill": {
     "duration": 0.392651,
     "end_time": "2025-07-20T02:33:35.363330",
     "exception": false,
     "start_time": "2025-07-20T02:33:34.970679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ede7f",
   "metadata": {
    "papermill": {
     "duration": 0.290055,
     "end_time": "2025-07-20T02:33:35.969244",
     "exception": false,
     "start_time": "2025-07-20T02:33:35.679189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30225.476008,
   "end_time": "2025-07-20T02:33:36.599824",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-19T18:09:51.123816",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
